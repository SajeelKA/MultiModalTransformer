{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"f60fda3a0aa4442490fd795d6c10e67c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_88cc678cb5c348c0b59b7e1a97e46fc4","IPY_MODEL_b192032cdeb74d35a63895f9a244cbf4","IPY_MODEL_c4695fedf31c40a0b747944a1d965793"],"layout":"IPY_MODEL_037ca27eabd941a59269a43d4ce0ef9e"}},"88cc678cb5c348c0b59b7e1a97e46fc4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c73caf33f2e4d44ab6009dd3fb86e81","placeholder":"​","style":"IPY_MODEL_b12cb6742aeb41c392a366def658c617","value":"Loading checkpoint shards: 100%"}},"b192032cdeb74d35a63895f9a244cbf4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8e3a72beaf29436398086adf647a3a9f","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8ff1673ea4e94af3a016cb61ced23491","value":2}},"c4695fedf31c40a0b747944a1d965793":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe84500081b8403485309b92ad6688a9","placeholder":"​","style":"IPY_MODEL_5498cbb54584402aaad1c8974ce44eff","value":" 2/2 [00:00&lt;00:00,  2.56it/s]"}},"037ca27eabd941a59269a43d4ce0ef9e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c73caf33f2e4d44ab6009dd3fb86e81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b12cb6742aeb41c392a366def658c617":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8e3a72beaf29436398086adf647a3a9f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ff1673ea4e94af3a016cb61ced23491":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fe84500081b8403485309b92ad6688a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5498cbb54584402aaad1c8974ce44eff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f0071ae3b47f4b45b446e39be644683f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5ef98d9cb2914fdea8005e986349b991","IPY_MODEL_03ab093ed6404bf0896b4f828f5d7cec","IPY_MODEL_89dcbfc592534819bac77a3b8c0dbf17"],"layout":"IPY_MODEL_e6d305f17907467abc3317ecceacb265"}},"5ef98d9cb2914fdea8005e986349b991":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b185ec411524166bb328f7f28d19c22","placeholder":"​","style":"IPY_MODEL_6c1379bbf54b4727b43d84ca6a5f0f50","value":"tokenizer_config.json: 100%"}},"03ab093ed6404bf0896b4f828f5d7cec":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2d81f12c01a48759ce67ecba877bcd8","max":242593,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b0dc83433ddc47218f2c67df75f6f1f9","value":242593}},"89dcbfc592534819bac77a3b8c0dbf17":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d13b16ffb0c48539289b31109981069","placeholder":"​","style":"IPY_MODEL_b9cef4799bf340f1b6cc4feb57d5323d","value":" 243k/243k [00:00&lt;00:00, 4.32MB/s]"}},"e6d305f17907467abc3317ecceacb265":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b185ec411524166bb328f7f28d19c22":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c1379bbf54b4727b43d84ca6a5f0f50":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e2d81f12c01a48759ce67ecba877bcd8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0dc83433ddc47218f2c67df75f6f1f9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5d13b16ffb0c48539289b31109981069":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9cef4799bf340f1b6cc4feb57d5323d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"023f960c501842eab5bdd634fd07595d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f8c72a230ed2482881bfbc8a8d92a372","IPY_MODEL_d6cb53599f394a9d9eee3c072a6f2e1a","IPY_MODEL_1c0e96230d424e89b238c9d05b5b113b"],"layout":"IPY_MODEL_a1f4cc46227e4bd884d729c2c8f8aec2"}},"f8c72a230ed2482881bfbc8a8d92a372":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a0b46dbea6845dabff42864dc15da2a","placeholder":"​","style":"IPY_MODEL_b596ede57dce456a85bac3446c1fc9fe","value":"tokenizer.json: 100%"}},"d6cb53599f394a9d9eee3c072a6f2e1a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eabec300e202417983d52508045ca46b","max":34600820,"min":0,"orientation":"horizontal","style":"IPY_MODEL_64b56ed766f240008eaa1509750ba316","value":34600820}},"1c0e96230d424e89b238c9d05b5b113b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f8611e23a00484fa348d9987ebde646","placeholder":"​","style":"IPY_MODEL_4c93888573cb4271bd170c2641a72fa7","value":" 34.6M/34.6M [00:00&lt;00:00, 61.6MB/s]"}},"a1f4cc46227e4bd884d729c2c8f8aec2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a0b46dbea6845dabff42864dc15da2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b596ede57dce456a85bac3446c1fc9fe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eabec300e202417983d52508045ca46b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64b56ed766f240008eaa1509750ba316":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5f8611e23a00484fa348d9987ebde646":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c93888573cb4271bd170c2641a72fa7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2e59a4c19fe74c5b9ee1ebfdcfb86064":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_450901ec5ecc4f83bd7f7f1cb79d2245","IPY_MODEL_924f3c77d4014601a22fe52043cad905","IPY_MODEL_59ce7baf6c8d45b8bcf0d7a4aad0808e"],"layout":"IPY_MODEL_9dc60cb90d664c0d9910c8dc5997907d"}},"450901ec5ecc4f83bd7f7f1cb79d2245":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e86eecca01e04755a8c2cfd5872de321","placeholder":"​","style":"IPY_MODEL_79aabbf8ab514eabb217f673e47207cf","value":"special_tokens_map.json: 100%"}},"924f3c77d4014601a22fe52043cad905":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7f9a12c4a5043c1bf72f038e7e290b8","max":733,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9a7b8d618b0142c1bd7b6c78ecffb2d6","value":733}},"59ce7baf6c8d45b8bcf0d7a4aad0808e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6eac381e23994808b0e0845bc8dd9727","placeholder":"​","style":"IPY_MODEL_0d9bb1a2660c42baa86089e26c4dfe3b","value":" 733/733 [00:00&lt;00:00, 24.0kB/s]"}},"9dc60cb90d664c0d9910c8dc5997907d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e86eecca01e04755a8c2cfd5872de321":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79aabbf8ab514eabb217f673e47207cf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7f9a12c4a5043c1bf72f038e7e290b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a7b8d618b0142c1bd7b6c78ecffb2d6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6eac381e23994808b0e0845bc8dd9727":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d9bb1a2660c42baa86089e26c4dfe3b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"raJvG94FaXGm"},"outputs":[],"source":["import torch\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from dataclasses import dataclass\n","import matplotlib.pyplot as plt\n","from typing import Optional"]},{"cell_type":"code","source":["!huggingface-cli login --token <your huggingface generated token here>"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZuSMa5oKkupg","outputId":"af0559b2-63d3-451a-c9e5-76b1af3b7cec","executionInfo":{"status":"ok","timestamp":1744558801685,"user_tz":240,"elapsed":2366,"user":{"displayName":"abcd demo","userId":"13372324590583046209"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: read).\n","The token `number1` has been saved to /root/.cache/huggingface/stored_tokens\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful.\n","The current active token is: `number1`\n"]}]},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","from transformers import PaliGemmaForConditionalGeneration, PaliGemmaConfig, SiglipVisionConfig, GemmaConfig\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"google/paligemma2-3b-pt-224\")\n","vision_config = SiglipVisionConfig(\n","    image_size=224,\n","    patch_size=14,\n","    num_hidden_layers=27,\n","    num_attention_heads=16,\n","    hidden_size=1152,\n","    intermediate_size=4096,\n","    vocab_size=257152,\n","    vision_use_head=False\n",")\n","\n","# Initialize Gemma text configuration\n","text_config = GemmaConfig(\n","    hidden_size=2048,\n","    num_hidden_layers=18,\n","    intermediate_size=16384,\n","    num_attention_heads=8,\n","    num_key_value_heads=1,\n","    is_encoder_decoder=False,\n","    vocab_size=257152\n",")\n","\n","paliConfig = PaliGemmaConfig(\n","    vision_config=vision_config.to_dict(),\n","    text_config=text_config.to_dict(),\n","    projection_dim=2048,\n","    hidden_size=2048\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":237,"referenced_widgets":["f0071ae3b47f4b45b446e39be644683f","5ef98d9cb2914fdea8005e986349b991","03ab093ed6404bf0896b4f828f5d7cec","89dcbfc592534819bac77a3b8c0dbf17","e6d305f17907467abc3317ecceacb265","9b185ec411524166bb328f7f28d19c22","6c1379bbf54b4727b43d84ca6a5f0f50","e2d81f12c01a48759ce67ecba877bcd8","b0dc83433ddc47218f2c67df75f6f1f9","5d13b16ffb0c48539289b31109981069","b9cef4799bf340f1b6cc4feb57d5323d","023f960c501842eab5bdd634fd07595d","f8c72a230ed2482881bfbc8a8d92a372","d6cb53599f394a9d9eee3c072a6f2e1a","1c0e96230d424e89b238c9d05b5b113b","a1f4cc46227e4bd884d729c2c8f8aec2","5a0b46dbea6845dabff42864dc15da2a","b596ede57dce456a85bac3446c1fc9fe","eabec300e202417983d52508045ca46b","64b56ed766f240008eaa1509750ba316","5f8611e23a00484fa348d9987ebde646","4c93888573cb4271bd170c2641a72fa7","2e59a4c19fe74c5b9ee1ebfdcfb86064","450901ec5ecc4f83bd7f7f1cb79d2245","924f3c77d4014601a22fe52043cad905","59ce7baf6c8d45b8bcf0d7a4aad0808e","9dc60cb90d664c0d9910c8dc5997907d","e86eecca01e04755a8c2cfd5872de321","79aabbf8ab514eabb217f673e47207cf","f7f9a12c4a5043c1bf72f038e7e290b8","9a7b8d618b0142c1bd7b6c78ecffb2d6","6eac381e23994808b0e0845bc8dd9727","0d9bb1a2660c42baa86089e26c4dfe3b"]},"id":"w8PWXfe5kzVw","outputId":"6d348bcd-b048-4925-8e4f-b4657022ac8a","executionInfo":{"status":"ok","timestamp":1744558834464,"user_tz":240,"elapsed":32783,"user":{"displayName":"abcd demo","userId":"13372324590583046209"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/243k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0071ae3b47f4b45b446e39be644683f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/34.6M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"023f960c501842eab5bdd634fd07595d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/733 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e59a4c19fe74c5b9ee1ebfdcfb86064"}},"metadata":{}}]},{"cell_type":"code","source":["@dataclass\n","class Configs:\n","  patchSize:int = 16\n","  embeddingChannels:int = 32\n","  batchSize: int = 2\n","  layerNormEps: int = 1e-5\n","  intermediateEmbedding :int = 32 * 4\n","  numLayers: int = 6\n","  numHeads: int = 4\n","  dropoutRate:float = 0.5\n","  training: bool = True\n","  imageDim:int = 28\n","  device:str = 'cuda'\n","\n","class SiglipVisionEmbedding(nn.Module):\n","\n","  def __init__(self, config: Configs, layerNormEps: int = 1e-5, attentionDropout: int = 0.1, numImageTokens: int = 0):\n","    super().__init__()\n","\n","    self.patchConv = nn.Conv2d(3, config.embeddingChannels,  config.patchSize, config.patchSize) #assuming rgb image\n","    self.numPositions = torch.arange((config.imageDim // config.patchSize)** 2).to(device = config.device) #after flattening, this is how many tokens to attach positional embedding to\n","    self.positionEmbeddings = nn.Embedding(self.numPositions.shape[0], config.embeddingChannels) #just embedding the index of patch\n","  def forward(self, x):\n","    x = self.patchConv(x)\n","    x = torch.flatten(x, 2) #B, C, H * W\n","    x = x.transpose(1, 2) #B, H * W, C (because we want a sequence of embeddings listed in order)\n","    x = self.positionEmbeddings(self.numPositions) + x\n","\n","    return x\n","\n","class SiglipFeedForward(nn.Module):\n","  def __init__(self, config: Configs):\n","    super().__init__()\n","    self.fc1 = nn.Linear(config.embeddingChannels, config.intermediateEmbedding)\n","    # self.activation = F.gelu(approximate = 'tanh')\n","    self.fc2 = nn.Linear(config.intermediateEmbedding, config.embeddingChannels)\n","\n","  def forward(self, x):\n","    x = self.fc1(x)\n","    x = self.fc2(F.gelu(x, approximate = 'tanh'))\n","    return x\n","\n","class SiglipSelfAttention(nn.Module):\n","  def __init__(self, config: Configs):\n","    super().__init__()\n","    self.config = config\n","    self.training = config.training\n","    self.wq = nn.Linear(config.embeddingChannels, config.embeddingChannels)\n","    self.wk = nn.Linear(config.embeddingChannels, config.embeddingChannels)\n","    self.wv = nn.Linear(config.embeddingChannels, config.embeddingChannels)\n","    self.wo = nn.Linear(config.embeddingChannels, config.embeddingChannels)\n","\n","  def forward(self, x):\n","\n","    config = self.config\n","\n","    if len(x.shape) == 2:\n","      seqLength = 1 #kvcache make seqLength = 1 always\n","      batchSize, numFeatures = x.shape\n","    else:\n","      batchSize,seqLength, numFeatures = x.shape\n","\n","    xq = self.wq(x)\n","    xk = self.wk(x)\n","    xv = self.wv(x)\n","\n","    # B, seqLength, embeddingDim > B, numHeads, seqLength, dimPerHead\n","    xq = xq.view(batchSize, seqLength, config.numHeads, numFeatures // config.numHeads).transpose(1,2)\n","    xk = xk.view(batchSize, seqLength, config.numHeads, numFeatures // config.numHeads).transpose(1,2)\n","    xv = xv.view(batchSize, seqLength, config.numHeads, numFeatures // config.numHeads).transpose(1,2)\n","\n","    attWeights = torch.matmul(xq, xk.transpose(2,3)) / torch.sqrt(torch.tensor(numFeatures // config.numHeads)) # xq matmul xk.T > B, numHeads, seqLength, seqLength\n","    attWeights = F.dropout(attWeights, p = self.config.dropoutRate, training = self.training)\n","    attWeights = F.softmax(attWeights, dim = -1)\n","\n","    o = torch.matmul(attWeights, xv).transpose(1,2) # B, seqLength, numHeads, dimPerHead\n","    # using reshape instead of view because of some memory issue that might occur\n","    o = o.reshape(batchSize, seqLength, numFeatures)# B, seqLength, totalDims\n","\n","    return self.wo(o.contiguous())\n","\n","class SiglipVisionEncoder(nn.Module):\n","  def __init__(self, config: Configs):\n","    super().__init__()\n","    self.embedDim = config.embeddingChannels\n","    self.norm1 = nn.LayerNorm(self.embedDim, eps = config.layerNormEps)\n","    self.att = SiglipSelfAttention(config)\n","    self.norm2 = nn.LayerNorm(self.embedDim, eps = config.layerNormEps)\n","    self.FF = SiglipFeedForward(config)\n","\n","  def forward(self, x):\n","    x = self.norm1(x)\n","    x = self.att(x)\n","    x = x + self.norm2(x)\n","    x = self.FF(x)\n","    return x\n","\n","class SiglipTransformer(nn.Module):\n","  def __init__(self, config: Configs):\n","    super().__init__()\n","    self.embed = SiglipVisionEmbedding(config) #nn.Embedding(config.vocabSize, config.embeddingChannels)\n","    self.layers = nn.ModuleList([SiglipVisionEncoder(config) for l in range(config.numLayers)])\n","    self.normFinal = nn.LayerNorm(config.embeddingChannels, eps = config.layerNormEps)\n","\n","  def forward(self, x):\n","    x = self.embed(x)\n","\n","    for l in self.layers:\n","      x = l(x)\n","\n","    x = self.normFinal(x)\n","\n","    return x"],"metadata":{"id":"t9R6eO1pa76h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@dataclass\n","class TextConfigs:\n","  vocabSize:int = 0\n","  visionEmbeddingChannels:int = 0\n","  textEmbeddingChannels:int = 0\n","  batchSize: int = 2\n","  rmsNormEps: int = 1e-5\n","  intermediateEmbedding :int = 32 * 4\n","  numLayers: int = 6\n","  numHeads: int = 4\n","  dropoutRate:float = 0.5\n","  training:bool = True\n","  base: int = 1000\n","  batchSize:int  = 8\n","  maxSeqLength:int = 32\n","  dropoutRate:float = 0.5\n","  kvCache: bool = False\n","  device:str = 'cpu'"],"metadata":{"id":"GggeK5ycWIt6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MultiModalProjector(nn.Module):\n","  def __init__(self, config: TextConfigs):\n","    super().__init__()\n","    self.project = nn.Linear(config.visionEmbeddingChannels, config.textEmbeddingChannels)\n","\n","  def forward(self, x):\n","    return self.project(x)"],"metadata":{"id":"s1v1dq_kV-DF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class RotaryEmbed(nn.Module):\n","  def __init__(self, config: TextConfigs):\n","    super().__init__()\n","    base = 10000\n","    self.freqs = 1.0 / (torch.pow(base, torch.arange(0, config.textEmbeddingChannels // config.numHeads, 2) / 2)).to(device = config.device)\n","\n","  #======\n","\n","  def rotate_half(self, x):\n","      x1 = x[..., : x.shape[-1] // 2] # Takes the first half of the last dimension\n","      x2 = x[..., x.shape[-1] // 2 :] # Takes the second half of the last dimension\n","      #not exactly the way it is in the paper as this simply gives something like the [-x_n, -x_n-1, ..., x_n/2 - 1, x_n/2], instead of the formula in the paper\n","      return torch.cat((-x2, x1), dim=-1)\n","\n","\n","  def apply_rotary_pos_emb(self, q, k, cos, sin, unsqueeze_dim=1):\n","      cos = cos.unsqueeze(unsqueeze_dim) # Add the head dimension\n","      sin = sin.unsqueeze(unsqueeze_dim) # Add the head dimension\n","      # Apply the formula (34) of the Rotary Positional Encoding paper.\n","      q_embed = (q * cos) + (self.rotate_half(q) * sin) #[[cos, -sin], [sin, cos]]\n","      k_embed = (k * cos) + (self.rotate_half(k) * sin)\n","      return q_embed, k_embed\n","\n","  #=======\n","\n","  def forward(self, positionIds, q, k):\n","    freqs = self.freqs[None, :, None].expand(positionIds.shape[0], -1, 1).float() #expand to number of positionIds (only need 1 if kv_cache being used)\n","    # freqs = self.freqs[None, :, None].expand(1, -1, 1).float() #expand to number of positionIds (only need 1 because of kv_cache being used)\n","    positionIds = positionIds[:,None,:].float()\n","    emb = (freqs @ positionIds).transpose(1, 2) #B, SeqLen, headDim//2\n","    emb = torch.cat((emb, emb), dim = -1)\n","    qPosEmb, kPosEmb = self.apply_rotary_pos_emb(q, k, emb.cos(), emb.sin(), unsqueeze_dim=1)\n","    return qPosEmb, kPosEmb\n","\n"],"metadata":{"id":"PHsNuFqT2UO4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class GemmaFeedForward(nn.Module):\n","    def __init__(self, config: TextConfigs):\n","        super().__init__()\n","        self.config = config\n","        self.inputSize = config.textEmbeddingChannels\n","        self.intermediateSize = config.intermediateEmbedding\n","        self.gate_proj = nn.Linear(self.inputSize, self.intermediateSize, bias=False)\n","        self.up_proj = nn.Linear(self.inputSize, self.intermediateSize, bias=False)\n","        self.down_proj = nn.Linear(self.intermediateSize, self.inputSize, bias=False)\n","\n","    def forward(self, x):\n","        return self.down_proj(F.gelu(self.gate_proj(x), approximate=\"tanh\") * self.up_proj(x))"],"metadata":{"id":"-oO2NswV2cOO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SelfAttentionText(nn.Module):\n","  def __init__(self, config: TextConfigs):\n","    super().__init__()\n","    self.config = config\n","    self.wq = nn.Linear(config.textEmbeddingChannels, config.textEmbeddingChannels)\n","    self.wk = nn.Linear(config.textEmbeddingChannels, config.textEmbeddingChannels)\n","    self.wv = nn.Linear(config.textEmbeddingChannels, config.textEmbeddingChannels)\n","    self.wo = nn.Linear(config.textEmbeddingChannels, config.textEmbeddingChannels)\n","    self.rotaryEmb = RotaryEmbed(config)\n","    if config.kvCache:\n","      self.kCache = torch.zeros((config.batchSize, config.numHeads, config.maxSeqLength, config.textEmbeddingChannels // config.numHeads)).to(devive = config.device)\n","      self.vCache = torch.zeros((config.batchSize, config.numHeads, config.maxSeqLength, config.textEmbeddingChannels // config.numHeads)).to(devive = config.device)\n","\n","  def forward(self, x, position_ids, currPositionInSeq = 0, attMask=None, kvCache = False):\n","\n","    if len(x.shape) == 2:\n","      seqLength = 1 #kvcache make seqLength = 1 always\n","      batchSize, features = x.shape\n","    else:\n","      batchSize,seqLength, features = x.shape\n","\n","    assert features % self.config.numHeads == 0\n","\n","    q = self.wq(x).view(batchSize, seqLength, self.config.numHeads, features // self.config.numHeads).transpose(1, 2) #b, h, l, hd\n","    k = self.wk(x).view(batchSize, seqLength, self.config.numHeads, features // self.config.numHeads).transpose(1, 2) #b, h, l, hd\n","    v = self.wv(x).view(batchSize, seqLength, self.config.numHeads, features // self.config.numHeads).transpose(1, 2) #b, h, l, hd\n","\n","    if not self.config.kvCache:\n","      q, k = self.rotaryEmb(position_ids , q, k)\n","      attMatrix = torch.matmul(q, k.transpose(2, 3)) #b, h, l, l\n","      #dont really need mask for this application because there is no way to access future tokens in a given multimodal input\n","      #attMatrix = attMatrix + attMask\n","\n","      out = torch.softmax(attMatrix, dim = -1)\n","      attMatrix = F.dropout(attMatrix, p = self.config.dropoutRate, training = self.config.training)\n","      out = torch.matmul(out, v).transpose(1,2)\n","\n","      out = out.reshape(batchSize, seqLength, features) #had to change it from out.view because it might cause memory issues due to not being contiguous\n","      out = self.wo(out)\n","    else:\n","      q, k = self.rotaryEmb(torch.tensor([currPositionInSeq], dtype = torch.uint16).unsqueeze(0) , q, k) #used torch.Tensor casting just to forego having to use the positionIds tensor\n","      maxCurrTokens = k.shape[2]\n","      self.kCache.detach() #detaching because gives error like (RuntimeError: Trying to backward through the graph a second time.. ) Not sure how to use kv cache while training, seems to be for inference only\n","      self.vCache.detach()\n","      self.kCache[:,:,currPositionInSeq + seqLength: maxCurrTokens,:] = k.clone().detach() #limit the kv_cache to the shape of k tokens (we initialized the kv cache without knowing the actual input)\n","      self.vCache[:,:,currPositionInSeq + seqLength: maxCurrTokens,:] = v.clone().detach()\n","      attMatrix = torch.matmul(q, self.kCache[:,:,:currPositionInSeq + seqLength: k.shape[2],:].transpose(2, 3)) #b, h, l, l\n","      #dont really need mask for this application because there is no way to access future tokens in a given multimodal input\n","      #attMatrix = attMatrix + attMask\n","\n","      out = torch.softmax(attMatrix, dim = -1)\n","      attMatrix = F.dropout(attMatrix, p = self.config.dropoutRate, training = self.config.training)\n","      out = torch.matmul(out, self.vCache[:,:,:currPositionInSeq + seqLength:maxCurrTokens,:]).transpose(1,2)\n","\n","\n","    return out.squeeze(1) #kv cache seqLength = 1\n"],"metadata":{"id":"1oqrPGbI487h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TextDecoder(nn.Module):\n","  def __init__(self, config: TextConfigs):\n","    super().__init__()\n","    self.att = SelfAttentionText(config)\n","    self.norm1 = nn.RMSNorm( config.textEmbeddingChannels, eps = config.rmsNormEps) #batchSize, seqLength = 1, features\n","    self.FF = GemmaFeedForward(config)\n","    self.norm2 = nn.RMSNorm(config.textEmbeddingChannels, eps = config.rmsNormEps)\n","\n","  def forward(self, x, currPos):\n","    x = self.att(x,currPos) + self.norm1(x)\n","    x = self.FF(x) + self.norm2(x)\n","    return x\n","\n","class MultiModalTransformer(nn.Module):\n","  def __init__(self, config: TextConfigs):\n","    super().__init__()\n","    self.decoderLayers = nn.ModuleList([TextDecoder(config) for layer_idx in range(config.numLayers)])\n","    self.normFinal = nn.RMSNorm( config.textEmbeddingChannels, eps = config.rmsNormEps)\n","    self.logits = nn.Linear(config.textEmbeddingChannels, config.vocabSize)\n","\n","  def forward(self, attention_mask,position_ids,multiModalToLogits,kv_cache=None): #miniature kv cache is implemented in the function itself\n","    for nextLayer in self.decoderLayers:\n","      if kv_cache is None:\n","        multiModalToLogits = nextLayer(multiModalToLogits, position_ids)\n","      else:\n","        seqLen = position_ids.shape[0]\n","        currPositionInSeq = 0\n","        while currPositionInSeq < seqLen: #for kv_cache usage\n","          multiModalToLogits[:, currPositionInSeq, :] = nextLayer(multiModalToLogits[:, currPositionInSeq, :].clone(), position_ids, currPositionInSeq)\n","          currPositionInSeq += 1\n","\n","    multiModalToLogits = self.normFinal(multiModalToLogits)\n","    multiModalToLogits = self.logits(multiModalToLogits)\n","\n","    return multiModalToLogits\n"],"metadata":{"id":"ZFclm6dVujlp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MultiModalPipeline(nn.Module):\n","  def __init__(self, textConfigs: TextConfigs, visionConfigs: Configs, paliConfig: PaliGemmaConfig, tokenizer: AutoTokenizer):\n","    super().__init__()\n","    self.textConfigs = textConfigs\n","    self.embed = nn.Embedding(textConfigs.vocabSize, textConfigs.textEmbeddingChannels).to(device=textConfigs.device)\n","    self.imageComponent = SiglipTransformer(visionConfigs).to(device=textConfigs.device)\n","    self.projected = MultiModalProjector(textConfigs).to(device=textConfigs.device)\n","    self.logits = MultiModalTransformer(textConfigs).to(device=textConfigs.device)\n","    self.config = paliConfig\n","    self.tokenizer = tokenizer\n","  def _merge_input_ids_with_image_features(self, image_features: torch.Tensor, inputs_embeds: torch.Tensor, input_ids: torch.Tensor, attention_mask: torch.Tensor, kv_cache = None):\n","        _, _, embed_dim = image_features.shape\n","        batch_size, sequence_length = input_ids.shape\n","        dtype, device = inputs_embeds.dtype, inputs_embeds.device\n","        # Shape: [Batch_Size, Seq_Len, Hidden_Size]\n","        scaled_image_features = image_features / (self.config.hidden_size**0.5)\n","\n","        # Combine the embeddings of the image tokens, the text tokens and mask out all the padding tokens.\n","        final_embedding = torch.zeros(batch_size, sequence_length, embed_dim, dtype=inputs_embeds.dtype, device=inputs_embeds.device)\n","        # Shape: [Batch_Size, Seq_Len]. whereever it's not an image or padding\n","        text_mask = (input_ids != self.config.image_token_index) & (input_ids != self.tokenizer.pad_token_id)\n","        # Shape: [Batch_Size, Seq_Len]. whereever input_ids = image_token_index\n","        image_mask = input_ids == self.config.image_token_index\n","        # Shape: [Batch_Size, Seq_Len]. whereever it's padding\n","        pad_mask = input_ids == self.tokenizer.pad_token_id #tokenizer.pad_token_type_id\n","\n","        # We need to expand the masks to the embedding dimension otherwise we can't use them in torch.where\n","        text_mask_expanded = text_mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n","        pad_mask_expanded = pad_mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n","        image_mask_expanded = image_mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n","\n","        # Add the text embeddings\n","        final_embedding = torch.where( text_mask_expanded, inputs_embeds, final_embedding) #select inputs_embeds where text_mask_expanded is 1\n","        # Insert image embeddings. We can't use torch.where because the sequence length of scaled_image_features is not equal to the sequence length of the final embedding\n","        final_embedding = final_embedding.masked_scatter(image_mask_expanded, scaled_image_features)\n","        # Zero out padding tokens\n","        final_embedding = torch.where(pad_mask_expanded, torch.zeros_like(final_embedding), final_embedding)\n","\n","        #### CREATE THE ATTENTION MASK ####\n","\n","        dtype, device = inputs_embeds.dtype, inputs_embeds.device\n","        min_dtype = torch.finfo(dtype).min\n","        q_len = inputs_embeds.shape[1]\n","\n","        if kv_cache is None or kv_cache.num_items() == 0:\n","            # Do not mask any token, because we're in the prefill phase\n","            # This only works when we have no padding\n","            causal_mask = torch.full(\n","                (batch_size, q_len, q_len), fill_value=0, dtype=dtype, device=device\n","            )\n","        else:\n","            # Since we are generating tokens, the query must be one single token\n","            assert q_len == 1\n","            kv_len = kv_cache.num_items() + q_len\n","            # Also in this case we don't need to mask anything, since each query should be able to attend all previous tokens.\n","            # This only works when we have no padding\n","            causal_mask = torch.full(\n","                (batch_size, q_len, kv_len), fill_value=0, dtype=dtype, device=device\n","            )\n","\n","        # Add the head dimension\n","        # [Batch_Size, Q_Len, KV_Len] -> [Batch_Size, Num_Heads_Q, Q_Len, KV_Len]\n","        causal_mask = causal_mask.unsqueeze(1)\n","\n","        if kv_cache is not None and kv_cache.num_items() > 0:\n","            # The position of the query is just the last position\n","            position_ids = attention_mask.cumsum(-1)[:, -1]\n","            if position_ids.dim() == 1:\n","                position_ids = position_ids.unsqueeze(0)\n","        else:\n","            # Create a position_ids based on the size of the attention_mask\n","            # For masked tokens, use the number 1 as position.\n","            position_ids = (attention_mask.cumsum(-1)).masked_fill_((attention_mask == 0), 1).to(device)\n","\n","        return final_embedding, causal_mask, position_ids\n","\n","  def forward(self, input_ids: torch.LongTensor = None,pixel_values: torch.FloatTensor = None,attention_mask: Optional[torch.Tensor] = None,kv_cache = None, labels = None):\n","    inputs_embeds = self.embed(input_ids)\n","    h = self.imageComponent(pixel_values)\n","    h = self.projected(h)\n","    inputs_embeds, attention_mask, position_ids = self._merge_input_ids_with_image_features(h, inputs_embeds, input_ids, attention_mask, kv_cache)\n","\n","    finalLogits = self.logits(attention_mask=attention_mask,position_ids=position_ids,multiModalToLogits=inputs_embeds,kv_cache=kv_cache)\n","    # pred = F.softmax(finalLogits[:,-labels.shape[0]:,:], dim = -1).squeeze(0) #[seqLength, features] (1st dim was batch, but batch is always 1 in this case)\n","    finalLogits = finalLogits[:, -1]\n","    labels = labels.reshape(labels.shape[0] * labels.shape[1])\n","    # pred = F.softmax(finalLogits.reshape(finalLogits.shape[0] * finalLogits.shape[1], self.textConfigs.vocabSize), dim = -1) #[seqLength, features]\n","    pred = finalLogits #.reshape(finalLogits.shape[0], self.textConfigs.vocabSize) #[seqLength, features]\n","\n","    # labels = torch.cat([labels, torch.zeros(finalLogits.shape[1] - 1, dtype = torch.long, device = self.textConfigs.device)], dim = 0)\n","    # pred = F.softmax(finalLogits[:,:,:], dim = -1).squeeze(0) #[seqLength, features] (1st dim was batch, but batch is always 1 in this case)\n","\n","    loss = F.cross_entropy(pred, labels)\n","\n","    return pred, loss\n"],"metadata":{"id":"TzlszVJ83FKh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import Dict, List, Optional, Union, Tuple, Iterable\n","import numpy as np\n","from PIL import Image\n","import torch\n","\n","IMAGENET_STANDARD_MEAN = [0.5, 0.5, 0.5]\n","IMAGENET_STANDARD_STD = [0.5, 0.5, 0.5]\n","\n","def add_image_tokens_to_prompt(prefix_prompt, bos_token, image_seq_len, image_token):\n","    # Quoting from the blog (https://huggingface.co/blog/paligemma#detailed-inference-process):\n","    #   The input text is tokenized normally.\n","    #   A <bos> token is added at the beginning, and an additional newline token (\\n) is appended.\n","    #   This newline token is an essential part of the input prompt the model was trained with, so adding it explicitly ensures it's always there.\n","    #   The tokenized text is also prefixed with a fixed number of <image> tokens.\n","    # NOTE: from the paper it looks like the `\\n` should be tokenized separately, but in the HF implementation this is not done.\n","    #       ref to HF implementation: https://github.com/huggingface/transformers/blob/7f79a97399bb52aad8460e1da2f36577d5dccfed/src/transformers/models/paligemma/processing_paligemma.py#L55-L73\n","    return f\"{image_token * image_seq_len}{bos_token}{prefix_prompt}\\n\"\n","\n","def rescale(\n","    image: np.ndarray, scale: float, dtype: np.dtype = np.float32\n",") -> np.ndarray:\n","    rescaled_image = image * scale\n","    rescaled_image = rescaled_image.astype(dtype)\n","    return rescaled_image\n","\n","def resize(\n","    image: Image,\n","    size: Tuple[int, int],\n","    resample: Image.Resampling = None,\n","    reducing_gap: Optional[int] = None,\n",") -> np.ndarray:\n","    height, width = size\n","    resized_image = image.resize(\n","        (width, height), resample=resample, reducing_gap=reducing_gap\n","    )\n","    return resized_image\n","\n","def normalize(\n","    image: np.ndarray,\n","    mean: Union[float, Iterable[float]],\n","    std: Union[float, Iterable[float]],\n",") -> np.ndarray:\n","    mean = np.array(mean, dtype=image.dtype)\n","    std = np.array(std, dtype=image.dtype)\n","    image = (image - mean) / std\n","    return image\n","\n","def process_images(\n","    images: List[Image.Image],\n","    size: Dict[str, int] = None,\n","    resample: Image.Resampling = None,\n","    rescale_factor: float = None,\n","    image_mean: Optional[Union[float, List[float]]] = None,\n","    image_std: Optional[Union[float, List[float]]] = None,\n",") -> List[np.ndarray]:\n","    height, width = size[0], size[1]\n","    images = [\n","        resize(image=image, size=(height, width), resample=resample) for image in images\n","    ]\n","    # Convert each image to a numpy array\n","    images = [np.array(image) for image in images]\n","    # Rescale the pixel values to be in the range [0, 1]\n","    images = [rescale(image, scale=rescale_factor) for image in images]\n","    # Normalize the images to have mean 0 and standard deviation 1\n","    images = [normalize(image, mean=image_mean, std=image_std) for image in images]\n","    # Move the channel dimension to the first dimension. The model expects images in the format [Channel, Height, Width]\n","    images = [image.transpose(2, 0, 1) for image in images]\n","    return images"],"metadata":{"id":"E0Jtnky7AT7v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PaliGemmaProcessor:\n","\n","    IMAGE_TOKEN = \"<image>\"\n","\n","    def __init__(self, tokenizer, num_image_tokens: int, image_size: int):\n","        super().__init__()\n","\n","        self.image_seq_length = num_image_tokens\n","        self.image_size = image_size\n","\n","        # Tokenizer described here: https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/paligemma/README.md#tokenizer\n","        tokens_to_add = {\"additional_special_tokens\": [self.IMAGE_TOKEN]}\n","        tokenizer.add_special_tokens(tokens_to_add)\n","        EXTRA_TOKENS = [\n","            f\"<loc{i:04d}>\" for i in range(1024)\n","        ]  # These tokens are used for object detection (bounding boxes)\n","        EXTRA_TOKENS += [\n","            f\"<seg{i:03d}>\" for i in range(128)\n","        ]  # These tokens are used for object segmentation\n","        tokenizer.add_tokens(EXTRA_TOKENS)\n","        self.image_token_id = tokenizer.convert_tokens_to_ids(self.IMAGE_TOKEN)\n","        # We will add the BOS and EOS tokens ourselves\n","        tokenizer.add_bos_token = False\n","        tokenizer.add_eos_token = False\n","\n","        self.tokenizer = tokenizer\n","\n","    def __call__(\n","        self,\n","        text: List[str],\n","        images: List[Image.Image],\n","        padding: str = \"longest\",\n","        truncation: bool = True,\n","    ) -> dict:\n","        # assert len(images) == 1 and len(text) == 1, f\"Received {len(images)} images for {len(text)} prompts.\"\n","\n","        pixel_values = process_images(\n","            images,\n","            size=(self.image_size, self.image_size),\n","            resample=Image.Resampling.BICUBIC,\n","            rescale_factor=1 / 255.0,\n","            image_mean=IMAGENET_STANDARD_MEAN,\n","            image_std=IMAGENET_STANDARD_STD,\n","        )\n","        # Convert the list of numpy arrays to a single numpy array with shape [Batch_Size, Channel, Height, Width]\n","        pixel_values = np.stack(pixel_values, axis=0)\n","        # Convert the numpy array to a PyTorch tensor\n","        pixel_values = torch.tensor(pixel_values)\n","\n","        # Prepend a `self.image_seq_length` number of image tokens to the prompt\n","        input_strings = [\n","            add_image_tokens_to_prompt(\n","                prefix_prompt=prompt,\n","                bos_token=self.tokenizer.bos_token,\n","                image_seq_len=self.image_seq_length,\n","                image_token=self.IMAGE_TOKEN,\n","            )\n","            for prompt in text\n","        ]\n","\n","        # Returns the input_ids and attention_mask as PyTorch tensors\n","        inputs = self.tokenizer(\n","            input_strings,\n","            return_tensors=\"pt\",\n","            padding = padding,\n","            truncation=truncation,\n","        )\n","\n","        return_data = {\"pixel_values\": pixel_values, **inputs}\n","\n","        return return_data"],"metadata":{"id":"WrMJc_NM9s2C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torchvision\n","from torchvision import transforms\n","\n","transform = transforms.Compose([\n","    # transforms.Resize((256, 256)),  # Resize if needed\n","    transforms.Grayscale(num_output_channels=3),  # Ensure 3 channels (if needed)\n","    transforms.ToTensor(),  # Convert to tensor\n","])\n","\n","to_pil = transforms.ToPILImage()\n","\n","gBatchSize = 1\n","gDevice = 'cuda'\n","train_data = torchvision.datasets.MNIST(\"./\", train=True, transform=transform, download=True)\n","test_data_xy = torchvision.datasets.MNIST(\"./\", train=False, transform=transform, download=True)\n","\n","batch_size = gBatchSize\n","trainLoader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n","testLoader = torch.utils.data.DataLoader(test_data_xy, batch_size=batch_size, shuffle=False, num_workers=2)\n","len(trainLoader)\n","\n","imageDim = 28\n","\n","iConf = Configs #vision configs\n","iConf.embeddingChannels = 4\n","iConf.batchSize = gBatchSize\n","iConf.patchSize = 1\n","iConf.numLayers = 1\n","iConf.device = gDevice\n","\n","tConf = TextConfigs\n","tConf.batchSize = gBatchSize\n","tConf.vocabSize = tokenizer.vocab_size + 1\n","tConf.visionEmbeddingChannels = iConf.embeddingChannels #this is so we just need to pass one config to the multiModalProjector\n","tConf.textEmbeddingChannels = 512 #1024\n","tConf.numLayers = 2\n","tConf.device = gDevice\n","\n","processing = PaliGemmaProcessor(tokenizer, imageDim * imageDim, imageDim)\n","\n","model1 = MultiModalPipeline(tConf, iConf, paliConfig, tokenizer).to(device=gDevice)\n","\n","# prof = FlopsProfiler(model1) ###\n","\n","prompt = ['Which digit is this?']\n","batchNumber = 0\n","\n","optimizer = torch.optim.Adam(model1.parameters(), lr=1e-5) #make optimizer for the decoder block\n","torch.autograd.set_detect_anomaly(True)\n","epochs = 10\n","total_loss = []\n","\n","# Assuming your dataset is defined as trainDataset\n","# Define your transformations\n","\n","for epoch in range(epochs):\n","    total_loss = []  # To track the total loss\n","    batchNumber = 0  # To count the batch number\n","\n","    for x, y in trainLoader:\n","        # Preprocessing step\n","        pil_images = [torchvision.transforms.functional.to_pil_image(x[i,...]) for i in range(x.shape[0])]\n","\n","        inputDict = processing(prompt * len(pil_images), pil_images)  # Adjust prompt repetition based on the batch size\n","\n","        # Tokenize the labels and move labels to the device (assuming 'gDevice' is defined)\n","        labels = torch.tensor([tokenizer.encode(f\"The digit is {digit.item()}\") for digit in y], dtype=torch.long).to(device=gDevice)\n","        # labels = torch.tensor([tokenizer.encode(f\"{digit.item()}\") for digit in y], dtype=torch.long)\n","\n","        optimizer.zero_grad()\n","        t = 0\n","        while t < labels.shape[1]:\n","          if t > 0:\n","            inputIds = torch.cat((inputIds.to(device=gDevice), outLogits.to(device=gDevice)), dim = 1)\n","            attMask = torch.cat((attMask.to(device=gDevice), torch.ones(outLogits.shape, dtype=torch.long).to(device=gDevice)), dim = 1)\n","          else:\n","            inputIds = inputDict['input_ids'].to(device=gDevice)\n","            attMask = inputDict['attention_mask'].to(device=gDevice)\n","          #generate 1 token\n","          predictions, loss = model1(\n","              inputIds,\n","              inputDict['pixel_values'].to(device=gDevice),\n","              attMask,\n","              labels=labels.clone()[:,t].unsqueeze(1)\n","          )\n","          outLogits = torch.argmax(predictions.clone().detach(), dim=-1).unsqueeze(1).clone().detach()\n","          if t == 0:\n","            outputString = outLogits.squeeze(1)\n","          else:\n","            outputString = torch.cat((outputString, outLogits.squeeze(1)), dim = 0)\n","\n","          t += 1\n","          # Backpropagation\n","          loss.backward()\n","          optimizer.step()\n","\n","        # Track total loss for the epoch\n","        batchNumber += 1\n","        total_loss.append(loss.item())\n","\n","        # Print out the loss every few batches (optional)\n","        if batchNumber % 5 == 0:\n","            print(f\"Batch {batchNumber}: Loss = {loss.item()}: Sample labels: {y}: Test Output:  {tokenizer.decode(outputString)}\")\n","\n","    # Print average loss after each epoch\n","    print('====', torch.mean(torch.tensor(total_loss)), '=====', 'Sample label: ', y[0].item(),\n","          'Test Output: ', tokenizer.decode(torch.argmax(predictions[0], dim=-1)))\n","x = 0\n","y = 0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NUYv9jrfYMY0","outputId":"1fbf6a55-209d-4a35-ca5a-4f6baca6b006"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Batch 5: Loss = 11.539907455444336: Sample labels: tensor([2]): Test Output:  The DapcontributionsZurück pena\n","Batch 10: Loss = 12.803092956542969: Sample labels: tensor([9]): Test Output:  The dijuallangsungmoderkeydown\n","Batch 15: Loss = 13.182027816772461: Sample labels: tensor([3]): Test Output:  The digitThe digitThe\n","Batch 20: Loss = 12.986420631408691: Sample labels: tensor([3]): Test Output:  The digitThe digitThe\n","Batch 25: Loss = 12.585506439208984: Sample labels: tensor([9]): Test Output:  The digitThe digitThe\n","Batch 30: Loss = 12.913053512573242: Sample labels: tensor([4]): Test Output:  The digit isThe digit\n","Batch 35: Loss = 11.476783752441406: Sample labels: tensor([1]): Test Output:  The digit isThe digit\n","Batch 40: Loss = 11.413060188293457: Sample labels: tensor([7]): Test Output:  The digit isThe digit\n","Batch 45: Loss = 11.430805206298828: Sample labels: tensor([1]): Test Output:  The digit isThe digit\n","Batch 50: Loss = 12.760971069335938: Sample labels: tensor([4]): Test Output:  The digit is The\n","Batch 55: Loss = 12.421852111816406: Sample labels: tensor([4]): Test Output:  The digit is  digit\n","Batch 60: Loss = 12.07867431640625: Sample labels: tensor([8]): Test Output:  The digit is  digit\n","Batch 65: Loss = 11.877609252929688: Sample labels: tensor([7]): Test Output:  The digit is  digit\n","Batch 70: Loss = 11.252182960510254: Sample labels: tensor([8]): Test Output:  The digit is  digit\n","Batch 75: Loss = 10.419319152832031: Sample labels: tensor([2]): Test Output:  The digit is  digit\n","Batch 80: Loss = 11.309686660766602: Sample labels: tensor([0]): Test Output:  The digit is  digit\n","Batch 85: Loss = 10.736367225646973: Sample labels: tensor([1]): Test Output:  The digit is  digit\n","Batch 90: Loss = 11.443181991577148: Sample labels: tensor([5]): Test Output:  The digit is  digit\n","Batch 95: Loss = 10.009292602539062: Sample labels: tensor([0]): Test Output:  The digit is  digit\n","Batch 100: Loss = 10.730864524841309: Sample labels: tensor([6]): Test Output:  The digit is  digit\n","Batch 105: Loss = 9.981569290161133: Sample labels: tensor([8]): Test Output:  The digit is  digit\n","Batch 110: Loss = 10.853816986083984: Sample labels: tensor([4]): Test Output:  The digit is  digit\n","Batch 115: Loss = 11.132808685302734: Sample labels: tensor([3]): Test Output:  The digit is  digit\n","Batch 120: Loss = 11.024259567260742: Sample labels: tensor([5]): Test Output:  The digit is  digit\n","Batch 125: Loss = 9.392021179199219: Sample labels: tensor([0]): Test Output:  The digit is  digit\n","Batch 130: Loss = 10.03463363647461: Sample labels: tensor([4]): Test Output:  The digit is  digit\n","Batch 135: Loss = 9.817523956298828: Sample labels: tensor([1]): Test Output:  The digit is  digit\n","Batch 140: Loss = 9.183507919311523: Sample labels: tensor([6]): Test Output:  The digit is  digit\n","Batch 145: Loss = 9.013826370239258: Sample labels: tensor([1]): Test Output:  The digit is  digit\n","Batch 150: Loss = 8.525379180908203: Sample labels: tensor([1]): Test Output:  The digit is  digit\n","Batch 155: Loss = 9.329906463623047: Sample labels: tensor([8]): Test Output:  The digit is  digit\n","Batch 160: Loss = 8.403053283691406: Sample labels: tensor([1]): Test Output:  The digit is  digit\n","Batch 165: Loss = 8.755166053771973: Sample labels: tensor([7]): Test Output:  The digit is  digit\n","Batch 170: Loss = 10.406953811645508: Sample labels: tensor([5]): Test Output:  The digit is  digit\n","Batch 175: Loss = 8.157654762268066: Sample labels: tensor([2]): Test Output:  The digit is  digit\n","Batch 180: Loss = 8.913719177246094: Sample labels: tensor([5]): Test Output:  The digit is  digit\n","Batch 185: Loss = 7.951016426086426: Sample labels: tensor([2]): Test Output:  The digit is  digit\n","Batch 190: Loss = 9.637706756591797: Sample labels: tensor([9]): Test Output:  The digit is  digit\n","Batch 195: Loss = 9.116368293762207: Sample labels: tensor([8]): Test Output:  The digit is  digit\n","Batch 200: Loss = 8.881505966186523: Sample labels: tensor([8]): Test Output:  The digit is  digit\n","Batch 205: Loss = 8.701276779174805: Sample labels: tensor([6]): Test Output:  The digit is  digit\n","Batch 210: Loss = 7.52236270904541: Sample labels: tensor([2]): Test Output:  The digit is  digit\n","Batch 215: Loss = 7.242208003997803: Sample labels: tensor([1]): Test Output:  The digit is  digit\n","Batch 220: Loss = 8.076445579528809: Sample labels: tensor([7]): Test Output:  The digit is  digit\n","Batch 225: Loss = 9.827760696411133: Sample labels: tensor([3]): Test Output:  The digit is  digit\n","Batch 230: Loss = 7.700558185577393: Sample labels: tensor([5]): Test Output:  The digit is  digit\n","Batch 235: Loss = 7.726538181304932: Sample labels: tensor([7]): Test Output:  The digit is  digit\n","Batch 240: Loss = 8.098995208740234: Sample labels: tensor([6]): Test Output:  The digit is  digit\n","Batch 245: Loss = 7.596124649047852: Sample labels: tensor([5]): Test Output:  The digit is  digit\n","Batch 250: Loss = 9.153326034545898: Sample labels: tensor([4]): Test Output:  The digit is  digit\n","Batch 255: Loss = 7.4094343185424805: Sample labels: tensor([0]): Test Output:  The digit is  digit\n","Batch 260: Loss = 7.25587272644043: Sample labels: tensor([0]): Test Output:  The digit is  digit\n","Batch 265: Loss = 8.547013282775879: Sample labels: tensor([4]): Test Output:  The digit is  digit\n","Batch 270: Loss = 7.872372150421143: Sample labels: tensor([6]): Test Output:  The digit is  digit\n","Batch 275: Loss = 8.461913108825684: Sample labels: tensor([4]): Test Output:  The digit is  digit\n","Batch 280: Loss = 6.5033087730407715: Sample labels: tensor([1]): Test Output:  The digit is  digit\n","Batch 285: Loss = 6.383227825164795: Sample labels: tensor([5]): Test Output:  The digit is  digit\n","Batch 290: Loss = 6.881091117858887: Sample labels: tensor([2]): Test Output:  The digit is  digit\n","Batch 295: Loss = 8.893749237060547: Sample labels: tensor([3]): Test Output:  The digit is  digit\n","Batch 300: Loss = 7.327314376831055: Sample labels: tensor([7]): Test Output:  The digit is  digit\n","Batch 305: Loss = 6.515602111816406: Sample labels: tensor([0]): Test Output:  The digit is  digit\n","Batch 310: Loss = 8.415740966796875: Sample labels: tensor([3]): Test Output:  The digit is  digit\n","Batch 315: Loss = 6.868729591369629: Sample labels: tensor([7]): Test Output:  The digit is  digit\n","Batch 320: Loss = 6.332901954650879: Sample labels: tensor([7]): Test Output:  The digit is  digit\n","Batch 325: Loss = 7.7262420654296875: Sample labels: tensor([9]): Test Output:  The digit is  digit\n","Batch 330: Loss = 6.939918041229248: Sample labels: tensor([8]): Test Output:  The digit is  digit\n","Batch 335: Loss = 6.136754989624023: Sample labels: tensor([1]): Test Output:  The digit is  digit\n","Batch 340: Loss = 7.8345136642456055: Sample labels: tensor([3]): Test Output:  The digit is  digit\n","Batch 345: Loss = 5.960141181945801: Sample labels: tensor([0]): Test Output:  The digit is  digit\n","Batch 350: Loss = 6.944545745849609: Sample labels: tensor([6]): Test Output:  The digit is  digit\n","Batch 355: Loss = 6.102593898773193: Sample labels: tensor([2]): Test Output:  The digit is  digit\n","Batch 360: Loss = 7.794286727905273: Sample labels: tensor([3]): Test Output:  The digit is  digit\n","Batch 365: Loss = 7.549073219299316: Sample labels: tensor([3]): Test Output:  The digit is  digit\n","Batch 370: Loss = 7.078000068664551: Sample labels: tensor([4]): Test Output:  The digit is  digit\n","Batch 375: Loss = 6.431121349334717: Sample labels: tensor([5]): Test Output:  The digit is  digit\n","Batch 380: Loss = 6.351951599121094: Sample labels: tensor([7]): Test Output:  The digit is  digit\n","Batch 385: Loss = 5.45561408996582: Sample labels: tensor([1]): Test Output:  The digit is  digit\n","Batch 390: Loss = 5.628266334533691: Sample labels: tensor([0]): Test Output:  The digit is  digit\n"]}]},{"cell_type":"code","source":["# tokenizer.decode(outputString)\n","outputString.shape, outLogits.shape\n","tokenizer.decode(torch.cat((outputString.squeeze(0), outLogits.squeeze(0)), dim = 0))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"lRZOGkixGVKJ","executionInfo":{"status":"ok","timestamp":1744563915950,"user_tz":240,"elapsed":50,"user":{"displayName":"abcd demo","userId":"13372324590583046209"}},"outputId":"12af4757-5c1b-4f3b-f2a9-d86d383f0957"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'merrymerry'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":124}]},{"cell_type":"code","source":[],"metadata":{"id":"TaQEMm9V4qDj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gc\n","# del predictions\n","# del outLogits\n","# del pil_images\n","# del model1\n","# del x\n","# del y\n","# del labels\n","# del inputDict\n","# del processing\n","# del loss\n","# del total_loss\n","# del optimizer #big chunk of memory here\n","# del trainLoader\n","# del testLoader\n","# del inputIds\n","# del attMask\n","# del train_data\n","# del test_data_xy\n","torch.cuda.empty_cache()\n","gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MwnKhkBp3YVH","executionInfo":{"status":"ok","timestamp":1744565403295,"user_tz":240,"elapsed":438,"user":{"displayName":"abcd demo","userId":"13372324590583046209"}},"outputId":"7e839162-1544-4323-9fdf-a5f991192363"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":131}]},{"cell_type":"code","source":["labels[:,t].unsqueeze(1).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zk-N6Grw6tWC","executionInfo":{"status":"ok","timestamp":1744559645022,"user_tz":240,"elapsed":48,"user":{"displayName":"abcd demo","userId":"13372324590583046209"}},"outputId":"4b7f9d8d-7564-4881-da59-5cc735416e28"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 1])"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","plt.imshow(inputDict['pixel_values'][0][0])"],"metadata":{"id":"0JckW1iostfZ","executionInfo":{"status":"ok","timestamp":1744334533509,"user_tz":240,"elapsed":312,"user":{"displayName":"abcd demo","userId":"13372324590583046209"}},"colab":{"base_uri":"https://localhost:8080/","height":447},"outputId":"ccf75779-816d-45cd-a396-f6a25bb1ae63"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7cc74183ad90>"]},"metadata":{},"execution_count":18},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wRp3yzCLcPyJ","executionInfo":{"status":"ok","timestamp":1744334590355,"user_tz":240,"elapsed":17,"user":{"displayName":"abcd demo","userId":"13372324590583046209"}},"outputId":"f8f603c3-bbc6-4869-ff37-970faa5de9a9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(1.), tensor(-1.), tensor(-0.8255))"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","x[0].shape\n","\n","plt.imshow(x[2].transpose(2,0))"],"metadata":{"id":"ab0u1OwFaubi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ov-fdM5qbGoA","executionInfo":{"status":"ok","timestamp":1744049059194,"user_tz":240,"elapsed":19,"user":{"displayName":"abcd demo","userId":"13372324590583046209"}},"outputId":"edbe9d09-210a-4520-f264-0397c606fb85"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 1, 0])"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["# pip install --upgrade calflops\n","!pip install deepspeed"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BUhvn5ckrHki","executionInfo":{"status":"ok","timestamp":1744058160362,"user_tz":240,"elapsed":20125,"user":{"displayName":"abcd demo","userId":"13372324590583046209"}},"outputId":"48c7927a-11fa-42c8-a12a-2b8d47698196"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting deepspeed\n","  Downloading deepspeed-0.16.5.tar.gz (1.5 MB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from deepspeed) (0.8.1)\n","Collecting hjson (from deepspeed)\n","  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.1.0)\n","Collecting ninja (from deepspeed)\n","  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (24.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from deepspeed) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from deepspeed) (9.0.0)\n","Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.11.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.6.0+cu124)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from deepspeed) (4.67.1)\n","Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.11/dist-packages (from deepspeed) (12.570.86)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (2.33.1)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (4.13.1)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (0.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.18.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->deepspeed) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->deepspeed) (3.0.2)\n","Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: deepspeed\n","  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for deepspeed: filename=deepspeed-0.16.5-py3-none-any.whl size=1580586 sha256=3883d1fa75d9ed88f5ffb38f717f3a02c8fd1288e08e7dc762641affff9eeb24\n","  Stored in directory: /root/.cache/pip/wheels/52/cc/7a/2a649465930859b5b227a30f75d9d19895f1d52f0b071a7486\n","Successfully built deepspeed\n","Installing collected packages: hjson, ninja, deepspeed\n","Successfully installed deepspeed-0.16.5 hjson-3.1.0 ninja-1.11.1.4\n"]}]},{"cell_type":"code","source":["gBatchSize = 4 #planning to implement more batch sizes\n","gDevice = 'cpu'\n","imageDim = 28\n","\n","iConf = Configs #vision configs\n","iConf.embeddingChannels = 512\n","iConf.batchSize = gBatchSize\n","iConf.patchSize = 1\n","iConf.numLayers = 2\n","iConf.device = gDevice\n","\n","tConf = TextConfigs\n","tConf.batchSize = gBatchSize\n","tConf.vocabSize = tokenizer.vocab_size + 1\n","tConf.visionEmbeddingChannels = iConf.embeddingChannels #this is so we just need to pass one config to the multiModalProjector\n","tConf.textEmbeddingChannels = 128\n","tConf.maxSeqLength = 32\n","tConf.numLayers = 1\n","tConf.device = gDevice\n","\n","model1 = MultiModalPipeline(tConf, iConf, paliConfig, tokenizer).to(device=gDevice)"],"metadata":{"id":"OtXEIe1LBp8n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sjCmB2XltPUR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from calflops import calculate_flops\n","\n","\n","\n","flops, macs, params = calculate_flops(model=model1,\n","                                      input_shape=(3,3,28,28),\n","                                      output_as_string=True,\n","                                      output_precision=4)\n","\n","flops, macs, params"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ii3Q5euLrXuo","executionInfo":{"status":"error","timestamp":1744053828036,"user_tz":240,"elapsed":191,"user":{"displayName":"abcd demo","userId":"13372324590583046209"}},"outputId":"357d26f0-6f99-474e-c657-de45088dde0f"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-c6fa7749bfb5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m flops, macs, params = calculate_flops(model=model1, \n\u001b[0m\u001b[1;32m      6\u001b[0m                                       \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                       \u001b[0moutput_as_string\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/calflops/flops_counter.py\u001b[0m in \u001b[0;36mcalculate_flops\u001b[0;34m(model, input_shape, transformer_tokenizer, args, kwargs, forward_mode, include_backPropagation, compute_bp_factor, print_results, print_detailed, output_as_string, output_precision, output_unit, ignore_modules)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforward_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'forward'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforward_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'generate'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1845\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1846\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m             \u001b[0;31m# run always called hooks if they have not already been run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1793\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1794\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m                 for hook_id, hook in (\n","\u001b[0;32m<ipython-input-11-56606a21bd02>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, kv_cache, labels)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkv_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimageComponent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1845\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1846\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m             \u001b[0;31m# run always called hooks if they have not already been run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1793\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1794\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m                 for hook_id, hook in (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/calflops/pytorch_ops.py\u001b[0m in \u001b[0;36mnewFunc\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodule_mac_count\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmacs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0mmodule_mac_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmacs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moldFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0mnewFunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__str__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__str__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/calflops/pytorch_ops.py\u001b[0m in \u001b[0;36mnewFunc\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodule_mac_count\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmacs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0mmodule_mac_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmacs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moldFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0mnewFunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__str__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__str__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"]}]},{"cell_type":"code","source":[],"metadata":{"id":"VAg2IgbirWMd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a = torch.tensor([2,2,3])\n","torch.cumprod(a, 0)[-1].item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dqld1UWICCIZ","outputId":"7b174d71-2abe-4bfe-84d1-99b696906369"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["12"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["t = 0\n","for p in model1.named_parameters():\n","  print(p[0], p[1].shape)\n","  eq = ''\n","  m = 1\n","  i =0\n","\n","  for n in p[1].shape:\n","    eq += str(n) + ' x '\n","    m *= n\n","    i += 1\n","    t += m\n","\n","  print(eq[:-3] + ' = ' + str(m) + ' total parameters for ' + p[0] if i > 1 else eq[:-3] + ' total parameters for ' + p[0])\n","  print()\n","print('================== total =============== {}'.format(t))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K8B1U-jQ_H56","outputId":"a655f4ec-dab0-48be-e05c-66c456028364","executionInfo":{"status":"ok","timestamp":1744049318374,"user_tz":240,"elapsed":18,"user":{"displayName":"abcd demo","userId":"13372324590583046209"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["embed.weight torch.Size([257153, 128])\n","257153 x 128 = 32915584 total parameters for embed.weight\n","\n","imageComponent.embed.patchConv.weight torch.Size([512, 3, 1, 1])\n","512 x 3 x 1 x 1 = 1536 total parameters for imageComponent.embed.patchConv.weight\n","\n","imageComponent.embed.patchConv.bias torch.Size([512])\n","512 total parameters for imageComponent.embed.patchConv.bias\n","\n","imageComponent.embed.positionEmbeddings.weight torch.Size([784, 512])\n","784 x 512 = 401408 total parameters for imageComponent.embed.positionEmbeddings.weight\n","\n","imageComponent.layers.0.norm1.weight torch.Size([512])\n","512 total parameters for imageComponent.layers.0.norm1.weight\n","\n","imageComponent.layers.0.norm1.bias torch.Size([512])\n","512 total parameters for imageComponent.layers.0.norm1.bias\n","\n","imageComponent.layers.0.att.wq.weight torch.Size([512, 512])\n","512 x 512 = 262144 total parameters for imageComponent.layers.0.att.wq.weight\n","\n","imageComponent.layers.0.att.wq.bias torch.Size([512])\n","512 total parameters for imageComponent.layers.0.att.wq.bias\n","\n","imageComponent.layers.0.att.wk.weight torch.Size([512, 512])\n","512 x 512 = 262144 total parameters for imageComponent.layers.0.att.wk.weight\n","\n","imageComponent.layers.0.att.wk.bias torch.Size([512])\n","512 total parameters for imageComponent.layers.0.att.wk.bias\n","\n","imageComponent.layers.0.att.wv.weight torch.Size([512, 512])\n","512 x 512 = 262144 total parameters for imageComponent.layers.0.att.wv.weight\n","\n","imageComponent.layers.0.att.wv.bias torch.Size([512])\n","512 total parameters for imageComponent.layers.0.att.wv.bias\n","\n","imageComponent.layers.0.att.wo.weight torch.Size([512, 512])\n","512 x 512 = 262144 total parameters for imageComponent.layers.0.att.wo.weight\n","\n","imageComponent.layers.0.att.wo.bias torch.Size([512])\n","512 total parameters for imageComponent.layers.0.att.wo.bias\n","\n","imageComponent.layers.0.norm2.weight torch.Size([512])\n","512 total parameters for imageComponent.layers.0.norm2.weight\n","\n","imageComponent.layers.0.norm2.bias torch.Size([512])\n","512 total parameters for imageComponent.layers.0.norm2.bias\n","\n","imageComponent.layers.0.FF.fc1.weight torch.Size([128, 512])\n","128 x 512 = 65536 total parameters for imageComponent.layers.0.FF.fc1.weight\n","\n","imageComponent.layers.0.FF.fc1.bias torch.Size([128])\n","128 total parameters for imageComponent.layers.0.FF.fc1.bias\n","\n","imageComponent.layers.0.FF.fc2.weight torch.Size([512, 128])\n","512 x 128 = 65536 total parameters for imageComponent.layers.0.FF.fc2.weight\n","\n","imageComponent.layers.0.FF.fc2.bias torch.Size([512])\n","512 total parameters for imageComponent.layers.0.FF.fc2.bias\n","\n","imageComponent.layers.1.norm1.weight torch.Size([512])\n","512 total parameters for imageComponent.layers.1.norm1.weight\n","\n","imageComponent.layers.1.norm1.bias torch.Size([512])\n","512 total parameters for imageComponent.layers.1.norm1.bias\n","\n","imageComponent.layers.1.att.wq.weight torch.Size([512, 512])\n","512 x 512 = 262144 total parameters for imageComponent.layers.1.att.wq.weight\n","\n","imageComponent.layers.1.att.wq.bias torch.Size([512])\n","512 total parameters for imageComponent.layers.1.att.wq.bias\n","\n","imageComponent.layers.1.att.wk.weight torch.Size([512, 512])\n","512 x 512 = 262144 total parameters for imageComponent.layers.1.att.wk.weight\n","\n","imageComponent.layers.1.att.wk.bias torch.Size([512])\n","512 total parameters for imageComponent.layers.1.att.wk.bias\n","\n","imageComponent.layers.1.att.wv.weight torch.Size([512, 512])\n","512 x 512 = 262144 total parameters for imageComponent.layers.1.att.wv.weight\n","\n","imageComponent.layers.1.att.wv.bias torch.Size([512])\n","512 total parameters for imageComponent.layers.1.att.wv.bias\n","\n","imageComponent.layers.1.att.wo.weight torch.Size([512, 512])\n","512 x 512 = 262144 total parameters for imageComponent.layers.1.att.wo.weight\n","\n","imageComponent.layers.1.att.wo.bias torch.Size([512])\n","512 total parameters for imageComponent.layers.1.att.wo.bias\n","\n","imageComponent.layers.1.norm2.weight torch.Size([512])\n","512 total parameters for imageComponent.layers.1.norm2.weight\n","\n","imageComponent.layers.1.norm2.bias torch.Size([512])\n","512 total parameters for imageComponent.layers.1.norm2.bias\n","\n","imageComponent.layers.1.FF.fc1.weight torch.Size([128, 512])\n","128 x 512 = 65536 total parameters for imageComponent.layers.1.FF.fc1.weight\n","\n","imageComponent.layers.1.FF.fc1.bias torch.Size([128])\n","128 total parameters for imageComponent.layers.1.FF.fc1.bias\n","\n","imageComponent.layers.1.FF.fc2.weight torch.Size([512, 128])\n","512 x 128 = 65536 total parameters for imageComponent.layers.1.FF.fc2.weight\n","\n","imageComponent.layers.1.FF.fc2.bias torch.Size([512])\n","512 total parameters for imageComponent.layers.1.FF.fc2.bias\n","\n","imageComponent.normFinal.weight torch.Size([512])\n","512 total parameters for imageComponent.normFinal.weight\n","\n","imageComponent.normFinal.bias torch.Size([512])\n","512 total parameters for imageComponent.normFinal.bias\n","\n","projected.project.weight torch.Size([128, 512])\n","128 x 512 = 65536 total parameters for projected.project.weight\n","\n","projected.project.bias torch.Size([128])\n","128 total parameters for projected.project.bias\n","\n","logits.decoderLayers.0.att.wq.weight torch.Size([128, 128])\n","128 x 128 = 16384 total parameters for logits.decoderLayers.0.att.wq.weight\n","\n","logits.decoderLayers.0.att.wq.bias torch.Size([128])\n","128 total parameters for logits.decoderLayers.0.att.wq.bias\n","\n","logits.decoderLayers.0.att.wk.weight torch.Size([128, 128])\n","128 x 128 = 16384 total parameters for logits.decoderLayers.0.att.wk.weight\n","\n","logits.decoderLayers.0.att.wk.bias torch.Size([128])\n","128 total parameters for logits.decoderLayers.0.att.wk.bias\n","\n","logits.decoderLayers.0.att.wv.weight torch.Size([128, 128])\n","128 x 128 = 16384 total parameters for logits.decoderLayers.0.att.wv.weight\n","\n","logits.decoderLayers.0.att.wv.bias torch.Size([128])\n","128 total parameters for logits.decoderLayers.0.att.wv.bias\n","\n","logits.decoderLayers.0.att.wo.weight torch.Size([128, 128])\n","128 x 128 = 16384 total parameters for logits.decoderLayers.0.att.wo.weight\n","\n","logits.decoderLayers.0.att.wo.bias torch.Size([128])\n","128 total parameters for logits.decoderLayers.0.att.wo.bias\n","\n","logits.decoderLayers.0.norm1.weight torch.Size([128])\n","128 total parameters for logits.decoderLayers.0.norm1.weight\n","\n","logits.decoderLayers.0.FF.gate_proj.weight torch.Size([128, 128])\n","128 x 128 = 16384 total parameters for logits.decoderLayers.0.FF.gate_proj.weight\n","\n","logits.decoderLayers.0.FF.up_proj.weight torch.Size([128, 128])\n","128 x 128 = 16384 total parameters for logits.decoderLayers.0.FF.up_proj.weight\n","\n","logits.decoderLayers.0.FF.down_proj.weight torch.Size([128, 128])\n","128 x 128 = 16384 total parameters for logits.decoderLayers.0.FF.down_proj.weight\n","\n","logits.decoderLayers.0.norm2.weight torch.Size([128])\n","128 total parameters for logits.decoderLayers.0.norm2.weight\n","\n","logits.normFinal.weight torch.Size([128])\n","128 total parameters for logits.normFinal.weight\n","\n","logits.logits.weight torch.Size([257153, 128])\n","257153 x 128 = 32915584 total parameters for logits.logits.weight\n","\n","logits.logits.bias torch.Size([257153])\n","257153 total parameters for logits.logits.bias\n","\n","================== total =============== 69567891\n"]}]},{"cell_type":"code","source":["tokenizer.decode(torch.argmax(predictions, dim=1)), str(y.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XYp3h7p66FCs","outputId":"e0fede3f-9913-4309-9b9f-c6144d4ef255"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('The digit is tensor([])])', '0')"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["labels = torch.tensor(tokenizer.encode(\"The digit is \" + str(y)), dtype = torch.long)\n","pred = F.softmax(logits[:,-labels.shape[0]:,:], dim = -1).reshape(logits.shape[0] * labels.shape[0], logits.shape[2])\n","labels = labels.tile((2))\n","F.cross_entropy(pred, labels)"],"metadata":{"id":"VGXcGaJxQETV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# labels = torch.tensor(tokenizer.encode(\"The digit is \" + str(y)), dtype = torch.int)\n","# labels = labels.tile((2))\n","\n","# labels.shape, pred.shape, logits.shape[0] * labels.shape[0]\n","# logits[:,-labels.shape[1]:,:].shape\n","pred.dtype"],"metadata":{"id":"jlNIaIYzDx8O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dbJDsqiy_5_j","outputId":"6eca157a-075b-4ec2-f4df-6b8a6869753e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[   651,  22926,    603,  24614,   5917, 235324, 235269, 235248, 235310,\n","           3013],\n","        [   651,  22926,    603,  24614,   5917, 235324, 235269, 235248, 235310,\n","           3013]], dtype=torch.int32)"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["F.softmax(logits, dim = -1).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sWdOSZba7Fc4","outputId":"42d2a5f9-2428-4c3d-b060-67869faeb2cf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 791, 257153])"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["logits.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8c_8igmIw3c7","outputId":"52427843-8cfa-4a40-bbc2-40a1ec5981ec"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 791, 257153])"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["x.shape, inputDict['input_ids'].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gzjl8CwV9PLU","outputId":"564ff4e4-3b3f-4d32-fee8-ad3c09a93221"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([2, 1, 28, 28]), torch.Size([1, 790]))"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["tokenizer.decode(inputDict['input_ids'][0], skip_special_tokens=True), tokenizer.decode(torch.argmax(logits)), logits.shape\n","#F.softmax(logits[0,0], dim = -1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fjIDs7iqsYR9","outputId":"a4af117d-b89f-451e-daeb-1040a5d79d60"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('The number is \\n', ' Mapper', torch.Size([1, 1, 257153]))"]},"metadata":{},"execution_count":113}]},{"cell_type":"code","source":["logits.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qMmCOarv6JTe","outputId":"c90df2fd-b0b4-440e-bb24-22e6b31dd2c9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 1, 257153])"]},"metadata":{},"execution_count":115}]},{"cell_type":"code","source":["inputDict['input_ids'][0,:3]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jlX27ou64uPA","outputId":"8b501aa9-3b6c-43aa-fac7-420755b17288"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([257152, 257152, 257152])"]},"metadata":{},"execution_count":97}]},{"cell_type":"code","source":["# inputDict['attention_mask'].cumsum(-1)[:,-1].unsqueeze(0).shape[0]\n","# (inputDict['attention_mask'].cumsum(-1)).masked_fill_((inputDict['attention_mask'] == 0), 1)"],"metadata":{"id":"8zJx2igYMooq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def makeLabels(s, tokenizer, imgSeqLength):\n","      # Prepend a `self.image_seq_length` number of image tokens to the prompt\n","      input_strings = [\n","          add_image_tokens_to_prompt(\n","              prefix_prompt=prompt,\n","              bos_token=tokenizer.bos_token,\n","              image_seq_len=imgSeqLength,\n","              image_token='<image>',\n","          )\n","          for prompt in s\n","      ]\n","\n","      # Returns the input_ids and attention_mask as PyTorch tensors\n","      inputs = tokenizer(\n","          input_strings,\n","          return_tensors=\"pt\",\n","          padding = \"longest\",\n","          truncation=True,\n","      )\n","\n","      return inputs"],"metadata":{"id":"QvUwdO2G4w11"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["makeLabels([\"number 5\"], tokenizer, 28*28)['input_ids'].transpose(1,0)[:,0].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fNqAG8nL5_Zq","outputId":"c1a0cb0c-e107-417b-9439-7c82b3f89a03"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([789])"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["makeLabels([\"number 5\"], tokenizer, 28*28)['input_ids']#,makeLabels([\"The number is \"], tokenizer, 28*28)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":280},"id":"83mU5buUvHNL","outputId":"42bc04c0-7663-4814-ec63-41aa640a6f21"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"\"log_softmax_lastdim_kernel_impl\" not implemented for 'Long'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-116-74da10d81713>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmakeLabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"number 5\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#,makeLabels([\"The number is \"], tokenizer, 28*28)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmakeLabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"The number is \"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmakeLabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"number 5\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3492\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3493\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3494\u001b[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001b[0m\u001b[1;32m   3495\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3496\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: \"log_softmax_lastdim_kernel_impl\" not implemented for 'Long'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"7Lj5FLlbIK64"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yvuog9BL7dBf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1hdwSe_fqTua","outputId":"5c23725c-a602-4368-e581-9dcead04c2de"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["60000"]},"metadata":{},"execution_count":72}]},{"cell_type":"code","source":[],"metadata":{"id":"7oe53g2BozNf","colab":{"base_uri":"https://localhost:8080/","height":339},"outputId":"f30da5de-c6e8-4346-98a4-54e9601f0c99"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"shape '[1, 196, 32]' is invalid for input of size 25088","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-153-0e492da73ba6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0minputDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'The number is '\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtheImage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#dict_keys(['pixel_values', 'input_ids', 'attention_mask']) ('attention mask is not really needed for our implementation')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;31m#input_ids: torch.LongTensor = None,pixel_values: torch.FloatTensor = None,attention_mask: Optional[torch.Tensor] = None,kv_cache = None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mmodel1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pixel_values'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-121-044f7c40d633>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, kv_cache)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m#self.language_model(attention_mask=attention_mask,position_ids=position_ids,inputs_embeds=inputs_embeds,kv_cache=kv_cache,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimageComponent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_input_ids_with_image_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-152-0737dbb6f342>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormFinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-152-0737dbb6f342>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     83\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-152-0737dbb6f342>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattWeights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# B, seqLength, numHeads, dimPerHead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseqLength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumFeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# B, seqLength, totalDims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# B, seqLength, totalDims > B, seqLength, embeddingDim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 196, 32]' is invalid for input of size 25088"]}]},{"cell_type":"code","source":["4 * 196 * 32"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YhdsXY7kKOR5","outputId":"d0d1a9e5-6fbc-46fc-e4f6-92bff9140023"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["25088"]},"metadata":{},"execution_count":151}]},{"cell_type":"code","source":["e = nn.Embedding(tokenizer.vocab_size + 1, 4)\n","e(inputDict['input_ids'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_U3_OQXqISOd","outputId":"3746ba38-e86c-428e-95c0-34000b1c997a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[-0.1572, -0.3590,  1.3985,  1.1816],\n","         [-0.1572, -0.3590,  1.3985,  1.1816],\n","         [-0.1572, -0.3590,  1.3985,  1.1816],\n","         ...,\n","         [-1.4358, -0.9771,  0.8117, -1.6045],\n","         [ 1.1771, -0.1869,  1.1198,  0.2791],\n","         [-0.8930,  1.4164,  0.5055,  0.3972]]], grad_fn=<EmbeddingBackward0>)"]},"metadata":{},"execution_count":133}]},{"cell_type":"code","source":["print(inputDict['input_ids'].shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5gWe-qRBtWFZ","outputId":"b4aa1dac-0dcf-4680-a4c8-ffee5719b8f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 790])\n"]}]},{"cell_type":"code","source":["# tokenizer.vocab_size"],"metadata":{"id":"s2o7A2Q9_I44"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\n","from PIL import Image\n","import requests\n","import torch\n","\n","model_id = \"google/paligemma-3b-mix-224\"\n","device = \"cpu\"#\"cuda:0\"\n","dtype = torch.bfloat16\n","\n","url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\n","# image = Image.open(requests.get(url, stream=True).raw)\n","image = torchvision.transforms.functional.to_pil_image(x[0].tile((3,1, 1)))\n","\n","model = PaliGemmaForConditionalGeneration.from_pretrained(\n","    model_id,\n","    torch_dtype=dtype,\n","    device_map=device,\n","    revision=\"bfloat16\",\n",").eval()\n","processor = AutoProcessor.from_pretrained(model_id)\n","\n","# Instruct the model to create a caption in Spanish\n","prompt = \"What number is this?\"\n","model_inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(model.device)\n","input_len = model_inputs[\"input_ids\"].shape[-1]\n","\n","with torch.inference_mode():\n","    generation = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\n","    generation = generation[0][input_len:]\n","    decoded = processor.decode(generation, skip_special_tokens=True)\n","    print('\\n Answer: \\n', decoded)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":138,"referenced_widgets":["f60fda3a0aa4442490fd795d6c10e67c","88cc678cb5c348c0b59b7e1a97e46fc4","b192032cdeb74d35a63895f9a244cbf4","c4695fedf31c40a0b747944a1d965793","037ca27eabd941a59269a43d4ce0ef9e","5c73caf33f2e4d44ab6009dd3fb86e81","b12cb6742aeb41c392a366def658c617","8e3a72beaf29436398086adf647a3a9f","8ff1673ea4e94af3a016cb61ced23491","fe84500081b8403485309b92ad6688a9","5498cbb54584402aaad1c8974ce44eff"]},"id":"IbCNl2pt4kuv","outputId":"5095964d-dbfb-4e43-a4fe-8e84871742b8"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f60fda3a0aa4442490fd795d6c10e67c"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Answer: \n"," 0\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","plt.imshow(image)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":447},"id":"CgIE5l-Tfb4C","outputId":"abd7acd7-cd62-4042-c015-85c52702d753"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f8c1ff07950>"]},"metadata":{},"execution_count":32},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["model.state_dict()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VL6SQIURfI1k","outputId":"ba3dfe00-d405-48e4-8f1e-5b87ece9d535"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OrderedDict([('vision_tower.vision_model.embeddings.patch_embedding.weight',\n","              tensor([[[[-1.8066e-02, -2.2949e-02, -2.4658e-02,  ..., -3.4027e-03,\n","                          3.2227e-02,  5.4443e-02],\n","                        [-8.7891e-03, -1.1841e-02, -1.5991e-02,  ...,  6.3171e-03,\n","                          2.3682e-02,  3.6133e-02],\n","                        [ 1.3275e-03,  3.0823e-03, -1.5381e-02,  ...,  1.0925e-02,\n","                          1.2665e-03,  1.3550e-02],\n","                        ...,\n","                        [-8.9722e-03,  3.6133e-02,  3.2471e-02,  ..., -1.7090e-02,\n","                         -2.4414e-02, -1.6479e-02],\n","                        [-1.3184e-02,  3.4912e-02,  4.3945e-02,  ..., -2.9053e-02,\n","                         -3.5645e-02, -3.1738e-02],\n","                        [-4.0283e-02,  4.7363e-02,  6.1768e-02,  ..., -3.0396e-02,\n","                         -3.2471e-02, -3.0518e-02]],\n","              \n","                       [[-2.4536e-02, -3.0518e-02, -3.5400e-02,  ..., -1.6968e-02,\n","                          3.3936e-02,  6.2012e-02],\n","                        [-1.2817e-02, -1.2085e-02, -1.8555e-02,  ..., -6.2866e-03,\n","                          2.2583e-02,  3.8818e-02],\n","                        [ 6.9885e-03,  1.5991e-02, -7.7820e-03,  ...,  4.6082e-03,\n","                          9.8419e-04,  1.6357e-02],\n","                        ...,\n","                        [-2.8320e-02,  3.3447e-02,  2.9297e-02,  ...,  1.5198e-02,\n","                          8.1787e-03,  7.9346e-03],\n","                        [-2.5024e-02,  4.4922e-02,  5.5420e-02,  ..., -5.2490e-03,\n","                         -1.4526e-02, -1.7944e-02],\n","                        [-4.5166e-02,  6.8359e-02,  9.0820e-02,  ..., -6.8359e-03,\n","                         -1.0071e-02, -1.4893e-02]],\n","              \n","                       [[-8.8501e-03, -4.6997e-03, -8.6060e-03,  ..., -1.9287e-02,\n","                          1.1169e-02,  3.4424e-02],\n","                        [-8.5068e-04,  9.2773e-03,  6.5002e-03,  ..., -2.0508e-02,\n","                         -6.8054e-03,  4.4861e-03],\n","                        [ 7.2937e-03,  2.2949e-02,  1.3306e-02,  ..., -1.5198e-02,\n","                         -1.9287e-02, -6.8359e-03],\n","                        ...,\n","                        [-2.4170e-02,  1.4832e-02, -2.2125e-03,  ...,  1.3184e-02,\n","                          5.6763e-03,  6.0730e-03],\n","                        [-1.9409e-02,  2.1362e-02,  1.6968e-02,  ...,  9.2163e-03,\n","                          2.5024e-03, -2.4719e-03],\n","                        [-3.3447e-02,  4.9072e-02,  5.5420e-02,  ...,  2.2461e-02,\n","                          2.0630e-02,  1.3489e-02]]],\n","              \n","              \n","                      [[[-1.0437e-02, -1.5335e-03,  5.4016e-03,  ..., -1.4954e-02,\n","                         -4.4861e-03, -7.5073e-03],\n","                        [-7.2937e-03, -6.0272e-04,  1.6602e-02,  ..., -1.2146e-02,\n","                          3.9482e-04, -1.1963e-02],\n","                        [ 4.7913e-03,  6.2866e-03,  9.5215e-03,  ..., -4.1809e-03,\n","                          1.2939e-02, -7.0496e-03],\n","                        ...,\n","                        [ 9.5367e-04,  2.5272e-05,  9.3994e-03,  ..., -5.7678e-03,\n","                          1.2268e-02, -3.5095e-03],\n","                        [ 4.6997e-03,  2.3804e-03,  3.2043e-03,  ...,  2.2888e-03,\n","                          1.8799e-02,  5.7220e-04],\n","                        [-1.8616e-03,  5.6076e-04, -7.7820e-03,  ...,  2.0752e-02,\n","                          2.3315e-02, -2.7466e-04]],\n","              \n","                       [[-1.3306e-02, -3.3417e-03,  4.4861e-03,  ...,  2.6855e-03,\n","                          1.4038e-02,  5.5237e-03],\n","                        [-1.6602e-02, -7.5073e-03,  1.2695e-02,  ...,  6.6833e-03,\n","                          2.0874e-02,  4.4250e-03],\n","                        [-5.6763e-03, -1.2894e-03,  7.5073e-03,  ...,  1.4282e-02,\n","                          3.5156e-02,  1.1169e-02],\n","                        ...,\n","                        [-1.0681e-03,  8.0490e-04,  1.1108e-02,  ..., -1.3611e-02,\n","                          3.5553e-03, -1.2268e-02],\n","                        [ 6.4087e-03,  1.0803e-02,  1.4343e-02,  ..., -2.5177e-03,\n","                          1.3123e-02, -5.0354e-03],\n","                        [-5.7983e-03,  4.7607e-03, -2.4796e-04,  ...,  1.4526e-02,\n","                          1.5747e-02, -8.6060e-03]],\n","              \n","                       [[-2.8992e-03,  8.0872e-04,  7.1716e-04,  ..., -1.1292e-03,\n","                          5.0964e-03,  2.3804e-03],\n","                        [-6.8970e-03, -4.3030e-03,  9.3994e-03,  ...,  9.7275e-04,\n","                          8.8501e-03, -1.5564e-03],\n","                        [ 1.2283e-03,  2.7924e-03,  8.5449e-03,  ...,  6.1340e-03,\n","                          1.9043e-02,  3.3722e-03],\n","                        ...,\n","                        [-1.0834e-03,  2.6321e-04,  4.2725e-03,  ..., -1.0071e-02,\n","                          1.7548e-03, -3.7537e-03],\n","                        [ 3.6469e-03,  7.3242e-03,  8.4839e-03,  ..., -6.9275e-03,\n","                          7.3242e-03,  1.9073e-03],\n","                        [-1.0437e-02,  8.9645e-04, -3.0518e-03,  ...,  3.8910e-03,\n","                          7.7820e-03, -4.3640e-03]]],\n","              \n","              \n","                      [[[-6.5613e-04, -1.4725e-03,  3.7231e-03,  ..., -2.5024e-03,\n","                         -2.1362e-03,  9.1553e-03],\n","                        [ 4.0588e-03,  1.8311e-03,  2.1839e-04,  ..., -1.3428e-02,\n","                         -9.4604e-03,  4.7607e-03],\n","                        [ 4.2419e-03,  3.5400e-03,  3.1738e-03,  ..., -6.4087e-03,\n","                         -4.5586e-04,  1.0803e-02],\n","                        ...,\n","                        [ 1.0742e-02,  8.1177e-03,  4.3335e-03,  ...,  4.0894e-03,\n","                          8.1787e-03,  8.0566e-03],\n","                        [ 1.1475e-02,  2.1667e-03,  9.8419e-04,  ...,  5.5237e-03,\n","                          7.7209e-03,  5.4626e-03],\n","                        [-3.0365e-03,  4.6387e-03,  2.2583e-03,  ...,  3.6011e-03,\n","                          4.5471e-03,  5.0049e-03]],\n","              \n","                       [[ 7.7515e-03,  3.3569e-03,  6.0120e-03,  ...,  2.1458e-04,\n","                         -1.8158e-03,  1.0559e-02],\n","                        [ 5.5542e-03, -1.1902e-03, -1.9989e-03,  ..., -9.6436e-03,\n","                         -7.4768e-03,  5.9204e-03],\n","                        [ 1.7624e-03,  7.3433e-05,  4.5776e-03,  ..., -4.9438e-03,\n","                          1.5335e-03,  1.3977e-02],\n","                        ...,\n","                        [ 5.1880e-03, -1.4591e-04, -4.4250e-03,  ...,  2.9755e-03,\n","                          1.7853e-03,  4.2725e-03],\n","                        [ 5.8289e-03, -8.4229e-03, -9.4604e-03,  ...,  1.3046e-03,\n","                          1.0452e-03,  3.7689e-03],\n","                        [-5.0354e-03, -4.3106e-04, -1.7166e-03,  ...,  1.9836e-03,\n","                          2.4872e-03,  7.7209e-03]],\n","              \n","                       [[ 1.2634e-02,  5.0049e-03,  6.0120e-03,  ...,  2.6550e-03,\n","                         -1.9360e-04,  8.1177e-03],\n","                        [ 5.0659e-03, -3.3112e-03, -3.5095e-03,  ..., -3.7994e-03,\n","                         -4.1199e-03,  4.0283e-03],\n","                        [ 6.4850e-05, -1.6403e-03,  5.5847e-03,  ...,  5.6458e-04,\n","                          3.4485e-03,  1.0925e-02],\n","                        ...,\n","                        [ 5.4932e-03,  4.9438e-03, -6.5231e-04,  ...,  4.8218e-03,\n","                         -1.7090e-03,  3.5858e-04],\n","                        [ 8.2397e-03, -2.3651e-03, -4.8828e-03,  ..., -1.5411e-03,\n","                         -4.7302e-03, -1.2665e-03],\n","                        [-5.2795e-03,  1.3428e-03, -1.6174e-03,  ..., -3.6621e-03,\n","                         -3.7689e-03,  2.7618e-03]]],\n","              \n","              \n","                      ...,\n","              \n","              \n","                      [[[ 1.0925e-02, -7.9346e-03,  1.8555e-02,  ..., -1.5259e-02,\n","                          4.6082e-03,  1.0559e-02],\n","                        [ 2.1973e-03, -3.9551e-02,  1.5137e-02,  ..., -1.8066e-02,\n","                          1.1353e-02, -1.7319e-03],\n","                        [-8.6060e-03, -4.6387e-02, -7.6904e-03,  ..., -2.8931e-02,\n","                         -6.6528e-03, -1.0193e-02],\n","                        ...,\n","                        [-3.3447e-02, -1.5945e-03, -3.5400e-02,  ..., -4.5898e-02,\n","                         -2.7344e-02, -2.9663e-02],\n","                        [ 1.1353e-02,  1.6785e-03, -2.4780e-02,  ...,  7.4463e-03,\n","                          1.5259e-03,  2.6611e-02],\n","                        [ 1.4099e-02, -1.3672e-02, -3.4424e-02,  ...,  9.5825e-03,\n","                         -4.4189e-02,  1.2329e-02]],\n","              \n","                       [[ 2.0874e-02, -6.3782e-03,  1.2146e-02,  ..., -2.0020e-02,\n","                          5.1880e-03,  1.0254e-02],\n","                        [ 1.4282e-02, -4.1992e-02,  8.3618e-03,  ..., -1.5503e-02,\n","                          2.0264e-02,  5.0049e-03],\n","                        [ 7.8735e-03, -4.5898e-02, -1.4587e-02,  ..., -2.7222e-02,\n","                         -9.0027e-04, -6.2561e-03],\n","                        ...,\n","                        [-1.6235e-02,  1.5076e-02, -3.8818e-02,  ..., -4.2969e-02,\n","                         -1.6235e-02, -1.6113e-02],\n","                        [ 2.8320e-02,  1.5381e-02, -2.9907e-02,  ...,  1.6846e-02,\n","                          1.1230e-02,  3.9551e-02],\n","                        [ 3.1128e-02, -5.7983e-04, -3.7842e-02,  ...,  1.6968e-02,\n","                         -4.1504e-02,  1.8066e-02]],\n","              \n","                       [[ 1.5625e-02, -4.8523e-03, -3.3417e-03,  ..., -1.0193e-02,\n","                          6.7444e-03,  4.5776e-03],\n","                        [ 1.4893e-02, -2.9907e-02,  2.9373e-04,  ..., -9.0942e-03,\n","                          2.2095e-02,  1.8616e-03],\n","                        [ 6.4392e-03, -3.7842e-02, -1.7578e-02,  ..., -2.0264e-02,\n","                          6.3782e-03, -3.5553e-03],\n","                        ...,\n","                        [-1.2207e-02,  9.5215e-03, -3.9795e-02,  ..., -3.0151e-02,\n","                         -1.0315e-02, -1.2329e-02],\n","                        [ 2.4902e-02,  1.2817e-02, -2.3682e-02,  ...,  2.1851e-02,\n","                          1.2695e-02,  3.3936e-02],\n","                        [ 2.0874e-02, -3.9368e-03, -3.2471e-02,  ...,  1.4099e-02,\n","                         -4.1016e-02,  8.4839e-03]]],\n","              \n","              \n","                      [[[-9.6436e-03, -8.0566e-03, -3.4790e-03,  ..., -9.5825e-03,\n","                          4.1809e-03,  1.0071e-02],\n","                        [-3.0670e-03,  5.2490e-03,  9.4604e-03,  ..., -1.8234e-03,\n","                          6.0425e-03,  1.5137e-02],\n","                        [ 2.0905e-03,  6.0425e-03,  1.0132e-02,  ...,  7.4863e-05,\n","                          1.4771e-02,  1.9653e-02],\n","                        ...,\n","                        [-5.0659e-03, -5.7983e-03, -6.1340e-03,  ..., -5.6763e-03,\n","                          3.1128e-03,  4.3335e-03],\n","                        [-1.1719e-02, -7.6294e-03, -2.3346e-03,  ..., -6.6223e-03,\n","                         -3.9673e-04, -1.9226e-03],\n","                        [-1.4160e-02, -1.3245e-02, -8.7280e-03,  ..., -1.0864e-02,\n","                         -1.5991e-02, -9.3994e-03]],\n","              \n","                       [[ 1.6479e-03, -1.7548e-03,  1.9531e-03,  ..., -1.3855e-02,\n","                         -9.5367e-04,  8.6060e-03],\n","                        [-1.1368e-03,  2.5787e-03,  9.1553e-03,  ..., -8.4839e-03,\n","                          2.8229e-04,  1.0559e-02],\n","                        [ 3.7689e-03,  5.8289e-03,  1.2573e-02,  ..., -5.0354e-03,\n","                          1.0803e-02,  1.7090e-02],\n","                        ...,\n","                        [-4.4250e-03, -6.7444e-03, -8.0566e-03,  ..., -2.8839e-03,\n","                          2.4261e-03,  5.0049e-03],\n","                        [-1.1475e-02, -7.8735e-03, -3.8910e-03,  ..., -8.0566e-03,\n","                         -2.2583e-03,  1.2054e-03],\n","                        [-1.0376e-02, -9.3994e-03, -3.9978e-03,  ..., -6.0730e-03,\n","                         -1.2207e-02,  2.8610e-06]],\n","              \n","                       [[ 7.5378e-03, -2.1076e-04,  1.0834e-03,  ..., -1.7395e-03,\n","                          4.0588e-03,  6.0425e-03],\n","                        [-6.4468e-04, -3.9101e-04,  3.9368e-03,  ...,  4.7874e-04,\n","                          7.3624e-04,  2.2888e-03],\n","                        [ 3.8910e-04,  1.1215e-03,  7.3547e-03,  ..., -1.0681e-03,\n","                          6.2561e-03,  4.1504e-03],\n","                        ...,\n","                        [-9.1934e-04, -6.7353e-06, -2.3041e-03,  ...,  2.8229e-03,\n","                          2.0447e-03, -3.3569e-04],\n","                        [-7.1716e-03, -4.4632e-04,  3.9062e-03,  ..., -3.6621e-03,\n","                         -1.9531e-03, -1.1826e-03],\n","                        [-7.6904e-03, -4.1504e-03,  1.6098e-03,  ...,  1.3962e-03,\n","                         -6.3477e-03,  1.7929e-03]]],\n","              \n","              \n","                      [[[ 1.5564e-02, -5.9814e-03,  6.0425e-03,  ...,  3.9673e-03,\n","                          1.2573e-02, -1.3977e-02],\n","                        [ 7.2632e-03,  5.9814e-03, -3.4637e-03,  ...,  2.0146e-05,\n","                          2.7924e-03,  6.3171e-03],\n","                        [ 2.3193e-03,  8.8120e-04, -4.3640e-03,  ..., -1.7643e-05,\n","                          1.9897e-02,  1.5137e-02],\n","                        ...,\n","                        [ 5.4626e-03,  9.7046e-03,  8.7891e-03,  ...,  7.7057e-04,\n","                          9.7656e-03,  9.1553e-03],\n","                        [ 8.3618e-03,  3.4637e-03,  5.9509e-03,  ..., -2.3041e-03,\n","                          5.4932e-03, -6.1951e-03],\n","                        [-6.7749e-03, -1.5640e-03, -1.0071e-02,  ..., -1.1108e-02,\n","                         -1.2085e-02,  3.3188e-04]],\n","              \n","                       [[ 1.7456e-02, -9.0942e-03, -2.3041e-03,  ..., -1.4191e-03,\n","                          5.0354e-03, -2.5269e-02],\n","                        [ 1.0254e-02,  5.2185e-03, -7.6599e-03,  ...,  2.1210e-03,\n","                          2.8381e-03,  3.9978e-03],\n","                        [ 4.1199e-03,  1.2665e-03, -4.2725e-03,  ...,  2.8992e-03,\n","                          2.1606e-02,  1.4465e-02],\n","                        ...,\n","                        [-1.8539e-03,  8.3008e-03,  8.2397e-03,  ...,  1.2878e-02,\n","                          1.7456e-02,  1.4038e-02],\n","                        [-6.0120e-03, -2.9144e-03,  4.0283e-03,  ...,  1.5259e-03,\n","                          7.5378e-03, -4.5776e-03],\n","                        [-2.8809e-02, -1.5381e-02, -1.7822e-02,  ..., -1.4221e-02,\n","                         -1.5076e-02, -4.4632e-04]],\n","              \n","                       [[ 1.5747e-02, -6.4087e-03, -1.0757e-03,  ...,  6.3171e-03,\n","                          1.2512e-02, -1.6846e-02],\n","                        [ 1.0071e-02,  7.2937e-03, -3.7842e-03,  ...,  7.9346e-03,\n","                          6.2256e-03,  4.3335e-03],\n","                        [ 5.0964e-03,  4.7302e-03,  6.9141e-05,  ...,  5.4321e-03,\n","                          1.8799e-02,  8.9111e-03],\n","                        ...,\n","                        [ 9.0942e-03,  1.4954e-02,  8.1177e-03,  ...,  3.4485e-03,\n","                          3.0365e-03,  4.7607e-03],\n","                        [ 9.7656e-03,  8.0566e-03,  9.6436e-03,  ..., -1.4267e-03,\n","                          2.2125e-03, -7.2327e-03],\n","                        [-9.3384e-03, -1.0452e-03, -6.2561e-03,  ..., -1.0193e-02,\n","                         -1.1597e-02,  1.2589e-03]]]], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.embeddings.patch_embedding.bias',\n","              tensor([ 0.0815,  0.1973, -0.0256,  ..., -0.0654,  0.2002,  0.3105],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.embeddings.position_embedding.weight',\n","              tensor([[-0.0559, -0.2236, -0.0957,  ...,  0.0762,  1.0234, -0.2451],\n","                      [-0.0591, -0.1758, -0.0262,  ...,  0.0991,  0.4336, -0.2676],\n","                      [-0.0369, -0.1514, -0.0464,  ...,  0.1055,  0.9219, -0.1973],\n","                      ...,\n","                      [ 0.2041, -0.4453,  0.0952,  ...,  0.1719, -1.3750, -0.2695],\n","                      [-0.0466, -0.1514, -0.0270,  ...,  0.1108, -0.1777, -0.2490],\n","                      [-0.0762, -0.1445, -0.0776,  ...,  0.0972, -0.8867, -0.2949]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight',\n","              tensor([[-0.0520,  0.0020,  0.0486,  ..., -0.0559,  0.0087, -0.0105],\n","                      [-0.0757,  0.0168,  0.0757,  ..., -0.0513, -0.0178,  0.0060],\n","                      [-0.2305,  0.0266,  0.0393,  ..., -0.0033, -0.0062,  0.0113],\n","                      ...,\n","                      [ 0.0562, -0.0089,  0.0859,  ..., -0.0023, -0.0579,  0.0100],\n","                      [-0.0025, -0.0168, -0.0265,  ..., -0.0209, -0.2002, -0.0007],\n","                      [-0.0133,  0.0029, -0.0087,  ..., -0.0098, -0.0349, -0.0017]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias',\n","              tensor([-0.5938,  0.3066, -0.5156,  ..., -0.9570,  0.5664, -0.4492],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight',\n","              tensor([[ 1.4893e-02, -6.4697e-03, -2.5024e-02,  ...,  1.1414e-02,\n","                       -1.6602e-02,  3.8300e-03],\n","                      [ 1.1475e-02, -2.4719e-03,  4.1504e-03,  ..., -5.0049e-03,\n","                       -9.7656e-03, -1.0010e-02],\n","                      [-3.1433e-03,  3.4790e-03, -1.3611e-02,  ..., -2.0599e-03,\n","                       -1.2024e-02,  2.6245e-02],\n","                      ...,\n","                      [ 1.1353e-02,  3.8385e-05, -5.7678e-03,  ...,  8.7738e-04,\n","                       -3.0060e-03, -9.5215e-03],\n","                      [-1.5869e-03,  1.0193e-02, -3.4027e-03,  ..., -1.8677e-02,\n","                       -2.5330e-03, -6.6528e-03],\n","                      [-1.0498e-02, -2.0905e-03, -1.3306e-02,  ...,  1.4709e-02,\n","                        2.2430e-03,  4.9438e-03]], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias',\n","              tensor([ 0.0048, -0.0009,  0.0037,  ..., -0.0069,  0.0016,  0.0040],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight',\n","              tensor([[-0.0483, -0.0181,  0.0430,  ..., -0.0466, -0.0063,  0.0082],\n","                      [-0.0527,  0.0115,  0.0625,  ..., -0.0217,  0.0050, -0.0022],\n","                      [-0.2041,  0.0040,  0.0025,  ..., -0.0040, -0.0383, -0.0054],\n","                      ...,\n","                      [ 0.0081, -0.0150,  0.0048,  ...,  0.0267, -0.0069, -0.0050],\n","                      [ 0.0084,  0.0021,  0.0156,  ...,  0.0115, -0.0092,  0.0118],\n","                      [ 0.0053,  0.0028, -0.0063,  ...,  0.0030, -0.0339, -0.0024]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias',\n","              tensor([-0.5352, -0.2715,  0.6797,  ...,  0.2695,  0.3965,  3.8281],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight',\n","              tensor([[-0.0312,  0.0007,  0.0005,  ...,  0.0012,  0.0070, -0.0073],\n","                      [-0.0327, -0.0027, -0.0007,  ...,  0.0075, -0.0187, -0.0201],\n","                      [ 0.0115, -0.0159,  0.0146,  ..., -0.0117, -0.0135,  0.0135],\n","                      ...,\n","                      [-0.0135,  0.0059,  0.0022,  ...,  0.0037, -0.0021, -0.0007],\n","                      [-0.0208,  0.0134,  0.0073,  ..., -0.0081, -0.0028, -0.0098],\n","                      [-0.0255,  0.0045, -0.0037,  ...,  0.0053,  0.0068,  0.0099]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias',\n","              tensor([-0.0066,  0.0190,  0.1543,  ...,  0.0288, -1.0469,  0.0471],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.0.layer_norm1.weight',\n","              tensor([3.3008e-01, 3.0327e-04, 6.7871e-02,  ..., 1.5469e+00, 6.9336e-02,\n","                      6.4850e-04], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.0.layer_norm1.bias',\n","              tensor([-2.8687e-03, -2.1935e-05,  1.1230e-02,  ..., -1.6846e-02,\n","                       3.1494e-02, -1.9455e-04], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight',\n","              tensor([[ 0.0101, -0.0381,  0.0106,  ..., -0.0166, -0.0115,  0.0127],\n","                      [-0.0249, -0.0037, -0.0168,  ...,  0.0322, -0.0083,  0.0170],\n","                      [ 0.0156, -0.0074, -0.0349,  ...,  0.0151, -0.0001,  0.0075],\n","                      ...,\n","                      [-0.0101,  0.0447, -0.0309,  ..., -0.0214, -0.0184,  0.0128],\n","                      [ 0.0048,  0.0172, -0.0056,  ...,  0.0033,  0.0084,  0.0227],\n","                      [-0.0237,  0.0102, -0.0047,  ...,  0.0013, -0.0016,  0.0212]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias',\n","              tensor([-5.7500, -5.4688, -4.9688,  ..., -4.4688, -0.9922, -5.4375],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight',\n","              tensor([[ 0.0197,  0.0208,  0.0006,  ...,  0.0493, -0.0023, -0.0205],\n","                      [ 0.0269,  0.0315,  0.0203,  ..., -0.0040, -0.0082, -0.0125],\n","                      [-0.0008, -0.0101, -0.0272,  ..., -0.0120,  0.0011,  0.0068],\n","                      ...,\n","                      [ 0.0147,  0.0205, -0.0271,  ..., -0.0046, -0.0103, -0.0189],\n","                      [ 0.0164, -0.0119, -0.0044,  ..., -0.0028,  0.0082, -0.0103],\n","                      [ 0.0090, -0.0140,  0.0004,  ..., -0.0190, -0.0060,  0.0178]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias',\n","              tensor([ 0.4824,  0.0072,  0.0221,  ..., -0.0674,  1.1719, -0.2637],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.0.layer_norm2.weight',\n","              tensor([ 1.0781e+00,  3.8086e-01,  2.9883e-01,  ...,  5.9688e+00,\n","                      -1.1015e-04,  2.5781e-01], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.0.layer_norm2.bias',\n","              tensor([-0.0132,  0.0435, -0.0752,  ..., -0.0056, -0.0003, -0.0146],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight',\n","              tensor([[-0.0209,  0.0238, -0.0181,  ...,  0.0135,  0.0059,  0.0035],\n","                      [ 0.0144, -0.0030,  0.0214,  ...,  0.0166,  0.0083, -0.0097],\n","                      [-0.0315,  0.0040,  0.0022,  ...,  0.0032, -0.0156,  0.0078],\n","                      ...,\n","                      [-0.0317,  0.0413,  0.0005,  ...,  0.0486,  0.0432,  0.0234],\n","                      [-0.0211, -0.0242, -0.0090,  ..., -0.0280, -0.0265, -0.0437],\n","                      [-0.0014,  0.0001, -0.0102,  ...,  0.0046, -0.0264,  0.0118]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias',\n","              tensor([-0.1221, -0.0908,  0.4922,  ..., -0.5625,  0.4648, -0.4062],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight',\n","              tensor([[-0.0086, -0.0032,  0.0006,  ..., -0.0041, -0.0114, -0.0020],\n","                      [-0.0100,  0.0060, -0.0021,  ..., -0.0005,  0.0017, -0.0008],\n","                      [-0.0032,  0.0112, -0.0008,  ..., -0.0143,  0.0151, -0.0143],\n","                      ...,\n","                      [ 0.0074, -0.0004, -0.0034,  ..., -0.0086,  0.0199,  0.0302],\n","                      [ 0.0066, -0.0153, -0.0028,  ...,  0.0220,  0.0049, -0.0225],\n","                      [-0.0189, -0.0064, -0.0043,  ..., -0.0073,  0.0056,  0.0258]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias',\n","              tensor([-0.0076, -0.0209,  0.2363,  ..., -0.0649,  0.0228, -0.0698],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight',\n","              tensor([[ 0.0019,  0.0383,  0.0008,  ...,  0.0107, -0.0039,  0.0092],\n","                      [ 0.0037, -0.0081, -0.0248,  ...,  0.0134,  0.0040, -0.0220],\n","                      [-0.0178,  0.0168,  0.0082,  ...,  0.0093, -0.0442,  0.0134],\n","                      ...,\n","                      [-0.0030, -0.0045, -0.0063,  ..., -0.0181,  0.0327, -0.0277],\n","                      [ 0.0339,  0.0239, -0.0026,  ...,  0.0094, -0.0228,  0.0176],\n","                      [-0.0046, -0.0430, -0.0020,  ...,  0.0005, -0.0021, -0.0162]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias',\n","              tensor([ 0.0469, -0.0216,  0.1230,  ..., -5.3750,  0.1060, -0.0135],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight',\n","              tensor([[-0.0011,  0.0034, -0.0065,  ..., -0.0223, -0.0261, -0.0123],\n","                      [-0.0054,  0.0081,  0.0115,  ..., -0.0070, -0.0007, -0.0079],\n","                      [ 0.0005,  0.0027,  0.0060,  ..., -0.0222,  0.0099, -0.0156],\n","                      ...,\n","                      [-0.0016, -0.0027,  0.0028,  ...,  0.0064,  0.0075,  0.0056],\n","                      [ 0.0227,  0.0045, -0.0325,  ...,  0.0032, -0.0172,  0.0080],\n","                      [ 0.0107, -0.0011, -0.0015,  ..., -0.0030,  0.0320, -0.0128]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias',\n","              tensor([ 0.0566, -0.0659, -0.4980,  ..., -0.1260,  0.4062, -0.1836],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.1.layer_norm1.weight',\n","              tensor([ 0.7539,  0.2793, -0.0012,  ...,  0.5195,  0.1465,  0.1543],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.1.layer_norm1.bias',\n","              tensor([-3.0151e-02, -3.4637e-03,  5.1117e-04,  ..., -2.2461e-02,\n","                       6.2988e-02,  3.6322e-07], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight',\n","              tensor([[-3.9551e-02, -3.2227e-02, -6.0059e-02,  ..., -3.1982e-02,\n","                        2.7618e-03, -4.4250e-03],\n","                      [-5.3711e-02,  5.0354e-03, -1.0498e-02,  ...,  2.5146e-02,\n","                        3.0518e-02, -2.6733e-02],\n","                      [ 3.1292e-06, -1.2815e-06,  4.2468e-07,  ..., -4.1723e-06,\n","                        5.4240e-06, -5.4389e-07],\n","                      ...,\n","                      [-7.4768e-04, -5.3406e-03,  4.4556e-03,  ..., -7.2327e-03,\n","                        3.6926e-03, -1.0498e-02],\n","                      [-5.2185e-03,  1.2817e-03, -2.6611e-02,  ...,  3.1982e-02,\n","                       -1.7471e-03,  1.2329e-02],\n","                      [-1.6113e-02, -2.2095e-02, -1.7090e-02,  ..., -5.8105e-02,\n","                       -1.4038e-02, -3.1250e-02]], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias',\n","              tensor([-2.7031, -1.1719, -6.0938,  ..., -1.1406, -2.0312, -4.1250],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight',\n","              tensor([[-4.2725e-03,  2.4902e-02,  5.0962e-06,  ..., -1.3977e-02,\n","                        4.4060e-04,  2.8564e-02],\n","                      [ 3.2959e-02, -7.1106e-03,  4.5002e-06,  ..., -8.9264e-04,\n","                       -8.0566e-03,  3.8818e-02],\n","                      [ 1.5442e-02, -3.2654e-03, -2.1309e-06,  ..., -2.5269e-02,\n","                       -1.6113e-02, -1.8677e-02],\n","                      ...,\n","                      [ 1.4526e-02, -1.4267e-03,  2.8461e-06,  ...,  1.1536e-02,\n","                        7.9346e-03, -2.5513e-02],\n","                      [-1.7822e-02,  8.8882e-04,  3.9339e-06,  ..., -1.6846e-02,\n","                        1.6602e-02,  4.3640e-03],\n","                      [-1.0376e-02, -9.8419e-04, -9.3579e-06,  ..., -1.7090e-02,\n","                       -2.3346e-03, -2.8198e-02]], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias',\n","              tensor([-0.0093,  0.1035, -0.0160,  ...,  0.0776,  0.6680,  0.2471],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.1.layer_norm2.weight',\n","              tensor([1.1719, 0.5664, 0.4746,  ..., 1.0156, 0.0869, 0.4492],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.1.layer_norm2.bias',\n","              tensor([ 0.0123,  0.0601,  0.1396,  ...,  0.1099, -0.0049,  0.0933],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight',\n","              tensor([[-0.0031, -0.0117,  0.0170,  ...,  0.0148, -0.0130, -0.0157],\n","                      [-0.0139, -0.0081, -0.0065,  ..., -0.0007, -0.0082, -0.0320],\n","                      [ 0.0245,  0.0192,  0.0006,  ...,  0.0126,  0.0043,  0.0295],\n","                      ...,\n","                      [ 0.0050,  0.0079,  0.0001,  ...,  0.0221,  0.0214, -0.0168],\n","                      [ 0.0097, -0.0623,  0.0005,  ...,  0.0498, -0.0579, -0.0112],\n","                      [ 0.0256,  0.0010, -0.0107,  ..., -0.0025, -0.0002,  0.0148]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias',\n","              tensor([-0.1445, -0.0938,  0.7383,  ..., -0.2754, -0.9688,  0.2695],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight',\n","              tensor([[-0.0041,  0.0287,  0.0090,  ..., -0.0610, -0.0176,  0.0544],\n","                      [ 0.0908, -0.0014, -0.0032,  ..., -0.0187,  0.0025, -0.0050],\n","                      [ 0.0186, -0.0018,  0.0044,  ..., -0.0008,  0.0113,  0.0085],\n","                      ...,\n","                      [-0.0259, -0.0162,  0.0074,  ...,  0.0053,  0.0103, -0.0391],\n","                      [-0.0134, -0.0067, -0.0025,  ..., -0.0161,  0.0021,  0.0088],\n","                      [ 0.0070,  0.0042,  0.0009,  ...,  0.0211, -0.0048, -0.0056]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias',\n","              tensor([ 0.0325, -0.0564,  0.0427,  ...,  0.0106,  0.0170,  0.0162],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight',\n","              tensor([[ 0.0278,  0.0099, -0.0125,  ...,  0.0294, -0.0162, -0.0073],\n","                      [-0.0057, -0.0090,  0.0065,  ..., -0.0084, -0.0101, -0.0007],\n","                      [ 0.0078,  0.0194, -0.0038,  ...,  0.0068, -0.0435,  0.0242],\n","                      ...,\n","                      [ 0.0007, -0.0057, -0.0251,  ...,  0.0002,  0.0150, -0.0115],\n","                      [-0.0112,  0.0103,  0.0116,  ...,  0.0309, -0.0334,  0.0352],\n","                      [-0.0216,  0.0061,  0.0181,  ..., -0.0249, -0.0087, -0.0146]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias',\n","              tensor([-0.0669, -0.0082,  0.0167,  ..., -0.4785, -0.1455, -0.0986],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight',\n","              tensor([[ 0.0092, -0.0505, -0.0352,  ...,  0.0066,  0.0212,  0.0051],\n","                      [-0.0233,  0.0066, -0.0125,  ...,  0.0014, -0.0013, -0.0045],\n","                      [-0.0107,  0.0359, -0.0260,  ...,  0.0063, -0.0014,  0.0203],\n","                      ...,\n","                      [ 0.0226,  0.0193, -0.0001,  ..., -0.0076, -0.0028, -0.0139],\n","                      [ 0.0508,  0.0273, -0.0077,  ..., -0.0190,  0.0137, -0.0107],\n","                      [-0.0540,  0.0075,  0.0043,  ...,  0.0229, -0.0325,  0.0095]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias',\n","              tensor([-0.0522, -0.0288, -0.2559,  ..., -0.0161,  0.0386,  0.1250],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.2.layer_norm1.weight',\n","              tensor([ 0.9727,  0.4395, -0.0030,  ...,  0.6250,  0.2969,  0.5117],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.2.layer_norm1.bias',\n","              tensor([-0.0312, -0.0069,  0.0004,  ..., -0.0035, -0.0048,  0.0742],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight',\n","              tensor([[-0.0157,  0.0021,  0.0084,  ..., -0.0150,  0.0369,  0.0098],\n","                      [ 0.0129,  0.0008,  0.0208,  ..., -0.0116, -0.0069,  0.0016],\n","                      [-0.0186,  0.0586, -0.0157,  ...,  0.0113, -0.0071, -0.0068],\n","                      ...,\n","                      [-0.0231,  0.0291,  0.0092,  ...,  0.0244,  0.0400, -0.0109],\n","                      [-0.0221,  0.0258,  0.0027,  ..., -0.0148,  0.0236,  0.0383],\n","                      [-0.0048,  0.0177,  0.0027,  ..., -0.0447, -0.0132,  0.0228]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias',\n","              tensor([-2.4844, -2.1250, -1.0703,  ..., -0.5078, -3.4219, -3.2812],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight',\n","              tensor([[ 6.6833e-03,  1.6327e-03,  1.3428e-02,  ...,  3.3417e-03,\n","                       -2.1118e-02, -1.0437e-02],\n","                      [-3.2196e-03,  1.3367e-02, -2.3193e-02,  ..., -5.4240e-06,\n","                        7.0801e-03,  4.9561e-02],\n","                      [-3.7537e-03, -3.6011e-03, -2.2339e-02,  ...,  1.1230e-02,\n","                        1.4038e-03, -1.3611e-02],\n","                      ...,\n","                      [-2.9541e-02,  1.5320e-02,  5.4321e-03,  ...,  1.5015e-02,\n","                        2.4536e-02, -5.8838e-02],\n","                      [ 1.8311e-02,  1.4099e-02,  1.8677e-02,  ...,  2.8229e-04,\n","                       -9.0942e-03,  2.8076e-02],\n","                      [-1.6785e-03,  1.8997e-03, -7.5340e-05,  ...,  3.1738e-02,\n","                        8.2397e-04,  3.0273e-02]], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias',\n","              tensor([ 0.2402,  0.2910,  0.0276,  ..., -0.1367, -0.2539,  0.0153],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.2.layer_norm2.weight',\n","              tensor([1.7891, 0.7812, 0.3613,  ..., 1.4141, 0.2637, 0.9648],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.2.layer_norm2.bias',\n","              tensor([ 0.0520,  0.0058,  0.0344,  ...,  0.0101, -0.0679,  0.0123],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight',\n","              tensor([[ 0.0019, -0.0030, -0.0131,  ...,  0.0054, -0.0208,  0.0044],\n","                      [ 0.0203,  0.0299,  0.0206,  ...,  0.0066, -0.0339,  0.0089],\n","                      [-0.0148, -0.0420,  0.0073,  ..., -0.0017,  0.0522,  0.0026],\n","                      ...,\n","                      [-0.0098, -0.0135, -0.0128,  ...,  0.0156, -0.0491, -0.0049],\n","                      [-0.0078, -0.0009, -0.0125,  ...,  0.0052,  0.0233, -0.0086],\n","                      [-0.0192, -0.0228, -0.0055,  ...,  0.0029, -0.0078, -0.0168]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias',\n","              tensor([-0.6719,  0.8828, -0.8008,  ..., -0.5508, -1.0547, -1.6094],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight',\n","              tensor([[-0.0015, -0.0067,  0.0112,  ..., -0.0035, -0.0140, -0.0623],\n","                      [-0.0063,  0.0173, -0.0150,  ...,  0.0161, -0.0017, -0.0016],\n","                      [-0.0111, -0.0234, -0.0199,  ...,  0.0103, -0.0077, -0.0110],\n","                      ...,\n","                      [-0.0103,  0.0002,  0.0062,  ..., -0.0117, -0.0101,  0.0113],\n","                      [-0.0074,  0.0090, -0.0164,  ...,  0.0645,  0.0294, -0.0688],\n","                      [-0.0007, -0.0014,  0.0027,  ...,  0.0204, -0.0027,  0.0243]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias',\n","              tensor([ 0.0008,  0.0278,  0.0144,  ..., -0.0084, -0.0155,  0.0087],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight',\n","              tensor([[-0.0033, -0.0271, -0.0300,  ..., -0.0112,  0.0010, -0.0012],\n","                      [-0.0173,  0.0050,  0.0034,  ..., -0.0144,  0.0057,  0.0104],\n","                      [ 0.0010,  0.0046,  0.0403,  ..., -0.0250, -0.0205,  0.0143],\n","                      ...,\n","                      [ 0.0022, -0.0214, -0.0107,  ...,  0.0053,  0.0112,  0.0171],\n","                      [-0.0008,  0.0024,  0.0090,  ..., -0.0075, -0.0322,  0.0103],\n","                      [ 0.0190,  0.0070,  0.0010,  ..., -0.0107, -0.0004,  0.0255]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias',\n","              tensor([ 0.0014, -0.0369,  0.2295,  ..., -0.1953,  0.6406,  0.1973],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight',\n","              tensor([[ 0.0037, -0.0133,  0.0005,  ..., -0.0236, -0.0198, -0.0034],\n","                      [ 0.0220, -0.0021,  0.0192,  ...,  0.0093, -0.0098,  0.0282],\n","                      [-0.0209,  0.0208,  0.0035,  ..., -0.0063, -0.0148, -0.0129],\n","                      ...,\n","                      [-0.0229, -0.0280,  0.0148,  ...,  0.0031, -0.0272, -0.0175],\n","                      [ 0.0243,  0.0299, -0.0004,  ...,  0.0243,  0.0078,  0.0253],\n","                      [ 0.0481,  0.0110,  0.0090,  ..., -0.0229,  0.0052, -0.0303]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias',\n","              tensor([ 0.0566,  0.1494, -0.1816,  ..., -0.1064,  0.0757,  0.0288],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.3.layer_norm1.weight',\n","              tensor([0.8164, 0.4980, 0.2637,  ..., 0.9141, 0.3164, 0.6680],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.3.layer_norm1.bias',\n","              tensor([ 0.0830,  0.0459, -0.0298,  ..., -0.0835,  0.0271,  0.0967],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight',\n","              tensor([[-0.0101,  0.0028,  0.0159,  ...,  0.0089, -0.0143, -0.0287],\n","                      [-0.0054, -0.0400,  0.0388,  ...,  0.0439,  0.0143, -0.0048],\n","                      [-0.0259, -0.0184,  0.0176,  ..., -0.0405,  0.0166, -0.0120],\n","                      ...,\n","                      [ 0.0110, -0.0306, -0.0107,  ...,  0.0137, -0.0146, -0.0009],\n","                      [-0.0004,  0.0400,  0.0150,  ..., -0.0227,  0.0190,  0.0007],\n","                      [-0.0104, -0.0244, -0.0183,  ..., -0.0201,  0.0244, -0.0145]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias',\n","              tensor([-0.8594, -2.9219, -0.7266,  ..., -3.1406, -0.7461, -3.4531],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight',\n","              tensor([[-0.0089, -0.0101,  0.0417,  ..., -0.0030,  0.0193, -0.0139],\n","                      [ 0.0150, -0.0381, -0.0015,  ...,  0.0027, -0.0059,  0.0028],\n","                      [-0.0034,  0.0315,  0.0080,  ..., -0.0078, -0.0128,  0.0096],\n","                      ...,\n","                      [ 0.0126,  0.0400,  0.0004,  ...,  0.0112,  0.0260,  0.0037],\n","                      [ 0.0123, -0.0014, -0.0189,  ..., -0.0123, -0.0049,  0.0223],\n","                      [-0.0066, -0.0082,  0.0053,  ...,  0.0179, -0.0101, -0.0120]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias',\n","              tensor([-0.1318, -0.0090,  0.1572,  ...,  0.0413,  0.0194, -0.0243],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.3.layer_norm2.weight',\n","              tensor([1.1328, 0.9219, 0.5234,  ..., 1.2344, 0.5859, 1.0781],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.3.layer_norm2.bias',\n","              tensor([ 0.1348, -0.0898,  0.0388,  ...,  0.0420, -0.0104,  0.1206],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight',\n","              tensor([[ 0.0327, -0.0273, -0.0049,  ...,  0.0064, -0.0461, -0.0108],\n","                      [-0.0210,  0.0070, -0.0100,  ...,  0.0215,  0.0342, -0.0004],\n","                      [-0.0229,  0.0036, -0.0562,  ...,  0.0072, -0.0135,  0.0029],\n","                      ...,\n","                      [ 0.0286, -0.0219,  0.0010,  ..., -0.0361,  0.0081,  0.0081],\n","                      [ 0.0091, -0.0120, -0.0050,  ..., -0.0063, -0.0002, -0.0134],\n","                      [-0.0181,  0.0256,  0.0096,  ..., -0.0044,  0.0295,  0.0029]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias',\n","              tensor([ 0.3535,  0.5195,  0.0835,  ..., -0.0029, -0.2334, -0.9141],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight',\n","              tensor([[-0.0156,  0.0051, -0.0028,  ...,  0.0076,  0.0195, -0.0359],\n","                      [ 0.0176,  0.0037, -0.0120,  ..., -0.0087, -0.0420,  0.0062],\n","                      [ 0.0212, -0.0040,  0.0408,  ...,  0.0262,  0.0028,  0.0302],\n","                      ...,\n","                      [ 0.0148,  0.0029,  0.0148,  ...,  0.0096, -0.0173, -0.0141],\n","                      [ 0.0065, -0.0102, -0.0099,  ...,  0.0376, -0.0087,  0.0049],\n","                      [ 0.0055,  0.0162,  0.0143,  ...,  0.0090,  0.0076, -0.0022]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias',\n","              tensor([-0.0236, -0.0571,  0.0510,  ..., -0.0459,  0.0610,  0.2891],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight',\n","              tensor([[-0.0327, -0.0046,  0.0322,  ...,  0.0195,  0.0007,  0.0165],\n","                      [-0.0015, -0.0229,  0.0256,  ..., -0.0259,  0.0142, -0.0140],\n","                      [ 0.0356, -0.0165, -0.0059,  ...,  0.0320,  0.0157,  0.0187],\n","                      ...,\n","                      [ 0.0143, -0.0439,  0.0081,  ..., -0.0344, -0.0308,  0.0136],\n","                      [-0.0082,  0.0035, -0.0312,  ..., -0.0054, -0.0032, -0.0132],\n","                      [-0.0130,  0.0403,  0.0165,  ...,  0.0050, -0.0060,  0.0065]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias',\n","              tensor([-0.1660,  0.3496, -0.3145,  ..., -0.3398, -0.2734,  0.1118],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight',\n","              tensor([[ 1.5991e-02,  7.3853e-03, -1.5015e-02,  ..., -2.6123e-02,\n","                       -1.4465e-02,  9.7656e-03],\n","                      [-9.0332e-03, -5.3711e-03,  2.0508e-02,  ...,  8.3008e-03,\n","                       -2.5146e-02,  1.2085e-02],\n","                      [-1.7212e-02,  4.1504e-03, -8.4839e-03,  ..., -2.5513e-02,\n","                        8.3618e-03, -2.9297e-03],\n","                      ...,\n","                      [-1.4709e-02,  1.3245e-02, -2.9419e-02,  ..., -2.0752e-02,\n","                       -1.4648e-02, -1.6724e-02],\n","                      [-1.9897e-02,  1.0193e-02,  1.8677e-02,  ...,  2.6245e-02,\n","                       -6.3477e-03,  3.2654e-03],\n","                      [ 1.5411e-03,  7.1411e-03, -9.5825e-03,  ...,  2.8687e-02,\n","                       -3.5095e-03,  6.6757e-05]], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias',\n","              tensor([-0.0505,  0.0047, -0.0574,  ...,  0.0444, -0.1475, -0.3105],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.4.layer_norm1.weight',\n","              tensor([0.9414, 0.7461, 0.6094,  ..., 1.0156, 0.5195, 0.9922],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.4.layer_norm1.bias',\n","              tensor([ 0.0153,  0.0425,  0.0159,  ..., -0.0776, -0.0168,  0.0306],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight',\n","              tensor([[-0.0189,  0.0014,  0.0236,  ...,  0.0061,  0.0016,  0.0022],\n","                      [ 0.0118,  0.0089,  0.0420,  ..., -0.0172, -0.0222,  0.0280],\n","                      [ 0.0289, -0.0021, -0.0175,  ..., -0.0126, -0.0118,  0.0349],\n","                      ...,\n","                      [ 0.0255,  0.0767,  0.0031,  ..., -0.0078,  0.0004, -0.0182],\n","                      [ 0.0227, -0.0121,  0.0076,  ..., -0.0013,  0.0410,  0.0050],\n","                      [-0.0008, -0.0061,  0.0008,  ...,  0.0205,  0.0031, -0.0262]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias',\n","              tensor([-0.7305, -1.5156, -1.1328,  ..., -1.0859, -1.2422, -1.1016],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight',\n","              tensor([[ 0.0457, -0.0193, -0.0206,  ..., -0.0156, -0.0203,  0.0187],\n","                      [ 0.0161,  0.0012, -0.0013,  ..., -0.0481, -0.0275,  0.0121],\n","                      [ 0.0062,  0.0041,  0.0069,  ..., -0.0099,  0.0046, -0.0061],\n","                      ...,\n","                      [ 0.0114, -0.0016,  0.0045,  ..., -0.0055, -0.0103, -0.0040],\n","                      [ 0.0014,  0.0101,  0.0378,  ..., -0.0071, -0.0121, -0.0065],\n","                      [-0.0006,  0.0347,  0.0186,  ...,  0.0327, -0.0093,  0.0181]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias',\n","              tensor([-0.0238, -0.0264,  0.1377,  ...,  0.0027, -0.0244, -0.1084],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.4.layer_norm2.weight',\n","              tensor([1.0000, 0.8711, 0.6836,  ..., 1.0234, 0.8164, 1.0078],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.4.layer_norm2.bias',\n","              tensor([ 0.0864,  0.0864,  0.0718,  ..., -0.1650,  0.1758,  0.3496],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight',\n","              tensor([[ 0.0063,  0.0066, -0.0114,  ..., -0.0052, -0.0004,  0.0077],\n","                      [-0.0070, -0.0044, -0.0225,  ..., -0.0057,  0.0060, -0.0055],\n","                      [ 0.0078, -0.0190,  0.0082,  ..., -0.0102,  0.0260,  0.0239],\n","                      ...,\n","                      [-0.0352, -0.0045, -0.0121,  ..., -0.0212,  0.0019, -0.0107],\n","                      [-0.0004, -0.0132,  0.0383,  ...,  0.0239, -0.0181, -0.0233],\n","                      [ 0.0124,  0.0006, -0.0074,  ...,  0.0044,  0.0171,  0.0127]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias',\n","              tensor([-0.4512,  0.1445,  1.1797,  ...,  0.3906, -0.6484, -0.1157],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight',\n","              tensor([[-4.1504e-02, -2.8198e-02,  1.2878e-02,  ...,  3.3203e-02,\n","                       -4.4189e-02, -2.7008e-03],\n","                      [ 2.7161e-03,  1.6022e-03,  6.1035e-03,  ..., -7.6904e-03,\n","                       -8.0490e-04, -1.5259e-02],\n","                      [ 3.3722e-03,  1.4954e-02, -9.0942e-03,  ...,  4.3701e-02,\n","                       -1.0071e-02, -8.4400e-05],\n","                      ...,\n","                      [-3.0273e-02, -1.7578e-02,  3.9062e-02,  ..., -2.4048e-02,\n","                        2.3926e-02,  7.2632e-03],\n","                      [ 5.9128e-04,  3.7384e-03,  1.7822e-02,  ...,  1.1536e-02,\n","                       -3.3691e-02, -3.4180e-02],\n","                      [-7.6294e-03,  2.9053e-02, -2.4292e-02,  ..., -4.1016e-02,\n","                       -9.3994e-03,  3.4912e-02]], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias',\n","              tensor([-0.0361,  0.0004, -0.0033,  ...,  0.0479, -0.1030,  0.0068],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight',\n","              tensor([[-0.0121,  0.0130, -0.0030,  ..., -0.0060,  0.0021, -0.0236],\n","                      [-0.0133,  0.0140, -0.0249,  ..., -0.0037,  0.0055,  0.0168],\n","                      [-0.0014, -0.0092, -0.0033,  ...,  0.0161, -0.0096,  0.0002],\n","                      ...,\n","                      [-0.0094, -0.0131,  0.0219,  ...,  0.0259, -0.0092, -0.0024],\n","                      [-0.0066,  0.0053,  0.0168,  ..., -0.0211, -0.0289,  0.0063],\n","                      [ 0.0201, -0.0139, -0.0275,  ..., -0.0172, -0.0007, -0.0057]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias',\n","              tensor([ 0.0601,  0.0322, -0.0098,  ...,  0.0349,  0.0459,  0.0277],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight',\n","              tensor([[ 0.0045,  0.0012, -0.0160,  ...,  0.0123, -0.0238, -0.0317],\n","                      [-0.0311, -0.0096, -0.0054,  ...,  0.0291, -0.0171, -0.0203],\n","                      [ 0.0014,  0.0181,  0.0088,  ..., -0.0001, -0.0430, -0.0021],\n","                      ...,\n","                      [-0.0132,  0.0134, -0.0150,  ...,  0.0330,  0.0198,  0.0051],\n","                      [ 0.0354, -0.0051,  0.0074,  ..., -0.0081,  0.0259,  0.0044],\n","                      [-0.0194, -0.0025, -0.0019,  ...,  0.0212, -0.0087, -0.0160]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias',\n","              tensor([ 0.1006,  0.1689, -0.0317,  ..., -0.0796, -0.0243, -0.0581],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.5.layer_norm1.weight',\n","              tensor([0.7539, 0.5312, 0.6602,  ..., 0.7383, 0.5586, 0.7539],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.5.layer_norm1.bias',\n","              tensor([-0.0239,  0.0461,  0.0398,  ..., -0.0811,  0.0065,  0.0239],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight',\n","              tensor([[-0.0179, -0.0137, -0.0156,  ..., -0.0225, -0.0184, -0.0449],\n","                      [-0.0092, -0.0330, -0.0021,  ...,  0.0255, -0.0024,  0.0244],\n","                      [-0.0208, -0.0084,  0.0088,  ..., -0.0417, -0.0240, -0.0060],\n","                      ...,\n","                      [ 0.0183, -0.0133,  0.0126,  ..., -0.0275, -0.0018,  0.0028],\n","                      [-0.0247,  0.0280, -0.0119,  ..., -0.0294, -0.0099, -0.0223],\n","                      [ 0.0035, -0.0037, -0.0166,  ...,  0.0165, -0.0223, -0.0037]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias',\n","              tensor([-2.8438, -1.4531, -1.1875,  ..., -0.9922, -2.8438, -1.7109],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight',\n","              tensor([[-0.0002, -0.0094,  0.0115,  ...,  0.0113, -0.0325,  0.0173],\n","                      [ 0.0444, -0.0203,  0.0002,  ...,  0.0110, -0.0056, -0.0193],\n","                      [ 0.0178,  0.0046,  0.0194,  ...,  0.0159,  0.0162, -0.0143],\n","                      ...,\n","                      [-0.0248,  0.0229, -0.0009,  ...,  0.0166, -0.0142, -0.0103],\n","                      [-0.0449,  0.0023, -0.0018,  ...,  0.0188, -0.0061,  0.0040],\n","                      [-0.0090,  0.0182, -0.0085,  ..., -0.0088, -0.0288, -0.0081]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias',\n","              tensor([ 0.1123,  0.2734,  0.2100,  ..., -0.2715,  0.1777, -0.0305],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.5.layer_norm2.weight',\n","              tensor([0.9375, 0.9023, 0.8242,  ..., 0.9492, 0.7656, 1.0078],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.5.layer_norm2.bias',\n","              tensor([-0.1494, -0.1172,  0.0610,  ..., -0.0396,  0.0008,  0.0332],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight',\n","              tensor([[-0.0160, -0.0122, -0.0143,  ..., -0.0063,  0.0215,  0.0009],\n","                      [-0.0069,  0.0096, -0.0199,  ..., -0.0354, -0.0349,  0.0082],\n","                      [-0.0417,  0.0138,  0.0083,  ...,  0.0171,  0.0069,  0.0019],\n","                      ...,\n","                      [ 0.0145,  0.0408,  0.0087,  ...,  0.0166,  0.0140, -0.0029],\n","                      [-0.0046,  0.0043, -0.0036,  ..., -0.0025, -0.0135,  0.0022],\n","                      [-0.0105, -0.0006,  0.0103,  ...,  0.0198,  0.0121, -0.0022]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias',\n","              tensor([ 0.6914,  0.4551,  0.3203,  ..., -1.0000,  0.7461,  1.2812],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight',\n","              tensor([[ 0.0267, -0.0106,  0.0115,  ...,  0.0132,  0.0090,  0.0087],\n","                      [-0.0503, -0.0135,  0.0178,  ..., -0.0108,  0.0004, -0.0101],\n","                      [ 0.0127, -0.0181, -0.0022,  ...,  0.0361, -0.0138, -0.0110],\n","                      ...,\n","                      [ 0.0104,  0.0159,  0.0091,  ..., -0.0076, -0.0012,  0.0408],\n","                      [ 0.0091, -0.0234,  0.0022,  ..., -0.0359, -0.0165,  0.0386],\n","                      [-0.0018,  0.0093,  0.0057,  ..., -0.0332, -0.0260, -0.0092]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias',\n","              tensor([-0.0234,  0.1533, -0.0659,  ...,  0.0112,  0.0114,  0.0028],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight',\n","              tensor([[-0.0082, -0.0084,  0.0132,  ...,  0.0361,  0.0262,  0.0098],\n","                      [-0.0103, -0.0148,  0.0054,  ...,  0.0077, -0.0283,  0.0275],\n","                      [-0.0442, -0.0253, -0.0098,  ...,  0.0020,  0.0354, -0.0396],\n","                      ...,\n","                      [ 0.0032,  0.0171, -0.0045,  ...,  0.0121, -0.0261, -0.0033],\n","                      [-0.0014,  0.0352, -0.0327,  ..., -0.0093, -0.0067, -0.0052],\n","                      [-0.0415, -0.0096,  0.0245,  ...,  0.0127, -0.0011,  0.0164]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias',\n","              tensor([ 0.1123, -0.4688, -0.0066,  ...,  0.0889, -0.0272, -0.0056],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight',\n","              tensor([[ 0.0070,  0.0081, -0.0015,  ...,  0.0120,  0.0122,  0.0040],\n","                      [ 0.0175,  0.0056,  0.0020,  ..., -0.0105,  0.0454, -0.0371],\n","                      [-0.0172,  0.0034,  0.0117,  ..., -0.0141, -0.0012,  0.0150],\n","                      ...,\n","                      [-0.0019,  0.0047, -0.0139,  ..., -0.0037,  0.0182,  0.0142],\n","                      [-0.0073,  0.0121, -0.0052,  ..., -0.0053,  0.0179,  0.0126],\n","                      [ 0.0111,  0.0049,  0.0107,  ..., -0.0045, -0.0413, -0.0159]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias',\n","              tensor([ 0.0181,  0.1299,  0.0396,  ...,  0.0243, -0.1001,  0.2520],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.6.layer_norm1.weight',\n","              tensor([0.7930, 0.5977, 0.7891,  ..., 0.6719, 0.5898, 0.6289],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.6.layer_norm1.bias',\n","              tensor([-0.0703,  0.0047,  0.0869,  ..., -0.0713, -0.0184,  0.0806],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight',\n","              tensor([[-0.0002,  0.0265, -0.0027,  ...,  0.0010, -0.0498,  0.0203],\n","                      [-0.0048, -0.0209, -0.0269,  ..., -0.0009,  0.0190,  0.0058],\n","                      [-0.0148, -0.0089, -0.0095,  ..., -0.0427,  0.0145, -0.0289],\n","                      ...,\n","                      [-0.0216, -0.0082,  0.0094,  ...,  0.0053, -0.0276, -0.0126],\n","                      [ 0.0615, -0.0091,  0.0126,  ...,  0.0420,  0.0449,  0.0009],\n","                      [ 0.0542, -0.0121,  0.0028,  ...,  0.0189, -0.0154, -0.0525]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias',\n","              tensor([-1.1172, -3.2031, -0.7695,  ..., -3.0781, -0.9141, -1.3672],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight',\n","              tensor([[-0.0105, -0.0192,  0.0271,  ..., -0.0349, -0.0080,  0.0115],\n","                      [-0.0069, -0.0219,  0.0237,  ...,  0.0118, -0.0137, -0.0049],\n","                      [-0.0192,  0.0033, -0.0022,  ...,  0.0167,  0.0258, -0.0059],\n","                      ...,\n","                      [-0.0007, -0.0162,  0.0199,  ...,  0.0042,  0.0084,  0.0096],\n","                      [ 0.0244,  0.0405, -0.0053,  ..., -0.0102, -0.0103,  0.0088],\n","                      [-0.0164, -0.0007,  0.0014,  ...,  0.0245,  0.0187, -0.0255]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias',\n","              tensor([ 0.0659, -0.0579,  0.0996,  ...,  0.0684,  0.0500,  0.1201],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.6.layer_norm2.weight',\n","              tensor([0.7930, 0.7773, 0.8281,  ..., 0.7695, 0.7305, 0.8867],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.6.layer_norm2.bias',\n","              tensor([-0.0366, -0.1416,  0.1035,  ..., -0.1445,  0.0654, -0.2539],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight',\n","              tensor([[ 0.0092,  0.0076, -0.0085,  ...,  0.0042, -0.0325, -0.0106],\n","                      [ 0.0153,  0.0233, -0.0001,  ..., -0.0130, -0.0098, -0.0198],\n","                      [-0.0145,  0.0219,  0.0256,  ..., -0.0003, -0.0033,  0.0164],\n","                      ...,\n","                      [-0.0002,  0.0151,  0.0050,  ...,  0.0088, -0.0112,  0.0046],\n","                      [ 0.0277,  0.0105,  0.0133,  ...,  0.0342,  0.0271,  0.0121],\n","                      [ 0.0031,  0.0186,  0.0107,  ...,  0.0104,  0.0222, -0.0004]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias',\n","              tensor([-0.9961,  0.2988,  1.0000,  ...,  0.0452, -0.4473, -0.0601],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight',\n","              tensor([[ 0.0067,  0.0117, -0.0214,  ...,  0.0170,  0.0186, -0.0159],\n","                      [-0.0070,  0.0112,  0.0072,  ..., -0.0116, -0.0251,  0.0058],\n","                      [-0.0342,  0.0228,  0.0030,  ...,  0.0098,  0.0115, -0.0182],\n","                      ...,\n","                      [-0.0039,  0.0140, -0.0028,  ..., -0.0082,  0.0118, -0.0308],\n","                      [ 0.0134,  0.0025,  0.0135,  ..., -0.0214,  0.0275, -0.0040],\n","                      [-0.0474,  0.0151, -0.0110,  ..., -0.0211,  0.0016, -0.0193]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias',\n","              tensor([-0.1777,  0.0229, -0.0164,  ..., -0.0505,  0.0054,  0.0483],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight',\n","              tensor([[ 0.0413, -0.0310,  0.0087,  ...,  0.0005, -0.0466, -0.0016],\n","                      [-0.0417, -0.0408, -0.0496,  ..., -0.0103, -0.0020,  0.0198],\n","                      [ 0.0095,  0.0124, -0.0298,  ...,  0.0008,  0.0047, -0.0496],\n","                      ...,\n","                      [ 0.0228, -0.0278, -0.0090,  ...,  0.0006, -0.0148,  0.0332],\n","                      [ 0.0189,  0.0139, -0.0339,  ...,  0.0232,  0.0092,  0.0287],\n","                      [-0.0347,  0.0278, -0.0236,  ..., -0.0038,  0.0046,  0.0093]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias',\n","              tensor([-0.0007, -0.0635,  0.0064,  ..., -0.1885, -0.0569, -0.0234],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight',\n","              tensor([[-2.8564e-02,  2.7832e-02,  1.4572e-03,  ..., -1.2939e-02,\n","                       -6.2866e-03,  3.5645e-02],\n","                      [ 2.5513e-02, -2.3804e-02, -1.5747e-02,  ..., -1.6113e-02,\n","                       -1.5869e-03, -2.6584e-05],\n","                      [ 1.6479e-02, -1.6357e-02, -4.5166e-02,  ...,  2.5635e-03,\n","                       -6.8054e-03,  2.4292e-02],\n","                      ...,\n","                      [ 8.5449e-03, -5.9509e-03, -3.0518e-02,  ..., -7.5684e-03,\n","                        2.8076e-02, -1.2573e-02],\n","                      [ 2.4109e-03, -2.1515e-03,  5.5237e-03,  ...,  1.9287e-02,\n","                       -3.2501e-03,  2.4292e-02],\n","                      [-2.4902e-02,  1.4877e-03,  1.9775e-02,  ...,  2.4780e-02,\n","                        4.9133e-03,  6.8970e-03]], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias',\n","              tensor([-0.0801, -0.0442,  0.0811,  ...,  0.0752,  0.0289, -0.1235],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.7.layer_norm1.weight',\n","              tensor([0.6523, 0.5938, 0.7500,  ..., 0.5820, 0.5430, 0.5586],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.7.layer_norm1.bias',\n","              tensor([-0.0481, -0.0354,  0.0620,  ..., -0.0618, -0.0061,  0.0106],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight',\n","              tensor([[ 0.0217,  0.0172, -0.0302,  ...,  0.0067,  0.0093,  0.0312],\n","                      [ 0.0057,  0.0242, -0.0050,  ...,  0.0117, -0.0125,  0.0090],\n","                      [ 0.0293,  0.0142, -0.0216,  ..., -0.0151,  0.0369, -0.0052],\n","                      ...,\n","                      [ 0.0035, -0.0122,  0.0205,  ...,  0.0034,  0.0070,  0.0234],\n","                      [ 0.0359, -0.0011,  0.0258,  ..., -0.0503,  0.0352,  0.0211],\n","                      [-0.0320, -0.0021, -0.0165,  ...,  0.0175,  0.0049,  0.0287]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias',\n","              tensor([-1.5859, -2.7188, -1.6875,  ..., -1.0000, -0.7539, -0.9805],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight',\n","              tensor([[-1.3977e-02,  1.3580e-03, -3.5095e-03,  ...,  4.1992e-02,\n","                        2.0508e-02, -2.3041e-03],\n","                      [-3.3417e-03,  3.1006e-02,  1.7944e-02,  ..., -1.5625e-02,\n","                        2.3926e-02, -1.8311e-02],\n","                      [-2.9175e-02,  5.0659e-03,  2.3560e-02,  ...,  2.1240e-02,\n","                        9.8267e-03, -2.8564e-02],\n","                      ...,\n","                      [-3.9551e-02,  2.8564e-02,  2.5757e-02,  ...,  7.6771e-05,\n","                       -5.0354e-03, -3.1471e-04],\n","                      [ 1.6479e-02,  1.3550e-02, -1.6602e-02,  ...,  2.1118e-02,\n","                        2.4567e-03,  1.5747e-02],\n","                      [ 3.0273e-02, -1.3123e-02,  6.2561e-03,  ...,  2.9541e-02,\n","                       -1.4404e-02, -1.3123e-02]], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias',\n","              tensor([ 0.2178,  0.0149, -0.0708,  ..., -0.0874,  0.4590, -0.4277],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.7.layer_norm2.weight',\n","              tensor([0.7344, 0.9180, 0.9102,  ..., 0.7695, 0.8633, 0.9414],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.7.layer_norm2.bias',\n","              tensor([ 0.1157,  0.0327,  0.0476,  ..., -0.0618, -0.1768,  0.2949],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight',\n","              tensor([[-0.0110,  0.0283, -0.0137,  ..., -0.0271,  0.0052,  0.0066],\n","                      [ 0.0226, -0.0095,  0.0264,  ..., -0.0356, -0.0071,  0.0035],\n","                      [-0.0121,  0.0396, -0.0354,  ...,  0.0037,  0.0208, -0.0087],\n","                      ...,\n","                      [ 0.0143, -0.0227,  0.0186,  ...,  0.0247,  0.0083, -0.0085],\n","                      [ 0.0134,  0.0153,  0.0113,  ...,  0.0159,  0.0072,  0.0153],\n","                      [-0.0108,  0.0017,  0.0250,  ...,  0.0075,  0.0119,  0.0295]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias',\n","              tensor([ 0.1768,  0.2480, -0.3672,  ...,  0.6641,  0.0391,  0.9492],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight',\n","              tensor([[ 0.0417, -0.0015, -0.0140,  ...,  0.0002,  0.0342,  0.0014],\n","                      [-0.0136, -0.0085,  0.0396,  ...,  0.0289, -0.0256, -0.0483],\n","                      [-0.0247,  0.0347,  0.0041,  ...,  0.0001, -0.0100, -0.0276],\n","                      ...,\n","                      [ 0.0369,  0.0231,  0.0050,  ...,  0.0038, -0.0099,  0.0209],\n","                      [ 0.0167,  0.0005,  0.0366,  ...,  0.0405,  0.0067,  0.0115],\n","                      [ 0.0157,  0.0024, -0.0189,  ...,  0.0118, -0.0103,  0.0020]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias',\n","              tensor([ 0.0123, -0.0457,  0.0079,  ..., -0.0776, -0.0141,  0.0178],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight',\n","              tensor([[-0.0063,  0.0178,  0.0239,  ..., -0.0016, -0.0228, -0.0010],\n","                      [-0.0178, -0.0188, -0.0371,  ...,  0.0144, -0.0131, -0.0193],\n","                      [-0.0164, -0.0349, -0.0038,  ..., -0.0016,  0.0030,  0.0128],\n","                      ...,\n","                      [ 0.0134,  0.0140,  0.0046,  ...,  0.0183,  0.0094, -0.0001],\n","                      [-0.0231,  0.0337,  0.0208,  ...,  0.0101, -0.0203, -0.0056],\n","                      [-0.0237,  0.0145,  0.0124,  ..., -0.0039, -0.0132,  0.0049]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias',\n","              tensor([-0.0248, -0.0913, -0.0625,  ..., -1.3047, -0.1196, -0.1650],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight',\n","              tensor([[-0.0327, -0.0172, -0.0134,  ..., -0.0165, -0.0024,  0.0084],\n","                      [ 0.0100, -0.0204, -0.0099,  ..., -0.0089,  0.0025, -0.0243],\n","                      [ 0.0503, -0.0273, -0.0114,  ..., -0.0234, -0.0072,  0.0128],\n","                      ...,\n","                      [-0.0079, -0.0396, -0.0132,  ...,  0.0098, -0.0250, -0.0059],\n","                      [-0.0503,  0.0172,  0.0310,  ...,  0.0299, -0.0354,  0.0126],\n","                      [ 0.0011,  0.0153,  0.0031,  ..., -0.0165,  0.0182, -0.0021]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias',\n","              tensor([-0.2275,  0.1465, -0.0131,  ..., -0.0884, -0.0518,  0.2559],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.8.layer_norm1.weight',\n","              tensor([0.7461, 0.8242, 0.9414,  ..., 0.6953, 0.7461, 0.7773],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.8.layer_norm1.bias',\n","              tensor([-0.0222,  0.0154,  0.0732,  ..., -0.0854, -0.0074,  0.0532],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight',\n","              tensor([[-0.0469,  0.0013,  0.0130,  ...,  0.0152,  0.0262, -0.0454],\n","                      [-0.0007, -0.0017, -0.0189,  ..., -0.0613, -0.0112,  0.0168],\n","                      [-0.0356, -0.0109, -0.0245,  ...,  0.0559, -0.0152,  0.0520],\n","                      ...,\n","                      [-0.0156,  0.0107, -0.0154,  ...,  0.0101,  0.0322,  0.0063],\n","                      [ 0.0227,  0.0023,  0.0043,  ...,  0.0061, -0.0077, -0.0029],\n","                      [-0.0015, -0.0403,  0.0354,  ..., -0.0049,  0.0082, -0.0175]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias',\n","              tensor([-1.5781, -1.4609, -1.0234,  ..., -1.2812, -2.1406, -0.6211],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight',\n","              tensor([[ 0.0041, -0.0137,  0.0352,  ...,  0.0102,  0.0092,  0.0047],\n","                      [ 0.0145, -0.0068, -0.0049,  ..., -0.0059, -0.0034,  0.0400],\n","                      [-0.0452,  0.0270,  0.0129,  ...,  0.0007,  0.0136, -0.0374],\n","                      ...,\n","                      [-0.0075, -0.0231,  0.0250,  ...,  0.0036, -0.0188,  0.0082],\n","                      [ 0.0074,  0.0200, -0.0139,  ...,  0.0024, -0.0249, -0.0067],\n","                      [ 0.0019,  0.0066,  0.0018,  ..., -0.0160, -0.0066,  0.0294]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias',\n","              tensor([ 0.0015,  0.0703,  0.0153,  ..., -0.2754,  0.0674,  0.2441],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.8.layer_norm2.weight',\n","              tensor([0.6992, 0.9297, 0.9062,  ..., 0.7344, 0.8242, 0.8789],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.8.layer_norm2.bias',\n","              tensor([ 0.1641, -0.1553,  0.0282,  ...,  0.0183,  0.0376, -0.3945],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight',\n","              tensor([[ 0.0175,  0.0258, -0.0020,  ...,  0.0092,  0.0251, -0.0082],\n","                      [ 0.0118, -0.0369,  0.0081,  ...,  0.0206, -0.0173, -0.0042],\n","                      [ 0.0012,  0.0295, -0.0110,  ..., -0.0128,  0.0310,  0.0211],\n","                      ...,\n","                      [ 0.0183, -0.0310,  0.0153,  ...,  0.0083, -0.0187, -0.0049],\n","                      [-0.0211,  0.0320, -0.0139,  ..., -0.0442,  0.0153, -0.0175],\n","                      [-0.0189,  0.0258,  0.0012,  ...,  0.0095, -0.0094,  0.0260]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias',\n","              tensor([-0.6445, -0.6875, -0.1670,  ..., -1.7344, -0.3789, -0.4160],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight',\n","              tensor([[ 0.0020,  0.0308,  0.0100,  ..., -0.0083, -0.0020,  0.0217],\n","                      [-0.0075, -0.0204,  0.0371,  ..., -0.0031, -0.0082,  0.0226],\n","                      [-0.0186,  0.0136, -0.0522,  ...,  0.0103,  0.0131,  0.0162],\n","                      ...,\n","                      [ 0.0092,  0.0066,  0.0217,  ...,  0.0105, -0.0242,  0.0018],\n","                      [-0.0035, -0.0135, -0.0023,  ...,  0.0303, -0.0132,  0.0053],\n","                      [-0.0012, -0.0211,  0.0066,  ..., -0.0060, -0.0259,  0.0250]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias',\n","              tensor([ 0.0122, -0.0299, -0.0864,  ...,  0.0100,  0.0371, -0.1172],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight',\n","              tensor([[-0.0090, -0.0435, -0.0062,  ...,  0.0278,  0.0239, -0.0374],\n","                      [-0.0056, -0.0164,  0.0172,  ...,  0.0273, -0.0330, -0.0120],\n","                      [ 0.0012,  0.0201, -0.0090,  ..., -0.0297,  0.0206,  0.0038],\n","                      ...,\n","                      [ 0.0175,  0.0023,  0.0212,  ...,  0.0016,  0.0117,  0.0206],\n","                      [ 0.0229, -0.0219,  0.0195,  ..., -0.0078, -0.0273, -0.0229],\n","                      [-0.0064, -0.0127,  0.0342,  ..., -0.0547, -0.0131, -0.0089]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias',\n","              tensor([-0.0767,  0.0840,  0.0549,  ..., -0.0383,  0.1445,  0.0791],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight',\n","              tensor([[ 0.0002, -0.0266, -0.0151,  ...,  0.0006, -0.0085,  0.0120],\n","                      [-0.0182, -0.0031, -0.0250,  ..., -0.0093, -0.0110, -0.0051],\n","                      [ 0.0137, -0.0425,  0.0227,  ..., -0.0081, -0.0143, -0.0123],\n","                      ...,\n","                      [ 0.0204,  0.0070,  0.0016,  ...,  0.0006, -0.0327, -0.0085],\n","                      [-0.0140, -0.0037,  0.0171,  ...,  0.0099,  0.0054,  0.0177],\n","                      [-0.0195, -0.0302,  0.0359,  ..., -0.0094, -0.0051, -0.0030]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias',\n","              tensor([-0.1201, -0.0461, -0.1152,  ...,  0.0952, -0.0271, -0.0869],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.9.layer_norm1.weight',\n","              tensor([0.7500, 0.8242, 0.9336,  ..., 0.6602, 0.6719, 0.7734],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.9.layer_norm1.bias',\n","              tensor([ 6.0558e-05, -3.5889e-02,  2.0752e-02,  ...,  4.4556e-03,\n","                       1.4832e-02, -1.9653e-02], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight',\n","              tensor([[-0.0032,  0.0308,  0.0132,  ..., -0.0065,  0.0045, -0.0080],\n","                      [ 0.0311,  0.0374, -0.0013,  ..., -0.0417,  0.0815,  0.0249],\n","                      [ 0.0091, -0.0086, -0.0141,  ..., -0.0214, -0.0009, -0.0383],\n","                      ...,\n","                      [ 0.0154,  0.0242, -0.0082,  ..., -0.0084, -0.0352, -0.0049],\n","                      [ 0.0144,  0.0471, -0.0176,  ..., -0.0322, -0.0026, -0.0513],\n","                      [ 0.0342,  0.0525,  0.0131,  ..., -0.0118,  0.0028,  0.0115]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias',\n","              tensor([-1.0078, -2.2344, -1.1406,  ..., -1.3750, -1.5234, -1.0703],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight',\n","              tensor([[-0.0124,  0.0311,  0.0002,  ...,  0.0085,  0.0082,  0.0476],\n","                      [ 0.0221, -0.0408,  0.0043,  ...,  0.0076,  0.0540,  0.0104],\n","                      [-0.0034, -0.0222,  0.0036,  ..., -0.0284, -0.0259, -0.0233],\n","                      ...,\n","                      [-0.0059, -0.0442, -0.0094,  ..., -0.0081, -0.0447, -0.0084],\n","                      [ 0.0011,  0.0540,  0.0146,  ..., -0.0228, -0.0161,  0.0070],\n","                      [-0.0288,  0.0234, -0.0366,  ..., -0.0167, -0.0437, -0.0262]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias',\n","              tensor([ 0.1289, -0.0659,  0.1973,  ..., -0.1055,  0.0183, -0.3027],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.9.layer_norm2.weight',\n","              tensor([0.7227, 1.0234, 0.9961,  ..., 0.7969, 0.9023, 1.0781],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.9.layer_norm2.bias',\n","              tensor([ 0.0554,  0.1426,  0.1138,  ..., -0.1641, -0.0522,  0.4766],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight',\n","              tensor([[-0.0131,  0.0116, -0.0030,  ..., -0.0047,  0.0041,  0.0298],\n","                      [-0.0131, -0.0146, -0.0356,  ...,  0.0255,  0.0374,  0.0148],\n","                      [ 0.0128,  0.0017, -0.0214,  ..., -0.0052, -0.0016,  0.0197],\n","                      ...,\n","                      [ 0.0299, -0.0216,  0.0050,  ...,  0.0148, -0.0369,  0.0210],\n","                      [ 0.0178,  0.0256,  0.0166,  ..., -0.0030, -0.0195, -0.0383],\n","                      [ 0.0141,  0.0261, -0.0135,  ...,  0.0014,  0.0234,  0.0172]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias',\n","              tensor([ 0.1094, -0.6133,  0.0386,  ..., -0.3008, -0.3789,  0.3027],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight',\n","              tensor([[ 0.0087,  0.0026,  0.0250,  ..., -0.0164,  0.0118,  0.0305],\n","                      [ 0.0105,  0.0022,  0.0082,  ...,  0.0272,  0.0063, -0.0229],\n","                      [-0.0464,  0.0199, -0.0072,  ..., -0.0065,  0.0127,  0.0076],\n","                      ...,\n","                      [-0.0028,  0.0014, -0.0103,  ...,  0.0007, -0.0187,  0.0212],\n","                      [ 0.0210, -0.0092,  0.0361,  ..., -0.0002, -0.0070,  0.0015],\n","                      [-0.0034, -0.0182,  0.0053,  ..., -0.0090, -0.0130, -0.0002]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias',\n","              tensor([ 0.0025,  0.0273,  0.0317,  ..., -0.0110, -0.0112,  0.0090],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight',\n","              tensor([[ 0.0342,  0.0288, -0.0059,  ..., -0.0027, -0.0471, -0.0011],\n","                      [-0.0063, -0.0070,  0.0137,  ...,  0.0254,  0.0598, -0.0219],\n","                      [ 0.0054, -0.0034, -0.0080,  ...,  0.0058, -0.0077, -0.0050],\n","                      ...,\n","                      [ 0.0253, -0.0059, -0.0087,  ..., -0.0175, -0.0178, -0.0527],\n","                      [ 0.0017,  0.0234, -0.0156,  ...,  0.0249, -0.0674, -0.0188],\n","                      [ 0.0048,  0.0028, -0.0025,  ...,  0.0312, -0.0022,  0.0140]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias',\n","              tensor([ 0.0767, -0.2314,  1.5000,  ..., -0.0737, -0.0515, -0.0669],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight',\n","              tensor([[ 0.0026,  0.0147,  0.0057,  ...,  0.0157, -0.0273,  0.0266],\n","                      [-0.0349,  0.0177,  0.0060,  ...,  0.0145,  0.0184, -0.0085],\n","                      [ 0.0093, -0.0186, -0.0051,  ...,  0.0031, -0.0187, -0.0065],\n","                      ...,\n","                      [ 0.0245,  0.0209,  0.0098,  ..., -0.0063,  0.0228,  0.0051],\n","                      [ 0.0016,  0.0079,  0.0184,  ...,  0.0082,  0.0092,  0.0247],\n","                      [ 0.0037,  0.0078, -0.0015,  ...,  0.0201,  0.0181,  0.0107]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias',\n","              tensor([-0.0457,  0.1387,  0.0192,  ..., -0.0684, -0.0342,  0.0742],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.10.layer_norm1.weight',\n","              tensor([0.7383, 0.8633, 0.9414,  ..., 0.5898, 0.7578, 0.8086],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.10.layer_norm1.bias',\n","              tensor([ 0.0015,  0.0039,  0.0330,  ...,  0.0214,  0.0260, -0.0228],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight',\n","              tensor([[ 0.0097,  0.0167,  0.0283,  ..., -0.0270,  0.0057,  0.0181],\n","                      [ 0.0359, -0.0081,  0.0483,  ...,  0.0132,  0.0159,  0.0111],\n","                      [-0.0322,  0.0369,  0.0408,  ...,  0.0050, -0.0115, -0.0082],\n","                      ...,\n","                      [-0.0060,  0.0082,  0.0054,  ..., -0.0400, -0.0112, -0.0087],\n","                      [-0.0225, -0.0231, -0.0144,  ...,  0.0117,  0.0320, -0.0141],\n","                      [-0.0070,  0.0199, -0.0442,  ...,  0.0070,  0.0033, -0.0466]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias',\n","              tensor([-1.8906, -1.8672, -1.3359,  ..., -0.7578, -1.3281, -0.5781],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight',\n","              tensor([[ 0.0253,  0.0427, -0.0177,  ...,  0.0019,  0.0046,  0.0437],\n","                      [ 0.0177, -0.0249,  0.0413,  ...,  0.0123, -0.0110, -0.0096],\n","                      [ 0.0199, -0.0063,  0.0212,  ...,  0.0034,  0.0025,  0.0083],\n","                      ...,\n","                      [-0.0030,  0.0114,  0.0283,  ..., -0.0029, -0.0171, -0.0006],\n","                      [ 0.0273,  0.0098, -0.0193,  ...,  0.0121, -0.0197, -0.0167],\n","                      [ 0.0273, -0.0381, -0.0229,  ..., -0.0097, -0.0149,  0.0029]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias',\n","              tensor([-0.0574,  0.0952,  0.0481,  ...,  0.2061, -0.0315, -0.1895],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.10.layer_norm2.weight',\n","              tensor([0.7148, 0.9883, 0.9570,  ..., 0.7891, 0.8633, 0.9453],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.10.layer_norm2.bias',\n","              tensor([ 0.0640, -0.2812,  0.0635,  ...,  0.1445,  0.0159, -0.0811],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight',\n","              tensor([[-0.0017,  0.0053,  0.0217,  ..., -0.0289,  0.0352, -0.0078],\n","                      [ 0.0422,  0.0149,  0.0247,  ..., -0.0035, -0.0019,  0.0410],\n","                      [-0.0028,  0.0302,  0.0481,  ...,  0.0087, -0.0118,  0.0586],\n","                      ...,\n","                      [-0.0186,  0.0334, -0.0352,  ...,  0.0031,  0.0209,  0.0205],\n","                      [ 0.0085, -0.0037,  0.0156,  ..., -0.0544, -0.0014,  0.0354],\n","                      [ 0.0025, -0.0608, -0.0041,  ...,  0.0013,  0.0383, -0.0481]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias',\n","              tensor([-0.5781,  0.6094,  0.7578,  ..., -0.6445,  0.8594,  0.5586],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight',\n","              tensor([[ 0.0101, -0.0118,  0.0048,  ...,  0.0221,  0.0005,  0.0222],\n","                      [ 0.0030,  0.0352,  0.0332,  ...,  0.0112,  0.0020,  0.0100],\n","                      [-0.0205, -0.0549, -0.0128,  ...,  0.0361,  0.0447, -0.0051],\n","                      ...,\n","                      [ 0.0099, -0.0176,  0.0193,  ...,  0.0099, -0.0193,  0.0288],\n","                      [-0.0131, -0.0126, -0.0104,  ...,  0.0110, -0.0253,  0.0052],\n","                      [-0.0065, -0.0461,  0.0237,  ..., -0.0283, -0.0113,  0.0140]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias',\n","              tensor([0.0127, 0.0649, 0.0064,  ..., 0.0237, 0.0327, 0.0204],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight',\n","              tensor([[-4.5776e-03,  2.1729e-02, -1.4877e-04,  ...,  1.9775e-02,\n","                        4.3945e-03,  2.6123e-02],\n","                      [ 1.2878e-02,  2.6345e-05,  2.0874e-02,  ...,  2.7313e-03,\n","                       -2.5146e-02, -1.0925e-02],\n","                      [-1.2436e-03, -1.1108e-02,  2.0752e-02,  ..., -4.0894e-03,\n","                       -2.6123e-02, -2.9297e-03],\n","                      ...,\n","                      [-5.2734e-02,  1.2207e-02, -2.1973e-02,  ...,  1.7700e-02,\n","                       -4.1504e-03,  4.0283e-03],\n","                      [-3.9978e-03, -2.3315e-02,  3.3936e-02,  ..., -5.4016e-03,\n","                        1.5625e-02,  3.1128e-02],\n","                      [-3.1982e-02, -5.1880e-03, -1.4587e-02,  ..., -9.4604e-03,\n","                        3.0396e-02, -6.3782e-03]], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias',\n","              tensor([ 1.1902e-02,  7.9102e-02,  2.5368e-04,  ...,  3.0859e-01,\n","                      -4.8096e-02, -5.6641e-02], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight',\n","              tensor([[-0.0295,  0.0123, -0.0220,  ..., -0.0147,  0.0127,  0.0138],\n","                      [ 0.0087, -0.0072,  0.0187,  ...,  0.0109, -0.0049, -0.0376],\n","                      [-0.0005, -0.0146,  0.0227,  ..., -0.0013, -0.0127, -0.0103],\n","                      ...,\n","                      [-0.0134, -0.0293, -0.0231,  ...,  0.0231, -0.0093, -0.0356],\n","                      [-0.0447,  0.0427, -0.0515,  ..., -0.0159,  0.0034,  0.0036],\n","                      [-0.0113, -0.0105,  0.0143,  ..., -0.0131, -0.0028, -0.0041]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias',\n","              tensor([-0.1709, -0.2871,  0.1689,  ...,  0.0684, -0.1338,  0.2910],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.11.layer_norm1.weight',\n","              tensor([0.8008, 0.8359, 0.9414,  ..., 0.6562, 0.7383, 0.7539],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.11.layer_norm1.bias',\n","              tensor([-0.0040,  0.0008,  0.0068,  ..., -0.0209,  0.0422, -0.0376],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight',\n","              tensor([[ 1.3916e-02, -3.4668e-02, -2.1973e-02,  ...,  1.0681e-02,\n","                        4.3945e-02, -1.0559e-02],\n","                      [ 1.3489e-02,  1.1658e-02, -1.3062e-02,  ..., -5.9509e-03,\n","                       -1.3916e-02, -3.9864e-04],\n","                      [-1.0803e-02, -1.3123e-02, -2.5391e-02,  ..., -5.6458e-03,\n","                       -1.5869e-02, -6.5613e-03],\n","                      ...,\n","                      [-9.1553e-03, -2.6093e-03,  6.1646e-03,  ...,  4.9353e-05,\n","                        1.4526e-02,  2.0020e-02],\n","                      [-4.5410e-02,  1.2024e-02, -1.3916e-02,  ..., -6.7139e-03,\n","                       -7.2632e-03, -2.8442e-02],\n","                      [-7.9956e-03, -6.5613e-03,  1.6479e-02,  ..., -6.1035e-03,\n","                       -7.1106e-03,  4.0588e-03]], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias',\n","              tensor([-1.0156, -1.6953, -1.0234,  ..., -1.9141, -1.0547, -1.6641],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight',\n","              tensor([[-0.0059,  0.0226, -0.0023,  ...,  0.0068,  0.0073,  0.0203],\n","                      [-0.0103, -0.0084,  0.0320,  ...,  0.0189,  0.0049, -0.0009],\n","                      [ 0.0352, -0.0300,  0.0177,  ..., -0.0067,  0.0028, -0.0149],\n","                      ...,\n","                      [ 0.0153, -0.0110,  0.0151,  ..., -0.0046,  0.0051,  0.0052],\n","                      [-0.0400, -0.0101, -0.0205,  ...,  0.0146, -0.0200, -0.0012],\n","                      [ 0.0114,  0.0080, -0.0269,  ...,  0.0085, -0.0022, -0.0198]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias',\n","              tensor([ 0.0295,  0.0542,  0.0028,  ..., -0.0703,  0.2129, -0.0635],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.11.layer_norm2.weight',\n","              tensor([0.7305, 0.9297, 1.0000,  ..., 0.7852, 0.8438, 0.8555],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.11.layer_norm2.bias',\n","              tensor([ 0.2373,  0.3809, -0.0957,  ...,  0.0061,  0.0840, -0.4492],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight',\n","              tensor([[-0.0532,  0.0020, -0.0171,  ...,  0.0139, -0.0303,  0.0063],\n","                      [ 0.0322, -0.0214,  0.0037,  ..., -0.0102, -0.0173, -0.0052],\n","                      [ 0.0005,  0.0153, -0.0165,  ...,  0.0233, -0.0034,  0.0294],\n","                      ...,\n","                      [ 0.0310, -0.0232, -0.0197,  ..., -0.0144, -0.0054,  0.0273],\n","                      [-0.0262, -0.0182, -0.0046,  ...,  0.0037,  0.0228, -0.0133],\n","                      [ 0.0099,  0.0115, -0.0476,  ...,  0.0228, -0.0033, -0.0383]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias',\n","              tensor([ 0.4844, -0.5547,  0.0664,  ...,  0.4238,  0.7656,  0.0996],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight',\n","              tensor([[-0.0216,  0.0103, -0.0273,  ...,  0.0383, -0.0050,  0.0178],\n","                      [ 0.0236,  0.0427, -0.0054,  ..., -0.0146, -0.0117, -0.0110],\n","                      [ 0.0095, -0.0222, -0.0089,  ...,  0.0197,  0.0093,  0.0302],\n","                      ...,\n","                      [ 0.0175, -0.0092,  0.0074,  ..., -0.0001,  0.0125,  0.0277],\n","                      [ 0.0069,  0.0198, -0.0027,  ..., -0.0020,  0.0344, -0.0190],\n","                      [ 0.0103, -0.0420, -0.0476,  ..., -0.0072, -0.0236, -0.0107]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias',\n","              tensor([-0.0022, -0.0112,  0.0300,  ...,  0.0315, -0.0674, -0.0022],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight',\n","              tensor([[-0.0325,  0.0311,  0.0192,  ...,  0.0018, -0.0273,  0.0238],\n","                      [-0.0092,  0.0052, -0.0167,  ...,  0.0108, -0.0239,  0.0084],\n","                      [-0.0425, -0.0183, -0.0067,  ...,  0.0146,  0.0117, -0.0017],\n","                      ...,\n","                      [ 0.0100, -0.0221,  0.0127,  ..., -0.0048, -0.0243,  0.0520],\n","                      [-0.0564, -0.0060,  0.0154,  ..., -0.0123,  0.0302,  0.0077],\n","                      [ 0.0022,  0.0245, -0.0332,  ..., -0.0024, -0.0038, -0.0038]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias',\n","              tensor([ 0.0205, -0.0947,  0.0491,  ...,  0.1367,  0.0393, -0.0513],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight',\n","              tensor([[ 0.0386, -0.0060,  0.0135,  ..., -0.0109,  0.0038,  0.0078],\n","                      [-0.0203, -0.0149, -0.0012,  ...,  0.0396, -0.0101,  0.0247],\n","                      [ 0.0152, -0.0024,  0.0164,  ...,  0.0020,  0.0117,  0.0187],\n","                      ...,\n","                      [ 0.0021,  0.0413,  0.0391,  ..., -0.0100,  0.0084,  0.0067],\n","                      [ 0.0172,  0.0150,  0.0435,  ..., -0.0093, -0.0270,  0.0171],\n","                      [ 0.0071,  0.0141, -0.0376,  ..., -0.0182,  0.0173,  0.0050]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias',\n","              tensor([-0.2500,  0.0981, -0.1934,  ..., -0.1855,  0.0273,  0.0176],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.12.layer_norm1.weight',\n","              tensor([0.8828, 0.8945, 1.0391,  ..., 0.7148, 0.8281, 0.8320],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.12.layer_norm1.bias',\n","              tensor([-0.0043,  0.0255, -0.0059,  ..., -0.0703,  0.0520, -0.0669],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight',\n","              tensor([[ 0.0022, -0.0022, -0.0160,  ...,  0.0488,  0.0010, -0.0233],\n","                      [ 0.0090,  0.0195, -0.0103,  ..., -0.0143, -0.0388, -0.0013],\n","                      [-0.0369, -0.0114, -0.0258,  ...,  0.0151,  0.0122,  0.0140],\n","                      ...,\n","                      [ 0.0003, -0.0177,  0.0095,  ...,  0.0037, -0.0330, -0.0354],\n","                      [ 0.0021,  0.0120,  0.0159,  ..., -0.0193,  0.0064,  0.0150],\n","                      [ 0.0320,  0.0139,  0.0161,  ..., -0.0179, -0.0238, -0.0089]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias',\n","              tensor([-0.8320, -0.9727, -0.9531,  ..., -1.1250, -0.7461, -0.7891],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight',\n","              tensor([[-0.0078, -0.0011,  0.0106,  ..., -0.0017, -0.0206, -0.0045],\n","                      [-0.0334,  0.0018,  0.0248,  ...,  0.0300,  0.0030,  0.0156],\n","                      [-0.0009, -0.0166, -0.0339,  ...,  0.0006,  0.0005,  0.0457],\n","                      ...,\n","                      [ 0.0159, -0.0047,  0.0029,  ...,  0.0294, -0.0146,  0.0415],\n","                      [-0.0132,  0.0165,  0.0088,  ..., -0.0179, -0.0116, -0.0322],\n","                      [ 0.0054, -0.0209,  0.0061,  ...,  0.0112, -0.0098,  0.0048]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias',\n","              tensor([-0.0060, -0.0304, -0.2734,  ..., -0.0261, -0.0452, -0.3770],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.12.layer_norm2.weight',\n","              tensor([0.7539, 1.0078, 1.2031,  ..., 0.8945, 1.0312, 1.1250],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.12.layer_norm2.bias',\n","              tensor([ 0.3477, -0.2158,  0.6445,  ...,  0.2402,  0.1455,  0.2324],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight',\n","              tensor([[ 0.0070,  0.0091,  0.0157,  ...,  0.0054, -0.0327,  0.0171],\n","                      [ 0.0090,  0.0059,  0.0129,  ..., -0.0115, -0.0072,  0.0114],\n","                      [ 0.0374,  0.0019,  0.0123,  ...,  0.0076, -0.0140, -0.0422],\n","                      ...,\n","                      [ 0.0004, -0.0143, -0.0211,  ...,  0.0208,  0.0146, -0.0466],\n","                      [ 0.0349, -0.0189, -0.0408,  ...,  0.0302,  0.0125, -0.0074],\n","                      [ 0.0369,  0.0228,  0.0137,  ...,  0.0432,  0.0092,  0.0457]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias',\n","              tensor([-0.2422, -0.7109, -0.2891,  ..., -0.2871,  0.4102, -0.2930],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight',\n","              tensor([[ 2.0996e-02, -2.1484e-02,  5.4016e-03,  ...,  6.8665e-03,\n","                        1.0529e-03,  2.3315e-02],\n","                      [-1.7944e-02, -5.1117e-04, -9.2773e-03,  ...,  9.2773e-03,\n","                        1.7578e-02, -1.8433e-02],\n","                      [ 1.8188e-02, -5.3883e-05, -1.0010e-02,  ..., -1.8311e-02,\n","                       -2.2583e-03,  4.0771e-02],\n","                      ...,\n","                      [ 3.1891e-03,  3.4912e-02,  2.6550e-03,  ...,  8.3008e-03,\n","                       -4.4861e-03, -1.2390e-02],\n","                      [ 2.7313e-03, -3.1250e-02, -3.3447e-02,  ...,  2.8229e-03,\n","                        5.8594e-03,  2.6093e-03],\n","                      [-1.9653e-02,  1.3062e-02, -5.8594e-02,  ...,  2.0630e-02,\n","                       -2.2583e-02,  3.3936e-02]], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias',\n","              tensor([-0.0284, -0.0544,  0.0918,  ...,  0.0069, -0.0190,  0.0292],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight',\n","              tensor([[ 0.0513, -0.0143, -0.0004,  ...,  0.0126, -0.0280,  0.0027],\n","                      [-0.0275,  0.0181, -0.0217,  ..., -0.0096, -0.0104, -0.0078],\n","                      [ 0.0403,  0.0276, -0.0176,  ...,  0.0162, -0.0150, -0.0049],\n","                      ...,\n","                      [ 0.0250,  0.0056, -0.0064,  ...,  0.0034,  0.0330, -0.0422],\n","                      [ 0.0146, -0.0322, -0.0078,  ...,  0.0173,  0.0149, -0.0165],\n","                      [ 0.0270,  0.0306, -0.0466,  ...,  0.0388,  0.0006,  0.0236]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias',\n","              tensor([-0.1289,  0.0154, -0.0347,  ..., -0.1318,  0.1021,  0.0947],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight',\n","              tensor([[-0.0204, -0.0007, -0.0261,  ...,  0.0233,  0.0112, -0.0464],\n","                      [ 0.0150,  0.0059, -0.0087,  ..., -0.0391, -0.0016,  0.0022],\n","                      [-0.0024,  0.0154,  0.0072,  ..., -0.0102,  0.0131,  0.0233],\n","                      ...,\n","                      [-0.0084,  0.0043,  0.0023,  ...,  0.0182,  0.0134, -0.0057],\n","                      [-0.0019,  0.0044,  0.0211,  ...,  0.0045, -0.0260,  0.0047],\n","                      [-0.0211,  0.0295, -0.0420,  ..., -0.0063, -0.0035, -0.0250]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias',\n","              tensor([-0.0024,  0.0894,  0.2168,  ..., -0.0139,  0.0161, -0.0549],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.13.layer_norm1.weight',\n","              tensor([0.9297, 0.9375, 1.1172,  ..., 0.7227, 0.8516, 0.9219],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.13.layer_norm1.bias',\n","              tensor([ 0.0942,  0.0281,  0.1299,  ..., -0.0038,  0.0125, -0.1245],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight',\n","              tensor([[ 0.0311,  0.0247, -0.0079,  ..., -0.0115, -0.0075,  0.0154],\n","                      [-0.0381,  0.0156, -0.0123,  ...,  0.0068, -0.0093, -0.0198],\n","                      [ 0.0025,  0.0320, -0.0233,  ..., -0.0013,  0.0103,  0.0059],\n","                      ...,\n","                      [-0.0061,  0.0153, -0.0486,  ...,  0.0139,  0.0327, -0.0242],\n","                      [-0.0179,  0.0095, -0.0310,  ..., -0.0074,  0.0079,  0.0120],\n","                      [ 0.0069,  0.0123, -0.0109,  ...,  0.0162, -0.0206, -0.0055]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias',\n","              tensor([-1.0234, -1.4219, -0.8594,  ..., -1.6250, -1.7500, -0.5156],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight',\n","              tensor([[ 1.3611e-02,  1.9775e-02,  1.8539e-03,  ..., -1.3351e-03,\n","                        1.5991e-02, -9.3384e-03],\n","                      [ 1.7212e-02, -6.6223e-03,  1.9531e-02,  ...,  3.3447e-02,\n","                        9.5825e-03,  2.2217e-02],\n","                      [ 2.0294e-03,  2.7710e-02, -1.3489e-02,  ..., -8.5449e-02,\n","                       -6.2256e-02, -6.8359e-03],\n","                      ...,\n","                      [-4.4861e-03, -1.9409e-02,  1.4221e-02,  ...,  1.9165e-02,\n","                       -2.5269e-02, -3.9062e-03],\n","                      [ 2.6733e-02,  1.7090e-02, -1.4099e-02,  ..., -2.5868e-05,\n","                        1.6968e-02, -2.0905e-03],\n","                      [-2.1729e-02, -1.5137e-02, -3.5400e-03,  ...,  2.6367e-02,\n","                        1.0193e-02, -8.6060e-03]], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias',\n","              tensor([-0.0011, -0.1289, -0.0197,  ..., -0.1611,  0.2139,  0.0210],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.13.layer_norm2.weight',\n","              tensor([0.7500, 1.1953, 1.0859,  ..., 0.9883, 1.0156, 1.1562],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.13.layer_norm2.bias',\n","              tensor([ 0.1660,  0.2432,  0.0259,  ...,  0.3613, -0.0894, -0.4219],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight',\n","              tensor([[ 0.0057, -0.0215,  0.0635,  ...,  0.0060, -0.0125, -0.0308],\n","                      [-0.0004,  0.0500,  0.0154,  ...,  0.0347,  0.0194, -0.0337],\n","                      [ 0.0114, -0.0008,  0.0266,  ..., -0.0457,  0.0193,  0.0403],\n","                      ...,\n","                      [ 0.0051,  0.0273, -0.0142,  ..., -0.0007,  0.0087, -0.0060],\n","                      [ 0.0298, -0.0371, -0.0047,  ..., -0.0097, -0.0251,  0.0378],\n","                      [-0.0115, -0.0280,  0.0228,  ...,  0.0256, -0.0146,  0.0082]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias',\n","              tensor([ 0.8359,  0.1025,  0.1904,  ..., -0.1836, -0.9258, -0.3379],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight',\n","              tensor([[ 0.0005, -0.0175, -0.0080,  ..., -0.0164, -0.0060,  0.0398],\n","                      [ 0.0101,  0.0040,  0.0098,  ...,  0.0124, -0.0239, -0.0464],\n","                      [ 0.0035,  0.0209,  0.0086,  ...,  0.0082,  0.0036,  0.0164],\n","                      ...,\n","                      [ 0.0121,  0.0025, -0.0114,  ..., -0.0270,  0.0113, -0.0101],\n","                      [ 0.0044, -0.0162,  0.0075,  ...,  0.0291, -0.0084,  0.0010],\n","                      [ 0.0087,  0.0249, -0.0383,  ...,  0.0388,  0.0309, -0.0060]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias',\n","              tensor([-0.0354,  0.0131,  0.2129,  ..., -0.0098,  0.0461, -0.0276],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight',\n","              tensor([[-0.0035, -0.0262,  0.0087,  ...,  0.0159,  0.0205, -0.0308],\n","                      [-0.0021,  0.0277,  0.0238,  ...,  0.0004, -0.0420, -0.0273],\n","                      [ 0.0240,  0.0164, -0.0187,  ...,  0.0356, -0.0297,  0.0176],\n","                      ...,\n","                      [ 0.0391, -0.0104,  0.0048,  ...,  0.0049,  0.0386, -0.0215],\n","                      [ 0.0001, -0.0055,  0.0003,  ..., -0.0049, -0.0128,  0.0273],\n","                      [-0.0293,  0.0239, -0.0119,  ..., -0.0359, -0.0139, -0.0294]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias',\n","              tensor([-0.0544, -0.0221, -0.0452,  ...,  0.0874, -0.0645, -0.0027],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight',\n","              tensor([[ 0.0374, -0.0183,  0.0123,  ..., -0.0168, -0.0157,  0.0010],\n","                      [-0.0179,  0.0179, -0.0064,  ..., -0.0113,  0.0084,  0.0240],\n","                      [ 0.0236, -0.0181,  0.0009,  ..., -0.0039, -0.0082, -0.0236],\n","                      ...,\n","                      [ 0.0284,  0.0020, -0.0256,  ...,  0.0469, -0.0145,  0.0091],\n","                      [ 0.0143, -0.0031,  0.0143,  ..., -0.0067,  0.0108, -0.0199],\n","                      [-0.0474, -0.0221, -0.0062,  ..., -0.0025, -0.0201, -0.0010]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias',\n","              tensor([0.0327, 0.1309, 0.0266,  ..., 0.0049, 0.1328, 0.1592],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.14.layer_norm1.weight',\n","              tensor([1.0391, 1.0312, 1.1094,  ..., 0.8711, 0.9141, 0.9922],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.14.layer_norm1.bias',\n","              tensor([ 0.1006, -0.0684,  0.0771,  ..., -0.0016, -0.0991, -0.0574],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight',\n","              tensor([[-0.0271, -0.0083,  0.0203,  ..., -0.0025,  0.0017,  0.0442],\n","                      [ 0.0150,  0.0491, -0.0079,  ..., -0.0210,  0.0088,  0.0181],\n","                      [ 0.0159,  0.0245,  0.0038,  ..., -0.0018, -0.0063, -0.0031],\n","                      ...,\n","                      [ 0.0055,  0.0200,  0.0011,  ...,  0.0208, -0.0173,  0.0033],\n","                      [ 0.0026,  0.0032,  0.0049,  ..., -0.0010,  0.0072, -0.0124],\n","                      [ 0.0059, -0.0064, -0.0288,  ..., -0.0311,  0.0101, -0.0006]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias',\n","              tensor([ 0.0146, -0.9375, -0.9375,  ..., -0.4023, -0.8867,  0.1465],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight',\n","              tensor([[ 0.0189, -0.0104,  0.0300,  ...,  0.0110,  0.0029, -0.0447],\n","                      [-0.0078, -0.0026,  0.0199,  ...,  0.0243, -0.0093, -0.0114],\n","                      [-0.0085,  0.0154,  0.0010,  ..., -0.0203, -0.0129, -0.0226],\n","                      ...,\n","                      [ 0.0327, -0.0305,  0.0239,  ...,  0.0408, -0.0141,  0.0101],\n","                      [-0.0237, -0.0178,  0.0078,  ..., -0.0542,  0.0048, -0.0137],\n","                      [-0.0242, -0.0087,  0.0016,  ..., -0.0117,  0.0123,  0.0068]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias',\n","              tensor([-0.0728, -0.3145, -0.1738,  ..., -0.0143, -0.0312,  0.0172],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.14.layer_norm2.weight',\n","              tensor([0.8477, 1.2500, 1.2344,  ..., 1.0312, 1.1641, 1.1797],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.14.layer_norm2.bias',\n","              tensor([ 0.1196, -0.2461,  0.3535,  ...,  0.2217,  0.0057, -0.0991],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight',\n","              tensor([[ 0.0057,  0.0078,  0.0083,  ..., -0.0260, -0.0305, -0.0151],\n","                      [-0.0152, -0.0562,  0.0150,  ...,  0.0206,  0.0055, -0.0073],\n","                      [-0.0205,  0.0270, -0.0219,  ..., -0.0291, -0.0287, -0.0027],\n","                      ...,\n","                      [-0.0317, -0.0142,  0.0273,  ..., -0.0286, -0.0195,  0.0110],\n","                      [-0.0172, -0.0209,  0.0203,  ...,  0.0042,  0.0104,  0.0283],\n","                      [ 0.0312,  0.0105,  0.0269,  ...,  0.0093,  0.0344,  0.0064]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias',\n","              tensor([-1.3125,  0.1836, -0.2246,  ...,  0.0435, -1.5234,  0.3203],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight',\n","              tensor([[-0.0016,  0.0156,  0.0087,  ..., -0.0064, -0.0299,  0.0086],\n","                      [ 0.0043,  0.0225, -0.0197,  ..., -0.0088,  0.0093,  0.0035],\n","                      [ 0.0002, -0.0243,  0.0388,  ..., -0.0248,  0.0042,  0.0040],\n","                      ...,\n","                      [ 0.0052,  0.0119, -0.0177,  ..., -0.0371,  0.0024, -0.0454],\n","                      [-0.0063, -0.0146, -0.0062,  ...,  0.0049, -0.0369, -0.0189],\n","                      [ 0.0159,  0.0058, -0.0171,  ..., -0.0176, -0.0125, -0.0300]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias',\n","              tensor([ 0.0011, -0.0087,  0.0205,  ..., -0.0334, -0.0249,  0.0069],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight',\n","              tensor([[-0.0111,  0.0211, -0.0133,  ...,  0.0167, -0.0084, -0.0226],\n","                      [ 0.0280,  0.0152,  0.0061,  ...,  0.0110,  0.0549, -0.0275],\n","                      [-0.0284,  0.0128, -0.0337,  ..., -0.0225,  0.0186,  0.0052],\n","                      ...,\n","                      [-0.0140, -0.0435, -0.0295,  ...,  0.0117, -0.0258, -0.0034],\n","                      [ 0.0076, -0.0082, -0.0195,  ...,  0.0076,  0.0332,  0.0171],\n","                      [-0.0103, -0.0212, -0.0364,  ...,  0.0017,  0.0255, -0.0159]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias',\n","              tensor([-0.0231,  0.0170,  0.0703,  ...,  0.0669, -0.0099, -0.1025],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight',\n","              tensor([[-0.0079, -0.0122, -0.0255,  ...,  0.0199, -0.0165, -0.0164],\n","                      [ 0.0168, -0.0009,  0.0075,  ..., -0.0018, -0.0364, -0.0442],\n","                      [ 0.0354,  0.0082, -0.0010,  ..., -0.0181, -0.0055,  0.0146],\n","                      ...,\n","                      [-0.0216, -0.0150,  0.0142,  ...,  0.0220,  0.0076,  0.0137],\n","                      [ 0.0229, -0.0266,  0.0100,  ...,  0.0188, -0.0049, -0.0040],\n","                      [-0.0094, -0.0236, -0.0146,  ...,  0.0282,  0.0243, -0.0126]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias',\n","              tensor([-0.0374,  0.1147,  0.1040,  ..., -0.0374,  0.0255,  0.1094],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.15.layer_norm1.weight',\n","              tensor([1.0469, 1.0781, 1.0469,  ..., 0.9141, 1.0234, 1.0469],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.15.layer_norm1.bias',\n","              tensor([ 0.1543,  0.0417,  0.1475,  ..., -0.0192, -0.1328, -0.0835],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight',\n","              tensor([[ 0.0500,  0.0106,  0.0261,  ..., -0.0393,  0.0233, -0.0215],\n","                      [-0.0289,  0.0048,  0.0030,  ...,  0.0217,  0.0199, -0.0063],\n","                      [ 0.0374, -0.0120,  0.0273,  ...,  0.0233,  0.0064, -0.0222],\n","                      ...,\n","                      [-0.0013,  0.0229,  0.0064,  ...,  0.0041, -0.0003, -0.0100],\n","                      [ 0.0024,  0.0049,  0.0154,  ...,  0.0146, -0.0106, -0.0344],\n","                      [-0.0282,  0.0400, -0.0586,  ...,  0.0228,  0.0171,  0.0103]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias',\n","              tensor([-0.2793, -1.2109, -0.3008,  ..., -1.0703, -0.2471, -0.5625],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight',\n","              tensor([[ 0.0330, -0.0052,  0.0226,  ...,  0.0220,  0.0376,  0.0064],\n","                      [-0.0018, -0.0057,  0.0393,  ..., -0.0442, -0.0275,  0.0332],\n","                      [ 0.0121,  0.0132,  0.0216,  ...,  0.0062,  0.0359, -0.0271],\n","                      ...,\n","                      [ 0.0003, -0.0400,  0.0134,  ..., -0.0129,  0.0247, -0.0325],\n","                      [ 0.0283, -0.0403, -0.0076,  ...,  0.0156, -0.0057, -0.0063],\n","                      [-0.0586,  0.0104,  0.0228,  ...,  0.0496, -0.0376,  0.0030]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias',\n","              tensor([ 0.1040, -0.1328,  0.0011,  ..., -0.0933, -0.1709, -0.1045],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.15.layer_norm2.weight',\n","              tensor([0.9688, 1.4531, 1.5000,  ..., 1.1953, 1.5234, 1.8125],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.15.layer_norm2.bias',\n","              tensor([ 0.1572, -0.3691, -0.3320,  ...,  0.3809,  0.3887,  0.8125],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight',\n","              tensor([[ 0.0388,  0.0160,  0.0205,  ..., -0.0014,  0.0183, -0.0210],\n","                      [ 0.0222, -0.0115,  0.0322,  ..., -0.0143,  0.0265,  0.0211],\n","                      [-0.0173, -0.0059,  0.0085,  ...,  0.0259, -0.0115, -0.0159],\n","                      ...,\n","                      [-0.0186, -0.0167, -0.0142,  ...,  0.0085, -0.0081, -0.0262],\n","                      [-0.0170,  0.0090,  0.0378,  ..., -0.0220, -0.0087,  0.0010],\n","                      [-0.0193, -0.0134, -0.0104,  ..., -0.0175,  0.0312,  0.0286]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias',\n","              tensor([ 1.4297, -0.5078, -1.1172,  ..., -0.5234, -0.2734,  0.4922],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight',\n","              tensor([[ 1.8311e-02,  2.2461e-02,  1.4282e-02,  ..., -2.2949e-02,\n","                        2.2461e-02,  8.9722e-03],\n","                      [-1.3000e-02,  3.0640e-02,  2.0752e-02,  ...,  1.5442e-02,\n","                        2.2461e-02,  1.8616e-03],\n","                      [-1.6968e-02,  1.4465e-02, -2.4048e-02,  ..., -3.0160e-05,\n","                        3.3722e-03,  1.8188e-02],\n","                      ...,\n","                      [ 3.1891e-03,  3.4332e-04,  1.3367e-02,  ...,  1.6235e-02,\n","                        1.8188e-02,  1.5991e-02],\n","                      [-8.1787e-03, -3.8330e-02, -7.9727e-04,  ..., -4.1748e-02,\n","                        2.3804e-02,  6.4697e-03],\n","                      [-5.3711e-03,  3.5400e-02,  2.9663e-02,  ..., -1.0620e-02,\n","                        3.2471e-02,  9.5215e-03]], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias',\n","              tensor([ 0.0115,  0.0388, -0.0266,  ...,  0.0674, -0.0140, -0.0427],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight',\n","              tensor([[-0.0047, -0.0041, -0.0234,  ...,  0.0027,  0.0110,  0.0099],\n","                      [ 0.0199,  0.0201,  0.0135,  ..., -0.0229,  0.0002, -0.0134],\n","                      [ 0.0046,  0.0145,  0.0366,  ..., -0.0140,  0.0024,  0.0204],\n","                      ...,\n","                      [ 0.0209, -0.0432,  0.0063,  ...,  0.0223, -0.0260,  0.0151],\n","                      [ 0.0013, -0.0378,  0.0081,  ..., -0.0006, -0.0334, -0.0138],\n","                      [-0.0227, -0.0107, -0.0141,  ...,  0.0255,  0.0009, -0.0413]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias',\n","              tensor([-0.0374, -0.1357,  0.0344,  ..., -0.1006, -0.1533, -0.0923],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight',\n","              tensor([[-0.0130, -0.0072,  0.0442,  ...,  0.0276, -0.0019, -0.0182],\n","                      [-0.0248,  0.0092, -0.0256,  ...,  0.0229, -0.0020, -0.0175],\n","                      [-0.0029,  0.0094,  0.0254,  ...,  0.0171,  0.0021, -0.0145],\n","                      ...,\n","                      [ 0.0115, -0.0104,  0.0188,  ..., -0.0188,  0.0209, -0.0034],\n","                      [-0.0391, -0.0198, -0.0005,  ...,  0.0093,  0.0047,  0.0204],\n","                      [ 0.0198,  0.0017,  0.0130,  ..., -0.0281,  0.0098,  0.0292]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias',\n","              tensor([ 0.0160, -0.1592,  0.1084,  ...,  0.1084, -0.0101,  0.0752],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.16.layer_norm1.weight',\n","              tensor([1.1484, 1.1016, 1.1250,  ..., 0.9570, 1.0859, 1.0859],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.16.layer_norm1.bias',\n","              tensor([ 0.1133, -0.0532,  0.0215,  ...,  0.0100,  0.0020, -0.1055],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight',\n","              tensor([[ 2.9907e-02,  1.0315e-02, -2.0020e-02,  ...,  2.4536e-02,\n","                       -3.6621e-02,  1.5335e-03],\n","                      [-1.4221e-02,  2.3071e-02, -4.8096e-02,  ...,  1.3504e-03,\n","                       -7.7209e-03, -1.6113e-02],\n","                      [-4.8523e-03,  2.4292e-02, -3.3203e-02,  ...,  1.0193e-02,\n","                        1.3245e-02, -2.7466e-02],\n","                      ...,\n","                      [-9.1309e-02,  8.2397e-03, -6.1035e-03,  ..., -8.8501e-03,\n","                        8.9722e-03,  3.2227e-02],\n","                      [-1.2817e-02, -7.5817e-05,  1.2817e-02,  ...,  1.6724e-02,\n","                        1.6968e-02,  2.0508e-02],\n","                      [ 1.8677e-02,  1.1780e-02, -4.7607e-03,  ...,  3.5286e-04,\n","                       -3.2715e-02,  1.4587e-02]], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias',\n","              tensor([-0.1582, -0.4512,  0.1895,  ..., -0.2354,  0.1367,  0.3984],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight',\n","              tensor([[-0.0168, -0.0141, -0.0325,  ..., -0.0325, -0.0198,  0.0053],\n","                      [ 0.0315,  0.0342,  0.0315,  ...,  0.0038, -0.0186, -0.0034],\n","                      [ 0.0130, -0.0322, -0.0220,  ...,  0.0234,  0.0038, -0.0090],\n","                      ...,\n","                      [-0.0216,  0.0025, -0.0073,  ..., -0.0036,  0.0403,  0.0188],\n","                      [-0.0203, -0.0297, -0.0161,  ..., -0.0237,  0.0055, -0.0003],\n","                      [-0.0037,  0.0013, -0.0063,  ..., -0.0251,  0.0236, -0.0087]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias',\n","              tensor([-0.0786,  0.0098,  0.1104,  ..., -0.2002,  0.1445, -0.1260],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.16.layer_norm2.weight',\n","              tensor([1.2422, 2.4844, 1.8125,  ..., 1.8672, 1.5625, 1.7500],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.16.layer_norm2.bias',\n","              tensor([ 0.0383, -1.6953, -0.7500,  ...,  1.4219,  0.2852,  0.1553],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight',\n","              tensor([[-0.0079, -0.0089, -0.0178,  ..., -0.0226,  0.0112,  0.0034],\n","                      [ 0.0187, -0.0118,  0.0022,  ..., -0.0018, -0.0143, -0.0452],\n","                      [ 0.0027, -0.0062, -0.0076,  ..., -0.0052,  0.0166,  0.0398],\n","                      ...,\n","                      [-0.0044, -0.0078, -0.0286,  ..., -0.0212,  0.0122, -0.0288],\n","                      [ 0.0102,  0.0036, -0.0315,  ..., -0.0033,  0.0002, -0.0113],\n","                      [ 0.0086, -0.0154, -0.0253,  ..., -0.0427, -0.0075, -0.0337]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias',\n","              tensor([ 0.3750, -0.2871, -0.7578,  ...,  0.4688, -0.0996, -0.7656],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight',\n","              tensor([[ 0.0334,  0.0103,  0.0015,  ..., -0.0008, -0.0045, -0.0231],\n","                      [ 0.0383, -0.0251, -0.0280,  ..., -0.0201,  0.0092,  0.0229],\n","                      [ 0.0150,  0.0059,  0.0349,  ..., -0.0056, -0.0033,  0.0306],\n","                      ...,\n","                      [-0.0110, -0.0102,  0.0043,  ..., -0.0083, -0.0286,  0.0270],\n","                      [-0.0105,  0.0016,  0.0142,  ...,  0.0117,  0.0142, -0.0244],\n","                      [ 0.0229, -0.0292,  0.0270,  ..., -0.0038,  0.0123,  0.0110]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias',\n","              tensor([ 0.0309, -0.0231,  0.0283,  ..., -0.0781,  0.0356, -0.0025],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight',\n","              tensor([[ 0.0022,  0.0083,  0.0153,  ..., -0.0354, -0.0017,  0.0170],\n","                      [-0.0276, -0.0240, -0.0420,  ..., -0.0267, -0.0280, -0.0054],\n","                      [-0.0327,  0.0023,  0.0016,  ...,  0.0088,  0.0164,  0.0317],\n","                      ...,\n","                      [-0.0025, -0.0092,  0.0002,  ...,  0.0023, -0.0223, -0.0237],\n","                      [ 0.0688,  0.0236, -0.0104,  ..., -0.0112,  0.0381,  0.0304],\n","                      [ 0.0217,  0.0054, -0.0537,  ..., -0.0141,  0.0078,  0.0146]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias',\n","              tensor([-0.1436, -0.0151, -0.0815,  ..., -0.1660,  0.0045, -0.1133],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight',\n","              tensor([[-0.0145,  0.0058, -0.0127,  ..., -0.0006, -0.0137, -0.0166],\n","                      [-0.0114,  0.0040, -0.0101,  ..., -0.0474, -0.0265,  0.0020],\n","                      [ 0.0037, -0.0327, -0.0220,  ..., -0.0376,  0.0106, -0.0192],\n","                      ...,\n","                      [-0.0050,  0.0025,  0.0049,  ...,  0.0024,  0.0059, -0.0079],\n","                      [ 0.0352,  0.0157,  0.0118,  ...,  0.0493, -0.0167, -0.0046],\n","                      [ 0.0098, -0.0184, -0.0128,  ..., -0.0040, -0.0005, -0.0247]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias',\n","              tensor([-0.1074, -0.3145, -0.0118,  ...,  0.3223,  0.1582, -0.0525],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.17.layer_norm1.weight',\n","              tensor([1.1953, 1.1641, 1.1406,  ..., 1.1016, 1.0703, 1.2344],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.17.layer_norm1.bias',\n","              tensor([ 0.1582,  0.0408, -0.0835,  ...,  0.0859, -0.0564, -0.1177],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight',\n","              tensor([[-0.0014, -0.0017,  0.0420,  ..., -0.0004,  0.0168,  0.0154],\n","                      [ 0.0047,  0.0052, -0.0081,  ..., -0.0057,  0.0004,  0.0172],\n","                      [ 0.0115,  0.0217, -0.0100,  ..., -0.0139, -0.0088, -0.0261],\n","                      ...,\n","                      [ 0.0006,  0.0128, -0.0091,  ..., -0.0050, -0.0238, -0.0378],\n","                      [ 0.0117, -0.0066, -0.0089,  ..., -0.0172, -0.0332, -0.0474],\n","                      [ 0.0123,  0.0044, -0.0452,  ..., -0.0173,  0.0025, -0.0476]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias',\n","              tensor([-0.5312, -1.0234, -0.8047,  ...,  0.1484, -0.6094, -0.4648],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight',\n","              tensor([[ 0.0166,  0.0134,  0.0175,  ...,  0.0203, -0.0019,  0.0167],\n","                      [-0.0571,  0.0112, -0.0167,  ..., -0.0137, -0.0227,  0.0112],\n","                      [ 0.0123, -0.0184,  0.0074,  ..., -0.0261, -0.0047,  0.0046],\n","                      ...,\n","                      [ 0.0022, -0.0029,  0.0036,  ...,  0.0131,  0.0009, -0.0430],\n","                      [ 0.0308,  0.0107,  0.0089,  ..., -0.0109,  0.0253, -0.0030],\n","                      [-0.0072, -0.0036, -0.0046,  ..., -0.0113,  0.0096,  0.0121]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias',\n","              tensor([ 0.3105,  0.4766, -0.1396,  ..., -0.2852, -0.1113,  0.1445],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.17.layer_norm2.weight',\n","              tensor([2.8438, 4.4375, 1.9688,  ..., 2.9375, 2.2344, 1.9844],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.17.layer_norm2.bias',\n","              tensor([-2.6094, -4.3125, -0.1592,  ...,  2.7344,  0.5820, -0.5664],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight',\n","              tensor([[-0.0209,  0.0215,  0.0134,  ...,  0.0053,  0.0090, -0.0036],\n","                      [-0.0193, -0.0214, -0.0075,  ...,  0.0356,  0.0153,  0.0101],\n","                      [ 0.0091,  0.0179,  0.0080,  ...,  0.0043,  0.0195,  0.0112],\n","                      ...,\n","                      [-0.0236, -0.0190, -0.0082,  ...,  0.0308,  0.0062,  0.0251],\n","                      [-0.0153, -0.0250,  0.0156,  ...,  0.0110,  0.0025, -0.0055],\n","                      [-0.0043, -0.0200,  0.0157,  ..., -0.0053,  0.0093,  0.0031]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias',\n","              tensor([ 0.8672, -0.4961, -0.6328,  ...,  1.1406,  0.6172, -0.2412],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight',\n","              tensor([[-0.0080, -0.0031,  0.0240,  ..., -0.0038, -0.0125, -0.0159],\n","                      [ 0.0452, -0.0229,  0.0157,  ..., -0.0123, -0.0017, -0.0206],\n","                      [ 0.0128,  0.0123, -0.0176,  ...,  0.0177,  0.0256, -0.0069],\n","                      ...,\n","                      [ 0.0127, -0.0322, -0.0289,  ...,  0.0300,  0.0008, -0.0051],\n","                      [ 0.0161,  0.0304,  0.0096,  ..., -0.0286,  0.0165,  0.0334],\n","                      [-0.0038, -0.0170,  0.0199,  ..., -0.0396,  0.0242, -0.0085]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias',\n","              tensor([ 0.0278, -0.0422, -0.0197,  ..., -0.0334, -0.0417, -0.0162],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight',\n","              tensor([[ 0.0146, -0.0060,  0.0129,  ..., -0.0020, -0.0182,  0.0171],\n","                      [-0.0044,  0.0013,  0.0057,  ...,  0.0114,  0.0033, -0.0153],\n","                      [-0.0079, -0.0017,  0.0315,  ..., -0.0126, -0.0142,  0.0325],\n","                      ...,\n","                      [ 0.0107,  0.0100,  0.0054,  ...,  0.0271, -0.0192,  0.0121],\n","                      [-0.0190, -0.0075,  0.0119,  ...,  0.0021, -0.0537,  0.0086],\n","                      [ 0.0029, -0.0090, -0.0226,  ...,  0.0156,  0.0075,  0.0302]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias',\n","              tensor([-0.0471, -0.1357,  0.2451,  ..., -0.1592, -0.1465,  1.3047],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight',\n","              tensor([[ 0.0142, -0.0177, -0.0079,  ...,  0.0072,  0.0254, -0.0049],\n","                      [-0.0084,  0.0417, -0.0020,  ...,  0.0298, -0.0172,  0.0015],\n","                      [ 0.0081,  0.0283, -0.0032,  ...,  0.0010, -0.0009,  0.0024],\n","                      ...,\n","                      [ 0.0098,  0.0018, -0.0267,  ..., -0.0168, -0.0011, -0.0203],\n","                      [ 0.0078,  0.0100, -0.0079,  ...,  0.0144,  0.0019, -0.0142],\n","                      [ 0.0231, -0.0149, -0.0201,  ..., -0.0069,  0.0051,  0.0420]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias',\n","              tensor([ 0.0171, -0.3145,  0.1235,  ...,  0.2393,  0.0430, -0.0476],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.18.layer_norm1.weight',\n","              tensor([1.1719, 1.1719, 1.1406,  ..., 1.1562, 1.1250, 1.1797],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.18.layer_norm1.bias',\n","              tensor([ 0.1309,  0.0515, -0.0430,  ..., -0.0184, -0.1260, -0.1279],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight',\n","              tensor([[-0.0325,  0.0161, -0.0112,  ..., -0.0064,  0.0315,  0.0410],\n","                      [-0.0209, -0.0116,  0.0007,  ..., -0.0042, -0.0106, -0.0059],\n","                      [ 0.0006, -0.0063, -0.0015,  ..., -0.0266, -0.0003, -0.0294],\n","                      ...,\n","                      [ 0.0164, -0.0143, -0.0044,  ...,  0.0057,  0.0204,  0.0039],\n","                      [-0.0173,  0.0212, -0.0077,  ..., -0.0096, -0.0150, -0.0136],\n","                      [ 0.0203,  0.0215,  0.0153,  ..., -0.0107,  0.0048,  0.0082]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias',\n","              tensor([-0.1719,  0.0654, -0.7070,  ..., -0.7500, -0.5234, -0.4258],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight',\n","              tensor([[-0.0225, -0.0193,  0.0027,  ...,  0.0204,  0.0181, -0.0053],\n","                      [-0.0432, -0.0208, -0.0138,  ...,  0.0420,  0.0344,  0.0305],\n","                      [ 0.0069,  0.0234,  0.0080,  ...,  0.0215, -0.0070, -0.0209],\n","                      ...,\n","                      [-0.0145, -0.0001,  0.0127,  ...,  0.0248, -0.0084, -0.0030],\n","                      [-0.0327,  0.0070,  0.0099,  ...,  0.0198,  0.0137,  0.0170],\n","                      [ 0.0106, -0.0236,  0.0347,  ..., -0.0226,  0.0080,  0.0184]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias',\n","              tensor([ 0.1846,  0.1138, -0.4902,  ..., -0.2275, -0.0254,  0.0703],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.18.layer_norm2.weight',\n","              tensor([2.6406, 4.0312, 3.0781,  ..., 3.4219, 2.4688, 2.2344],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.18.layer_norm2.bias',\n","              tensor([-1.6250, -2.7500,  1.4844,  ...,  2.7188,  0.8047, -0.5391],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight',\n","              tensor([[-0.0049, -0.0037,  0.0060,  ...,  0.0366, -0.0090, -0.0195],\n","                      [-0.0038,  0.0103, -0.0081,  ..., -0.0481, -0.0103,  0.0287],\n","                      [-0.0238,  0.0151, -0.0053,  ..., -0.0129, -0.0447, -0.0371],\n","                      ...,\n","                      [ 0.0132, -0.0569, -0.0093,  ..., -0.0217,  0.0303, -0.0085],\n","                      [-0.0281,  0.0193,  0.0513,  ...,  0.0209, -0.0288, -0.0182],\n","                      [ 0.0099, -0.0221, -0.0220,  ...,  0.0288, -0.0032, -0.0159]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias',\n","              tensor([ 0.4199,  0.2949, -0.3301,  ..., -0.1592, -0.3320, -1.2656],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight',\n","              tensor([[-2.2583e-03, -2.8076e-02,  2.4170e-02,  ...,  1.5869e-02,\n","                        5.2490e-03,  3.8757e-03],\n","                      [ 2.7832e-02, -1.6602e-02, -1.6357e-02,  ...,  2.4780e-02,\n","                       -2.7222e-02, -2.0752e-03],\n","                      [ 1.5991e-02,  9.8877e-03,  1.0315e-02,  ...,  8.8120e-04,\n","                       -1.0490e-04, -3.4424e-02],\n","                      ...,\n","                      [ 1.4282e-02,  1.9165e-02,  2.2949e-02,  ..., -5.2795e-03,\n","                       -8.2397e-03,  2.0020e-02],\n","                      [ 6.4087e-03,  3.9307e-02,  8.1787e-03,  ..., -3.0518e-02,\n","                        2.7954e-02,  2.5269e-02],\n","                      [ 1.2207e-02, -2.0264e-02, -3.7354e-02,  ..., -2.6822e-05,\n","                        1.5747e-02,  7.4463e-03]], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias',\n","              tensor([-0.0713, -0.0864, -0.0459,  ..., -0.0825,  0.0505, -0.0044],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight',\n","              tensor([[-0.0251,  0.0146,  0.0082,  ...,  0.0396, -0.0177, -0.0276],\n","                      [-0.0159, -0.0234,  0.0004,  ..., -0.0457, -0.0082, -0.0137],\n","                      [-0.0088,  0.0069, -0.0172,  ...,  0.0013, -0.0317, -0.0393],\n","                      ...,\n","                      [ 0.0229, -0.0238, -0.0014,  ..., -0.0264, -0.0398,  0.0039],\n","                      [-0.0219, -0.0048,  0.0170,  ...,  0.0051, -0.0160, -0.0262],\n","                      [ 0.0123,  0.0008, -0.0087,  ...,  0.0420,  0.0554, -0.0107]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias',\n","              tensor([ 0.0403,  0.0630, -0.1260,  ...,  0.0598, -0.0298, -0.1357],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight',\n","              tensor([[-0.0212,  0.0308, -0.0225,  ...,  0.0016,  0.0170,  0.0210],\n","                      [ 0.0229,  0.0146,  0.0135,  ..., -0.0022, -0.0066,  0.0045],\n","                      [ 0.0028,  0.0106, -0.0031,  ...,  0.0160,  0.0150,  0.0205],\n","                      ...,\n","                      [-0.0132, -0.0181, -0.0093,  ..., -0.0160, -0.0024, -0.0205],\n","                      [-0.0051,  0.0102, -0.0233,  ...,  0.0024, -0.0178, -0.0015],\n","                      [ 0.0062,  0.0120,  0.0413,  ..., -0.0078, -0.0236, -0.0025]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias',\n","              tensor([-0.1689, -0.3398, -0.4199,  ...,  0.3945,  0.0913,  0.0791],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.19.layer_norm1.weight',\n","              tensor([1.3359, 1.2500, 1.3125,  ..., 1.2578, 1.2422, 1.3203],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.19.layer_norm1.bias',\n","              tensor([ 0.1318,  0.2539,  0.1357,  ..., -0.0596, -0.1553, -0.0894],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight',\n","              tensor([[ 0.0237,  0.0117,  0.0079,  ..., -0.0030,  0.0544,  0.0491],\n","                      [-0.0231,  0.0227,  0.0105,  ...,  0.0112, -0.0273,  0.0016],\n","                      [-0.0087,  0.0055,  0.0410,  ...,  0.0058,  0.0013, -0.0152],\n","                      ...,\n","                      [ 0.0378,  0.0054, -0.0002,  ..., -0.0004,  0.0032, -0.0322],\n","                      [-0.0037,  0.0217,  0.0119,  ..., -0.0027, -0.0154,  0.0096],\n","                      [ 0.0518, -0.0063,  0.0228,  ...,  0.0320, -0.0320,  0.0204]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias',\n","              tensor([-0.5391, -0.1504,  0.1885,  ..., -0.4316, -0.7812, -0.3770],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight',\n","              tensor([[-9.7656e-03,  3.3203e-02,  1.0193e-02,  ...,  6.9580e-03,\n","                       -4.6875e-02,  2.9541e-02],\n","                      [-3.7109e-02, -6.7383e-02, -1.0193e-02,  ..., -3.6865e-02,\n","                       -3.7842e-03, -8.3618e-03],\n","                      [-6.1279e-02, -4.5166e-02,  3.1494e-02,  ..., -2.1851e-02,\n","                        1.0864e-02,  2.8442e-02],\n","                      ...,\n","                      [-3.9978e-03,  2.7466e-02,  5.2490e-02,  ...,  4.5898e-02,\n","                       -1.5442e-02, -2.7466e-02],\n","                      [-1.2451e-02,  5.3101e-03, -3.3691e-02,  ...,  2.0264e-02,\n","                        6.0120e-03,  5.2002e-02],\n","                      [ 1.3184e-02, -4.2969e-02, -1.1169e-02,  ...,  7.2632e-03,\n","                       -2.1851e-02,  7.9632e-05]], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias',\n","              tensor([ 0.0171,  0.3789,  0.1406,  ..., -0.3301, -0.0253, -0.1738],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.19.layer_norm2.weight',\n","              tensor([3.1250, 5.5000, 4.0000,  ..., 4.6250, 3.2656, 3.0938],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.19.layer_norm2.bias',\n","              tensor([-0.8633, -3.2500, -1.0938,  ...,  3.2969,  0.7188,  0.0520],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight',\n","              tensor([[ 0.0203,  0.0155,  0.0029,  ..., -0.0139, -0.0162,  0.0071],\n","                      [-0.0237,  0.0003,  0.0139,  ..., -0.0228, -0.0243, -0.0136],\n","                      [ 0.0104, -0.0243,  0.0047,  ...,  0.0010,  0.0110, -0.0137],\n","                      ...,\n","                      [-0.0152,  0.0156,  0.0175,  ...,  0.0183, -0.0444, -0.0013],\n","                      [ 0.0153, -0.0019, -0.0369,  ..., -0.0151,  0.0074,  0.0121],\n","                      [ 0.0151,  0.0361, -0.0030,  ..., -0.0081, -0.0310, -0.0422]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias',\n","              tensor([-0.3027, -0.7148, -0.2480,  ...,  1.4766, -0.4004, -0.0469],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight',\n","              tensor([[ 0.0059,  0.0027,  0.0063,  ...,  0.0126, -0.0127,  0.0073],\n","                      [-0.0562,  0.0044,  0.0190,  ..., -0.0410, -0.0239, -0.0038],\n","                      [-0.0126,  0.0432, -0.0330,  ...,  0.0281, -0.0227, -0.0170],\n","                      ...,\n","                      [ 0.0239, -0.0013, -0.0197,  ..., -0.0459, -0.0085,  0.0053],\n","                      [-0.0469,  0.0041, -0.0123,  ...,  0.0150, -0.0371, -0.0098],\n","                      [ 0.0194,  0.0090,  0.0334,  ..., -0.0116, -0.0300, -0.0327]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias',\n","              tensor([-0.2578, -0.0032,  0.0728,  ..., -0.0889,  0.0051,  0.0413],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight',\n","              tensor([[-1.6235e-02,  1.0132e-02, -2.0142e-02,  ..., -2.5177e-03,\n","                        2.1338e-05, -7.7820e-03],\n","                      [-6.5002e-03, -6.0120e-03,  1.3855e-02,  ..., -1.0864e-02,\n","                       -5.8594e-03,  1.0498e-02],\n","                      [ 7.7515e-03, -3.1982e-02, -4.3945e-03,  ...,  2.7771e-03,\n","                       -4.1748e-02, -6.7444e-03],\n","                      ...,\n","                      [ 1.3367e-02,  3.0273e-02,  1.9775e-02,  ...,  1.5259e-03,\n","                       -2.3193e-02, -2.4658e-02],\n","                      [-4.4556e-03, -2.0599e-03,  1.9409e-02,  ...,  5.9326e-02,\n","                       -2.1851e-02,  3.5095e-03],\n","                      [ 7.4463e-03,  1.9043e-02, -3.4943e-03,  ..., -1.4648e-02,\n","                        2.0630e-02, -8.0566e-03]], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias',\n","              tensor([ 0.3047,  0.0508, -0.1406,  ..., -0.1040, -0.0054,  0.1206],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight',\n","              tensor([[ 0.0115,  0.0265, -0.0233,  ..., -0.0245,  0.0334,  0.0254],\n","                      [ 0.0053,  0.0248, -0.0247,  ...,  0.0266,  0.0096, -0.0247],\n","                      [ 0.0054,  0.0251,  0.0063,  ..., -0.0254, -0.0031,  0.0063],\n","                      ...,\n","                      [-0.0259,  0.0298,  0.0157,  ...,  0.0151,  0.0079, -0.0134],\n","                      [ 0.0181,  0.0183,  0.0115,  ...,  0.0135,  0.0195,  0.0530],\n","                      [-0.0237,  0.0188,  0.0332,  ...,  0.0281, -0.0024,  0.0022]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias',\n","              tensor([-0.3164, -0.0986, -0.0610,  ...,  0.1787, -0.0137, -0.0476],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.20.layer_norm1.weight',\n","              tensor([1.3984, 1.3047, 1.3750,  ..., 1.3672, 1.2344, 1.3750],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.20.layer_norm1.bias',\n","              tensor([ 0.3066,  0.3516,  0.3301,  ..., -0.1279, -0.1758,  0.0598],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight',\n","              tensor([[-0.0132, -0.0112, -0.0136,  ...,  0.0325, -0.0344,  0.0164],\n","                      [ 0.0067, -0.0171, -0.0237,  ...,  0.0405,  0.0104, -0.0072],\n","                      [ 0.0125, -0.0311,  0.0228,  ..., -0.0325, -0.0120, -0.0008],\n","                      ...,\n","                      [-0.0131, -0.0114, -0.0474,  ..., -0.0330,  0.0415,  0.0016],\n","                      [ 0.0079,  0.0019,  0.0295,  ..., -0.0183, -0.0315, -0.0388],\n","                      [-0.0128, -0.0029,  0.0165,  ..., -0.0244,  0.0093,  0.0698]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias',\n","              tensor([ 4.3359e-01, -6.0156e-01, -3.6240e-04,  ..., -3.6914e-01,\n","                      -3.2031e-01, -5.4297e-01], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight',\n","              tensor([[-0.0554, -0.0284, -0.0325,  ..., -0.0140, -0.0151,  0.0048],\n","                      [-0.0295,  0.0391,  0.0151,  ...,  0.0073, -0.0270,  0.0085],\n","                      [ 0.0432,  0.0121,  0.0074,  ..., -0.0383,  0.0148, -0.0153],\n","                      ...,\n","                      [ 0.0148,  0.0277, -0.0121,  ...,  0.0391,  0.0220, -0.0051],\n","                      [-0.0255,  0.0027, -0.0106,  ..., -0.0233,  0.0457, -0.0108],\n","                      [ 0.0020,  0.0092,  0.0031,  ..., -0.0530,  0.0081, -0.0041]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias',\n","              tensor([ 0.2832,  0.3926,  0.0542,  ..., -0.1191,  0.0850,  0.1885],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.20.layer_norm2.weight',\n","              tensor([4.8750, 5.8750, 3.6562,  ..., 4.5938, 3.4531, 4.2188],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.20.layer_norm2.bias',\n","              tensor([-2.6250, -2.4844,  1.1719,  ...,  1.6016,  0.0422, -1.5703],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight',\n","              tensor([[ 0.0154, -0.0173, -0.0022,  ...,  0.0265,  0.0017, -0.0520],\n","                      [ 0.0043,  0.0039,  0.0142,  ...,  0.0101, -0.0215,  0.0427],\n","                      [ 0.0157, -0.0159, -0.0154,  ..., -0.0063, -0.0413,  0.0139],\n","                      ...,\n","                      [ 0.0232, -0.0157, -0.0011,  ..., -0.0215,  0.0247,  0.0039],\n","                      [-0.0139, -0.0067, -0.0091,  ..., -0.0356,  0.0005, -0.0160],\n","                      [ 0.0182,  0.0026, -0.0134,  ...,  0.0032,  0.0175,  0.0312]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias',\n","              tensor([ 0.3223,  0.3477, -0.6992,  ..., -0.8633,  0.1074,  1.4922],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight',\n","              tensor([[ 0.0264, -0.0430,  0.0009,  ..., -0.0234,  0.0255,  0.0124],\n","                      [-0.0045,  0.0237, -0.0043,  ...,  0.0215, -0.0082, -0.0488],\n","                      [-0.0211,  0.0120, -0.0723,  ..., -0.0063,  0.0052,  0.0113],\n","                      ...,\n","                      [ 0.0115,  0.0117, -0.0166,  ...,  0.0123, -0.0479,  0.0004],\n","                      [-0.0503, -0.0049, -0.0144,  ..., -0.0188,  0.0210,  0.0137],\n","                      [-0.0247,  0.0137,  0.0040,  ...,  0.0325,  0.0125, -0.0077]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias',\n","              tensor([-0.0398,  0.0413,  0.0062,  ..., -0.0996,  0.0645, -0.0559],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight',\n","              tensor([[ 0.0129, -0.0239, -0.0011,  ..., -0.0276, -0.0103, -0.0427],\n","                      [-0.0103, -0.0009,  0.0276,  ...,  0.0116, -0.0310,  0.0219],\n","                      [ 0.0131, -0.0232, -0.0439,  ...,  0.0029, -0.0132,  0.0125],\n","                      ...,\n","                      [ 0.0139, -0.0342,  0.0181,  ..., -0.0050,  0.0039,  0.0130],\n","                      [-0.0312,  0.0289,  0.0064,  ..., -0.0155,  0.0228,  0.0070],\n","                      [-0.0084, -0.0261,  0.0093,  ..., -0.0464,  0.0093,  0.0157]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias',\n","              tensor([-0.0669, -0.0244, -0.0317,  ...,  0.0491, -0.0522,  0.0889],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight',\n","              tensor([[-0.0378,  0.0299,  0.0166,  ..., -0.0095, -0.0264,  0.0229],\n","                      [-0.0107, -0.0160, -0.0247,  ...,  0.0078,  0.0293,  0.0322],\n","                      [-0.0199,  0.0320,  0.0459,  ...,  0.0249, -0.0026,  0.0142],\n","                      ...,\n","                      [ 0.0342, -0.0064, -0.0042,  ...,  0.0354, -0.0276,  0.0120],\n","                      [-0.0198,  0.0240, -0.0410,  ...,  0.0530,  0.0178, -0.0097],\n","                      [-0.0104,  0.0413, -0.0535,  ..., -0.0192, -0.0025, -0.0322]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias',\n","              tensor([ 0.0101,  0.1201, -0.0747,  ...,  0.2080, -0.0106, -0.0076],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.21.layer_norm1.weight',\n","              tensor([1.3594, 1.1719, 1.4375,  ..., 1.3906, 1.3281, 1.4219],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.21.layer_norm1.bias',\n","              tensor([ 0.3867,  0.3340,  0.3457,  ..., -0.1069, -0.1914,  0.1245],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight',\n","              tensor([[-0.0168, -0.0028,  0.0221,  ..., -0.0183,  0.0248,  0.0337],\n","                      [-0.0014,  0.0142,  0.0045,  ...,  0.0027, -0.0320,  0.0415],\n","                      [ 0.0320,  0.0200, -0.0237,  ...,  0.0315,  0.0176, -0.0097],\n","                      ...,\n","                      [ 0.0239,  0.0120,  0.0192,  ..., -0.0127, -0.0605,  0.0105],\n","                      [-0.0033,  0.0120,  0.0260,  ..., -0.0260, -0.0898, -0.0231],\n","                      [-0.0004, -0.0135, -0.0317,  ...,  0.0114, -0.0144, -0.0033]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias',\n","              tensor([-0.4512, -0.5156,  0.1592,  ..., -0.2383,  0.5977, -0.4980],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight',\n","              tensor([[ 0.0210,  0.0046,  0.0060,  ..., -0.0178,  0.0082,  0.0121],\n","                      [ 0.0092,  0.0201,  0.0046,  ...,  0.0146, -0.0089,  0.0381],\n","                      [ 0.0026, -0.0167, -0.0217,  ...,  0.0071,  0.0004, -0.0410],\n","                      ...,\n","                      [ 0.0189,  0.0081, -0.0175,  ..., -0.0035, -0.0125,  0.0132],\n","                      [ 0.0123, -0.0112, -0.0015,  ...,  0.0104, -0.0549,  0.0176],\n","                      [ 0.0503, -0.0007,  0.0013,  ...,  0.0410, -0.0080,  0.0233]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias',\n","              tensor([ 0.5312,  0.7188,  0.1533,  ..., -0.2266,  0.5820,  0.5273],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.21.layer_norm2.weight',\n","              tensor([5.9688, 5.8750, 5.2188,  ..., 5.4688, 4.7500, 6.3750],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.21.layer_norm2.bias',\n","              tensor([-1.5547, -0.7422,  1.6172,  ...,  1.1094,  0.2520, -3.0469],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight',\n","              tensor([[-0.0092,  0.0094, -0.0330,  ..., -0.0031, -0.0105,  0.0283],\n","                      [ 0.0309,  0.0148, -0.0178,  ...,  0.0070, -0.0005, -0.0354],\n","                      [-0.0142,  0.0219, -0.0130,  ..., -0.0026,  0.0181, -0.0311],\n","                      ...,\n","                      [ 0.0148,  0.0261,  0.0083,  ...,  0.0344, -0.0097, -0.0288],\n","                      [ 0.0264,  0.0023,  0.0123,  ...,  0.0161, -0.0280, -0.0139],\n","                      [ 0.0205, -0.0427, -0.0089,  ...,  0.0315,  0.0217, -0.0234]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias',\n","              tensor([ 0.5742, -0.5742, -0.4883,  ..., -0.1045, -1.0938, -0.1094],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight',\n","              tensor([[-0.0011,  0.0244,  0.0028,  ..., -0.0074,  0.0297,  0.0361],\n","                      [-0.0283,  0.0167, -0.0269,  ..., -0.0166,  0.0405,  0.0073],\n","                      [ 0.0025, -0.0183, -0.0024,  ..., -0.0011,  0.0019,  0.0172],\n","                      ...,\n","                      [-0.0184, -0.0075,  0.0430,  ..., -0.0125,  0.0074,  0.0087],\n","                      [-0.0356,  0.0132, -0.0173,  ..., -0.0021, -0.0374,  0.0128],\n","                      [ 0.0009, -0.0427, -0.0141,  ..., -0.0199, -0.0032, -0.0197]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias',\n","              tensor([ 0.1475, -0.6055, -0.1953,  ..., -0.0142,  0.0197,  0.0493],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight',\n","              tensor([[-0.0057,  0.0038, -0.0157,  ...,  0.0120,  0.0183,  0.0229],\n","                      [ 0.0153,  0.0121, -0.0123,  ...,  0.0049, -0.0199, -0.0422],\n","                      [-0.0177,  0.0320, -0.0107,  ..., -0.0002,  0.0234, -0.0044],\n","                      ...,\n","                      [-0.0066, -0.0151,  0.0114,  ..., -0.0195,  0.0251,  0.0106],\n","                      [-0.0130,  0.0166,  0.0095,  ...,  0.0137,  0.0140,  0.0059],\n","                      [ 0.0159,  0.0110,  0.0104,  ...,  0.0092,  0.0027,  0.0027]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias',\n","              tensor([ 0.0879, -0.0923, -0.0645,  ..., -0.1123,  0.1885, -0.0222],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight',\n","              tensor([[-3.3203e-02,  2.8076e-02, -9.0942e-03,  ...,  8.3923e-04,\n","                       -7.7820e-03, -3.0029e-02],\n","                      [-9.4604e-03,  8.8215e-05,  2.3315e-02,  ...,  3.4790e-03,\n","                        3.3936e-02, -8.9722e-03],\n","                      [-3.3203e-02,  1.0010e-02,  8.3008e-03,  ...,  2.5269e-02,\n","                       -2.2705e-02,  3.3447e-02],\n","                      ...,\n","                      [-2.7954e-02, -1.0437e-02,  2.3193e-02,  ..., -5.4932e-03,\n","                       -2.3315e-02, -2.1118e-02],\n","                      [-3.1128e-02, -3.7598e-02,  2.0386e-02,  ...,  3.0518e-03,\n","                       -1.1475e-02, -1.5137e-02],\n","                      [-1.3916e-02,  2.3071e-02,  1.6357e-02,  ...,  1.5926e-04,\n","                       -9.3994e-03, -2.7832e-02]], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias',\n","              tensor([ 0.1206,  0.5586, -0.1680,  ..., -0.1875,  0.6445,  0.2656],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.22.layer_norm1.weight',\n","              tensor([1.4219, 1.3281, 1.3516,  ..., 1.3281, 1.3516, 1.4609],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.22.layer_norm1.bias',\n","              tensor([ 0.2930,  0.1377,  0.4551,  ..., -0.1006, -0.3301,  0.0457],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight',\n","              tensor([[ 0.0159,  0.0219,  0.0047,  ...,  0.0801, -0.0254, -0.0016],\n","                      [ 0.0339,  0.0583, -0.0488,  ...,  0.0295, -0.0056, -0.0063],\n","                      [ 0.0391,  0.0152, -0.0083,  ..., -0.0110, -0.0071,  0.0139],\n","                      ...,\n","                      [ 0.0025, -0.0226,  0.0295,  ..., -0.0400, -0.0322, -0.0147],\n","                      [-0.0062, -0.0033, -0.0123,  ..., -0.0121, -0.0051, -0.0225],\n","                      [-0.0093, -0.0334,  0.0181,  ...,  0.0055, -0.0221, -0.0104]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias',\n","              tensor([ 0.1377, -0.1865,  0.0457,  ..., -0.3613, -0.1445,  0.2734],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight',\n","              tensor([[-0.0007, -0.0315,  0.0171,  ...,  0.0006,  0.0106,  0.0232],\n","                      [-0.0317,  0.0400, -0.0334,  ..., -0.0123,  0.0107, -0.0140],\n","                      [ 0.0332, -0.0361, -0.0049,  ..., -0.0139, -0.0151, -0.0378],\n","                      ...,\n","                      [-0.0195, -0.0376,  0.0037,  ..., -0.0134, -0.0105,  0.0239],\n","                      [ 0.0249,  0.0165,  0.0127,  ...,  0.0136,  0.0132, -0.0055],\n","                      [-0.0251,  0.0054, -0.0022,  ...,  0.0417,  0.0125,  0.0378]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias',\n","              tensor([ 0.2773,  0.3281,  1.0859,  ..., -0.2217,  0.0449, -0.1455],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.22.layer_norm2.weight',\n","              tensor([ 5.6875,  7.3125,  8.0000,  ...,  5.6875, 10.0000,  7.1562],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.22.layer_norm2.bias',\n","              tensor([ 1.5781,  2.1719, -1.1250,  ..., -0.7031,  5.8438,  1.3984],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight',\n","              tensor([[-0.0039, -0.0027, -0.0011,  ..., -0.0011, -0.0232,  0.0135],\n","                      [ 0.0025,  0.0121, -0.0201,  ..., -0.0178,  0.0045,  0.0078],\n","                      [ 0.0010,  0.0137,  0.0050,  ...,  0.0168, -0.0194,  0.0276],\n","                      ...,\n","                      [-0.0082, -0.0192, -0.0330,  ...,  0.0275,  0.0222,  0.0101],\n","                      [ 0.0193, -0.0359, -0.0194,  ..., -0.0067, -0.0030,  0.0127],\n","                      [ 0.0007, -0.0349, -0.0403,  ...,  0.0101, -0.0352,  0.0010]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias',\n","              tensor([ 0.8008,  0.0160,  1.2188,  ..., -0.7070,  1.0703, -0.4141],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight',\n","              tensor([[-8.0566e-03, -9.3384e-03, -3.4668e-02,  ...,  4.9316e-02,\n","                       -9.0942e-03,  3.3691e-02],\n","                      [ 1.0742e-02, -1.9302e-03,  3.9551e-02,  ..., -4.0588e-03,\n","                       -4.7607e-03, -1.9531e-03],\n","                      [ 3.3936e-02, -1.1353e-02,  1.1169e-02,  ..., -1.8188e-02,\n","                       -7.5378e-03,  2.5635e-02],\n","                      ...,\n","                      [ 1.6357e-02,  7.6771e-05,  1.1047e-02,  ..., -1.5137e-02,\n","                        3.3691e-02,  4.6921e-04],\n","                      [ 3.2959e-02,  1.5320e-02, -9.6436e-03,  ...,  3.9062e-03,\n","                        7.4768e-03,  2.4719e-03],\n","                      [ 2.2583e-02, -3.3264e-03,  2.2217e-02,  ...,  1.4832e-02,\n","                        3.7109e-02, -5.6458e-03]], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias',\n","              tensor([ 0.0261, -0.0267,  0.1562,  ..., -0.0444, -0.1299, -0.0674],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight',\n","              tensor([[-0.0035, -0.0056, -0.0056,  ...,  0.0211, -0.0073, -0.0064],\n","                      [ 0.0061,  0.0068, -0.0398,  ..., -0.0066,  0.0050, -0.0104],\n","                      [ 0.0200,  0.0359,  0.0135,  ...,  0.0227, -0.0304,  0.0282],\n","                      ...,\n","                      [-0.0021,  0.0144, -0.0068,  ...,  0.0103, -0.0442, -0.0089],\n","                      [-0.0212, -0.0176, -0.0289,  ..., -0.0060,  0.0165, -0.0093],\n","                      [ 0.0184,  0.0187,  0.0168,  ...,  0.0294, -0.0181, -0.0242]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias',\n","              tensor([-0.1387,  0.0605, -0.0019,  ..., -0.2080,  0.0762,  0.1133],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight',\n","              tensor([[-0.0106,  0.0016, -0.0298,  ...,  0.0175,  0.0233, -0.0203],\n","                      [ 0.0046, -0.0102,  0.0090,  ..., -0.0270, -0.0214,  0.0356],\n","                      [ 0.0189, -0.0284, -0.0347,  ..., -0.0083, -0.0012,  0.0114],\n","                      ...,\n","                      [-0.0569, -0.0074,  0.0206,  ...,  0.0254, -0.0286, -0.0371],\n","                      [-0.0283, -0.0003, -0.0184,  ..., -0.0067,  0.0364,  0.0042],\n","                      [-0.0193,  0.0078, -0.0021,  ..., -0.0146,  0.0157,  0.0243]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias',\n","              tensor([0.3223, 0.3750, 0.4492,  ..., 0.1758, 0.1309, 0.0659],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.23.layer_norm1.weight',\n","              tensor([1.4609, 1.3672, 1.4062,  ..., 1.3828, 1.3203, 1.5078],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.23.layer_norm1.bias',\n","              tensor([ 0.3027, -0.1504,  0.4766,  ...,  0.0654, -0.5664,  0.1279],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight',\n","              tensor([[ 0.0238, -0.0201, -0.0322,  ...,  0.0092,  0.0022, -0.0160],\n","                      [-0.0193, -0.0147,  0.0050,  ..., -0.0030, -0.0282, -0.0173],\n","                      [ 0.0113,  0.0222, -0.0057,  ...,  0.0261, -0.0118, -0.0057],\n","                      ...,\n","                      [-0.0305, -0.0322,  0.0097,  ...,  0.0581, -0.0049,  0.0091],\n","                      [ 0.0075, -0.0205, -0.0109,  ..., -0.0376, -0.0006, -0.0474],\n","                      [ 0.0005,  0.0540,  0.0080,  ..., -0.0098,  0.0237, -0.0021]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias',\n","              tensor([-0.3398, -0.1240, -0.0179,  ..., -0.1836,  0.1021, -0.3633],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight',\n","              tensor([[ 0.0371,  0.0148,  0.0020,  ..., -0.0209,  0.0006, -0.0038],\n","                      [-0.0150, -0.0009, -0.0011,  ...,  0.0347,  0.0261,  0.0222],\n","                      [-0.0515,  0.0054, -0.0284,  ..., -0.0297, -0.0391, -0.0028],\n","                      ...,\n","                      [-0.0009, -0.0142, -0.0101,  ..., -0.0045, -0.0070, -0.0317],\n","                      [-0.0233,  0.0087, -0.0113,  ...,  0.0022, -0.0035, -0.0152],\n","                      [ 0.0120, -0.0525, -0.0004,  ...,  0.0203, -0.0154,  0.0215]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias',\n","              tensor([ 0.5625, -0.4199,  0.7188,  ..., -0.6523, -0.0147,  0.1270],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.23.layer_norm2.weight',\n","              tensor([ 7.5938, 10.6250,  8.4375,  ...,  7.8125, 11.3125,  7.9062],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.23.layer_norm2.bias',\n","              tensor([ 2.3750,  3.3750,  3.3750,  ...,  2.0156,  4.9688, -0.8555],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.weight',\n","              tensor([[ 0.0159,  0.0454,  0.0002,  ..., -0.0099,  0.0233,  0.0215],\n","                      [-0.0361, -0.0199,  0.0388,  ...,  0.0143, -0.0020,  0.0118],\n","                      [-0.0141,  0.0145, -0.0069,  ..., -0.0106, -0.0161, -0.0079],\n","                      ...,\n","                      [-0.0029,  0.0223, -0.0212,  ...,  0.0293, -0.0076,  0.0076],\n","                      [ 0.0021, -0.0122,  0.0198,  ...,  0.0132, -0.0109,  0.0271],\n","                      [-0.0156, -0.0420,  0.0102,  ...,  0.0244,  0.0250,  0.0177]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.bias',\n","              tensor([-0.6992,  1.9062, -1.6875,  ..., -0.7422, -0.0039, -1.1484],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.weight',\n","              tensor([[-1.1169e-02,  2.3682e-02, -8.3008e-03,  ..., -4.0039e-02,\n","                        1.4526e-02, -4.1504e-03],\n","                      [ 3.8330e-02,  1.0315e-02,  1.4343e-03,  ..., -2.1484e-02,\n","                       -2.9785e-02,  3.6774e-03],\n","                      [ 3.3936e-02,  2.7222e-02, -2.6978e-02,  ...,  2.5879e-02,\n","                       -4.9438e-03, -4.3154e-05],\n","                      ...,\n","                      [ 3.4943e-03, -1.3809e-03, -1.3489e-02,  ..., -1.6968e-02,\n","                       -4.9133e-03, -2.9175e-02],\n","                      [-4.2419e-03,  3.5553e-03,  7.3242e-03,  ..., -2.1667e-03,\n","                        2.3346e-03, -3.0518e-03],\n","                      [-1.2024e-02, -1.4771e-02, -9.6436e-03,  ...,  3.8574e-02,\n","                       -6.5002e-03, -2.9755e-03]], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.bias',\n","              tensor([ 0.2852, -0.0427, -0.0786,  ..., -0.3164,  0.1836,  0.1875],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.weight',\n","              tensor([[-0.0220,  0.0047, -0.0049,  ..., -0.0020,  0.0275,  0.0105],\n","                      [-0.0170, -0.0332, -0.0260,  ..., -0.0166, -0.0126, -0.0064],\n","                      [ 0.0033,  0.0275, -0.0432,  ..., -0.0413, -0.0515,  0.0069],\n","                      ...,\n","                      [ 0.0121,  0.0131,  0.0142,  ...,  0.0107,  0.0062, -0.0028],\n","                      [ 0.0102,  0.0175,  0.0432,  ..., -0.0149,  0.0486,  0.0151],\n","                      [-0.0082,  0.0728, -0.0051,  ..., -0.0315, -0.0027, -0.0028]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.bias',\n","              tensor([-0.1836, -0.0342,  0.0374,  ...,  0.0045,  0.0684,  0.2031],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.weight',\n","              tensor([[-1.8188e-02,  1.7944e-02, -3.4943e-03,  ...,  3.9368e-03,\n","                        3.3875e-03, -1.9455e-04],\n","                      [ 1.6602e-02,  3.2471e-02,  4.9072e-02,  ...,  1.1047e-02,\n","                       -4.0283e-02,  1.4465e-02],\n","                      [ 3.4485e-03, -9.7656e-04,  1.1169e-02,  ..., -3.2471e-02,\n","                       -3.6774e-03, -2.0447e-03],\n","                      ...,\n","                      [-2.5177e-03, -1.0986e-02, -2.8076e-02,  ...,  2.7954e-02,\n","                        1.8066e-02,  6.3477e-03],\n","                      [-3.3417e-03,  2.0447e-03,  1.1292e-02,  ..., -4.0054e-05,\n","                        8.2397e-03,  1.0681e-02],\n","                      [ 1.2756e-02, -1.1414e-02, -2.0630e-02,  ..., -1.1749e-03,\n","                        5.0659e-03, -5.3467e-02]], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.bias',\n","              tensor([ 0.2891,  0.0072, -0.1787,  ...,  0.0121,  0.1196,  0.0181],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.24.layer_norm1.weight',\n","              tensor([1.4922, 1.3125, 1.5078,  ..., 1.3594, 1.4531, 1.5547],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.24.layer_norm1.bias',\n","              tensor([ 0.1025, -0.1484,  0.0654,  ...,  0.0806, -0.6211,  0.0718],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.24.mlp.fc1.weight',\n","              tensor([[ 0.0107, -0.0309, -0.0087,  ..., -0.0115, -0.0228,  0.0132],\n","                      [ 0.0016,  0.0172,  0.0332,  ...,  0.0199,  0.0045,  0.0574],\n","                      [ 0.0269, -0.0281,  0.0610,  ...,  0.0203, -0.0154,  0.0070],\n","                      ...,\n","                      [ 0.0046, -0.0144,  0.0049,  ..., -0.0117,  0.0330,  0.0420],\n","                      [-0.0223,  0.0284,  0.0172,  ..., -0.0262,  0.0211,  0.0349],\n","                      [ 0.0092, -0.0030,  0.0225,  ..., -0.0067, -0.0101, -0.0260]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.24.mlp.fc1.bias',\n","              tensor([-0.7188,  0.1045, -0.1611,  ...,  0.3887, -0.2676, -0.2461],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.24.mlp.fc2.weight',\n","              tensor([[ 1.2573e-02,  2.5391e-02, -3.8330e-02,  ..., -5.9509e-03,\n","                        9.5215e-03, -2.0264e-02],\n","                      [-1.0376e-02, -3.0518e-03,  1.6846e-02,  ..., -8.7891e-03,\n","                        1.5869e-02, -5.7129e-02],\n","                      [ 3.4790e-03,  8.6670e-03,  3.4424e-02,  ...,  1.7578e-02,\n","                       -5.4321e-03,  5.0964e-03],\n","                      ...,\n","                      [ 3.6133e-02, -1.2451e-02,  3.5645e-02,  ..., -1.4038e-02,\n","                       -1.7578e-02,  4.9744e-03],\n","                      [-8.3008e-03, -4.6143e-02, -4.8340e-02,  ...,  4.3945e-02,\n","                        1.1846e-06, -1.2268e-02],\n","                      [-3.3447e-02, -2.9907e-02,  2.1667e-03,  ..., -1.8433e-02,\n","                        5.3406e-03,  4.2480e-02]], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.24.mlp.fc2.bias',\n","              tensor([ 0.4883, -0.7578,  0.2363,  ..., -0.5977, -0.9141, -0.1543],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.24.layer_norm2.weight',\n","              tensor([10.1875,  9.7500, 10.2500,  ...,  8.1875, 14.5000,  9.8125],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.24.layer_norm2.bias',\n","              tensor([3.7031, 1.2344, 3.8125,  ..., 0.6914, 6.7500, 0.8047],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.weight',\n","              tensor([[-0.0059,  0.0273, -0.0016,  ...,  0.0109, -0.0048, -0.0065],\n","                      [-0.0120,  0.0100, -0.0052,  ...,  0.0071, -0.0128, -0.0265],\n","                      [-0.0256,  0.0168, -0.0181,  ..., -0.0292, -0.0063, -0.0028],\n","                      ...,\n","                      [-0.0132, -0.0245, -0.0062,  ...,  0.0393, -0.0019, -0.0015],\n","                      [-0.0232, -0.0101,  0.0125,  ..., -0.0330,  0.0068, -0.0175],\n","                      [-0.0092,  0.0325, -0.0165,  ..., -0.0586, -0.0168,  0.0150]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.bias',\n","              tensor([ 0.2949,  0.9883,  0.8711,  ..., -0.3926,  0.2373, -0.0474],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.weight',\n","              tensor([[ 1.3367e-02,  2.8687e-03,  2.0630e-02,  ...,  1.2695e-02,\n","                       -2.2339e-02,  7.7820e-03],\n","                      [ 3.6621e-03,  1.1780e-02, -7.2021e-03,  ..., -1.4038e-02,\n","                        2.5391e-02, -1.2024e-02],\n","                      [-1.5747e-02, -1.6235e-02, -2.0386e-02,  ...,  4.3213e-02,\n","                        1.2329e-02,  2.7710e-02],\n","                      ...,\n","                      [-1.3916e-02, -9.7275e-05,  5.4321e-03,  ...,  4.4678e-02,\n","                       -6.9275e-03, -3.1982e-02],\n","                      [ 3.1250e-02, -7.8125e-02,  5.9509e-03,  ..., -3.1738e-02,\n","                       -3.0823e-03, -1.3000e-02],\n","                      [ 1.2512e-02, -3.0365e-03,  1.4648e-02,  ...,  2.1458e-04,\n","                       -3.2227e-02,  8.9111e-03]], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.bias',\n","              tensor([ 0.2354,  0.2637,  0.1514,  ..., -0.1504, -0.0110, -0.0223],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.weight',\n","              tensor([[ 0.0114, -0.0413, -0.0090,  ..., -0.0068,  0.0035,  0.0064],\n","                      [ 0.0347, -0.0156,  0.0289,  ..., -0.0166,  0.0154,  0.0349],\n","                      [ 0.0139,  0.0099,  0.0209,  ...,  0.0371, -0.0118, -0.0151],\n","                      ...,\n","                      [ 0.0205,  0.0131, -0.0177,  ...,  0.0111,  0.0129, -0.0226],\n","                      [-0.0008, -0.0293,  0.0167,  ..., -0.0398, -0.0208, -0.0032],\n","                      [ 0.0008, -0.0065, -0.0004,  ..., -0.0101,  0.0037,  0.0171]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.bias',\n","              tensor([ 0.0374, -0.0262, -0.0786,  ..., -0.0723, -0.0024, -0.0197],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.weight',\n","              tensor([[-0.0229, -0.0078,  0.0004,  ...,  0.0056,  0.0173, -0.0226],\n","                      [-0.0126,  0.0022,  0.0208,  ...,  0.0168,  0.0276,  0.0217],\n","                      [ 0.0083,  0.0111,  0.0067,  ..., -0.0483,  0.0162, -0.0038],\n","                      ...,\n","                      [-0.0205, -0.0089, -0.0074,  ..., -0.0381, -0.0129, -0.0106],\n","                      [ 0.0075, -0.0049,  0.0320,  ..., -0.0050, -0.0216, -0.0047],\n","                      [-0.0210,  0.0070, -0.0270,  ...,  0.0287,  0.0508, -0.0025]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.bias',\n","              tensor([-0.4160,  0.0684, -0.2617,  ..., -0.2520, -0.9102,  0.2070],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.25.layer_norm1.weight',\n","              tensor([1.2578, 1.1328, 1.2422,  ..., 1.1406, 1.1094, 1.1875],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.25.layer_norm1.bias',\n","              tensor([-0.1348, -0.0299,  0.0018,  ...,  0.1621, -0.5859,  0.0337],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.25.mlp.fc1.weight',\n","              tensor([[-0.0113,  0.0248, -0.0309,  ...,  0.0096, -0.0292, -0.0275],\n","                      [-0.0044,  0.0347,  0.0664,  ...,  0.0147, -0.0056,  0.0099],\n","                      [-0.0457,  0.0625, -0.0104,  ..., -0.0281, -0.0204, -0.0405],\n","                      ...,\n","                      [-0.0337,  0.0179, -0.0092,  ...,  0.0034,  0.0204,  0.0058],\n","                      [-0.0094, -0.0053, -0.0276,  ...,  0.0096,  0.0064,  0.0386],\n","                      [-0.0117,  0.0183, -0.0356,  ..., -0.0192,  0.0075, -0.0143]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.25.mlp.fc1.bias',\n","              tensor([ 0.4648,  0.2334,  0.0309,  ..., -0.3906,  0.1025,  0.0791],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.25.mlp.fc2.weight',\n","              tensor([[-0.0152, -0.0359,  0.0383,  ...,  0.0003,  0.0018,  0.0393],\n","                      [ 0.0439,  0.0187,  0.0243,  ..., -0.0265, -0.0006,  0.0217],\n","                      [-0.0063,  0.0105, -0.0081,  ..., -0.0204, -0.0073, -0.0315],\n","                      ...,\n","                      [ 0.0021, -0.0019, -0.0503,  ...,  0.0128,  0.0062, -0.0251],\n","                      [ 0.0032, -0.0781,  0.0016,  ...,  0.0286,  0.0058,  0.0239],\n","                      [ 0.0159, -0.0238,  0.0156,  ...,  0.0148,  0.0247, -0.0527]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.25.mlp.fc2.bias',\n","              tensor([-1.9688, -0.8203, -0.3027,  ...,  0.3594, -1.6953,  0.0110],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.25.layer_norm2.weight',\n","              tensor([9.9375, 7.1875, 7.3438,  ..., 7.8750, 9.6875, 7.1562],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.25.layer_norm2.bias',\n","              tensor([ 4.5312,  0.1167,  1.8516,  ..., -2.0781,  4.7188, -0.0447],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.weight',\n","              tensor([[ 0.0098,  0.0118, -0.0150,  ..., -0.0075, -0.0299,  0.0339],\n","                      [-0.0223,  0.0199,  0.0486,  ..., -0.0219, -0.0016, -0.0032],\n","                      [-0.0065, -0.0386, -0.0300,  ..., -0.0050, -0.0299,  0.0297],\n","                      ...,\n","                      [ 0.0430,  0.0275, -0.0027,  ...,  0.0146,  0.0312, -0.0117],\n","                      [-0.0315,  0.0284,  0.0239,  ..., -0.0161,  0.0464, -0.0132],\n","                      [ 0.0008,  0.0060, -0.0121,  ...,  0.0457,  0.0006, -0.0055]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.bias',\n","              tensor([-0.1572,  0.7461,  0.1533,  ..., -0.7656, -0.7891, -0.0162],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.weight',\n","              tensor([[ 0.0068, -0.0103, -0.0126,  ..., -0.0044,  0.0065, -0.0005],\n","                      [-0.0027,  0.0135, -0.0300,  ..., -0.0282,  0.0139,  0.0265],\n","                      [-0.0115,  0.0098, -0.0361,  ..., -0.0306,  0.0190,  0.0251],\n","                      ...,\n","                      [ 0.0056, -0.0070, -0.0096,  ..., -0.0266, -0.0225,  0.0023],\n","                      [-0.0303, -0.0064, -0.0249,  ..., -0.0161, -0.0074,  0.0065],\n","                      [-0.0161,  0.0013,  0.0099,  ...,  0.0120, -0.0239,  0.0176]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.bias',\n","              tensor([-0.2158,  0.3516,  0.2715,  ...,  0.2871,  0.2148, -0.1416],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.weight',\n","              tensor([[-8.3008e-03,  2.4780e-02, -9.3384e-03,  ...,  1.3504e-03,\n","                       -3.6774e-03,  2.0508e-02],\n","                      [-1.7944e-02,  1.7944e-02,  1.3977e-02,  ..., -1.9409e-02,\n","                       -3.8574e-02, -5.0049e-03],\n","                      [-3.2959e-03, -1.7822e-02,  1.2390e-02,  ...,  4.4632e-04,\n","                       -1.7090e-02, -5.6458e-03],\n","                      ...,\n","                      [ 3.4027e-03,  2.4902e-02, -3.2501e-03,  ...,  2.9785e-02,\n","                        1.5020e-05,  9.1934e-04],\n","                      [-3.8330e-02, -9.7656e-03, -2.6398e-03,  ..., -1.8921e-02,\n","                        1.0437e-02,  6.3171e-03],\n","                      [-2.5513e-02, -2.0386e-02,  1.2390e-02,  ..., -3.1433e-03,\n","                        2.1362e-03,  3.2715e-02]], dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.bias',\n","              tensor([ 0.0277, -0.0593, -0.0679,  ..., -0.0918,  0.2461,  0.6367],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.weight',\n","              tensor([[-0.0114,  0.0194,  0.0220,  ...,  0.0027,  0.0084,  0.0005],\n","                      [ 0.0095,  0.0049,  0.0082,  ..., -0.0157, -0.0217, -0.0005],\n","                      [ 0.0057,  0.0090,  0.0103,  ...,  0.0080,  0.0114, -0.0039],\n","                      ...,\n","                      [-0.0011,  0.0142,  0.0087,  ..., -0.0021,  0.0208, -0.0107],\n","                      [-0.0063, -0.0173, -0.0176,  ...,  0.0405,  0.0006,  0.0108],\n","                      [ 0.0068, -0.0306, -0.0265,  ..., -0.0182, -0.0266, -0.0028]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.bias',\n","              tensor([-1.5312, -0.2734, -0.3945,  ...,  0.1045, -1.3984,  0.3809],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.26.layer_norm1.weight',\n","              tensor([0.9023, 0.8906, 0.8594,  ..., 0.7734, 0.8242, 0.8555],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.26.layer_norm1.bias',\n","              tensor([-0.1465,  0.0767, -0.0491,  ...,  0.1719, -0.1436, -0.0237],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.26.mlp.fc1.weight',\n","              tensor([[-0.0154, -0.0145, -0.0098,  ..., -0.0026,  0.0073, -0.0159],\n","                      [-0.0027,  0.0008, -0.0045,  ...,  0.0085, -0.0292,  0.0190],\n","                      [-0.0148, -0.0197, -0.0156,  ...,  0.0183,  0.0425,  0.0493],\n","                      ...,\n","                      [-0.0339, -0.0223, -0.0043,  ..., -0.0055, -0.0154, -0.0150],\n","                      [ 0.0046,  0.0094, -0.0074,  ..., -0.0232,  0.0132, -0.0034],\n","                      [-0.0095,  0.0269,  0.0026,  ..., -0.0049,  0.0209, -0.0356]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.26.mlp.fc1.bias',\n","              tensor([ 0.1885,  1.0000, -0.3555,  ...,  0.0400, -0.7969, -0.3535],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.26.mlp.fc2.weight',\n","              tensor([[-0.0054,  0.0074,  0.0138,  ...,  0.0075,  0.0016,  0.0131],\n","                      [-0.0104, -0.0166,  0.0364,  ...,  0.0016, -0.0075, -0.0684],\n","                      [-0.0040,  0.0298, -0.0242,  ...,  0.0201, -0.0322,  0.0535],\n","                      ...,\n","                      [-0.0303,  0.0092,  0.0530,  ..., -0.0165,  0.0115, -0.0315],\n","                      [ 0.0225, -0.0171,  0.0583,  ..., -0.0253,  0.0051,  0.0189],\n","                      [ 0.0098,  0.0109, -0.0118,  ..., -0.0072,  0.0028, -0.0203]],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.26.mlp.fc2.bias',\n","              tensor([-1.2266, -0.1235, -0.6445,  ...,  0.7695, -0.1650,  0.0747],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.26.layer_norm2.weight',\n","              tensor([5.7812, 5.5000, 5.9375,  ..., 5.6562, 4.5625, 5.0312],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.encoder.layers.26.layer_norm2.bias',\n","              tensor([ 2.5000, -0.2490,  3.0625,  ..., -1.3828,  1.5312, -0.8750],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.post_layernorm.weight',\n","              tensor([1.6250, 1.4297, 1.5938,  ..., 1.6094, 1.7031, 1.4141],\n","                     dtype=torch.bfloat16)),\n","             ('vision_tower.vision_model.post_layernorm.bias',\n","              tensor([0.3652, 0.3047, 0.1816,  ..., 0.0366, 0.2598, 0.0684],\n","                     dtype=torch.bfloat16)),\n","             ('multi_modal_projector.linear.weight',\n","              tensor([[-0.0134,  0.0154, -0.0154,  ..., -0.0125, -0.0095, -0.0085],\n","                      [-0.0105, -0.0010, -0.0083,  ...,  0.0065, -0.0038, -0.0055],\n","                      [ 0.0003,  0.0090, -0.0023,  ..., -0.0026,  0.0098, -0.0077],\n","                      ...,\n","                      [ 0.0045,  0.0020,  0.0025,  ..., -0.0018,  0.0090, -0.0111],\n","                      [-0.0023,  0.0021,  0.0052,  ..., -0.0014,  0.0055,  0.0063],\n","                      [-0.0023,  0.0018, -0.0004,  ...,  0.0060, -0.0082, -0.0022]],\n","                     dtype=torch.bfloat16)),\n","             ('multi_modal_projector.linear.bias',\n","              tensor([-0.0160, -0.0022, -0.0107,  ..., -0.0095,  0.0028,  0.0009],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.embed_tokens.weight',\n","              tensor([[ 0.5352, -0.0250,  0.0874,  ...,  0.0432,  0.2617,  0.0674],\n","                      [ 0.1611, -0.1611, -0.1455,  ..., -0.0271,  0.0056, -0.0359],\n","                      [ 0.1177,  0.0214, -0.0289,  ..., -0.0042,  0.0044, -0.0056],\n","                      ...,\n","                      [ 0.3965, -0.0184,  0.0253,  ...,  0.0159,  0.1309,  0.0148],\n","                      [ 0.3965, -0.0182,  0.0260,  ...,  0.0157,  0.1309,  0.0151],\n","                      [ 0.3965, -0.0186,  0.0258,  ...,  0.0156,  0.1318,  0.0151]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.0.self_attn.q_proj.weight',\n","              tensor([[-3.6508e-06, -1.0559e-02, -2.0630e-02,  ...,  8.8501e-03,\n","                        2.1851e-02, -1.9409e-02],\n","                      [-4.9591e-05, -3.7109e-02,  1.0071e-02,  ...,  1.6235e-02,\n","                       -1.1719e-02,  2.1851e-02],\n","                      [ 1.0538e-04,  2.8839e-03,  1.7578e-02,  ..., -1.4893e-02,\n","                       -5.0964e-03,  7.7820e-04],\n","                      ...,\n","                      [-1.4343e-03,  1.0864e-02,  1.5335e-03,  ..., -4.7913e-03,\n","                        1.9531e-02, -2.0752e-03],\n","                      [-5.9509e-04, -2.2339e-02,  6.9885e-03,  ..., -1.6479e-02,\n","                       -4.8828e-03,  1.6937e-03],\n","                      [ 5.2643e-04, -9.8267e-03,  1.3550e-02,  ...,  1.9165e-02,\n","                        3.3447e-02,  1.6113e-02]], dtype=torch.bfloat16)),\n","             ('language_model.model.layers.0.self_attn.k_proj.weight',\n","              tensor([[-0.0004,  0.0091,  0.0033,  ..., -0.0129,  0.0009,  0.0047],\n","                      [ 0.0007, -0.0043,  0.0123,  ..., -0.0057, -0.0052,  0.0120],\n","                      [-0.0012, -0.0095, -0.0209,  ...,  0.0132,  0.0045,  0.0069],\n","                      ...,\n","                      [-0.0002,  0.0084, -0.0122,  ...,  0.0087, -0.0085,  0.0008],\n","                      [ 0.0008,  0.0035,  0.0011,  ...,  0.0063, -0.0153, -0.0208],\n","                      [ 0.0002,  0.0068,  0.0126,  ...,  0.0073, -0.0066,  0.0115]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.0.self_attn.v_proj.weight',\n","              tensor([[-0.0002, -0.0005, -0.0092,  ...,  0.0457,  0.0059,  0.0708],\n","                      [-0.0001, -0.0206, -0.0085,  ..., -0.0034, -0.0193,  0.0052],\n","                      [ 0.0008,  0.0396, -0.0188,  ..., -0.0361, -0.0201, -0.0145],\n","                      ...,\n","                      [-0.0012, -0.0299,  0.0121,  ..., -0.0022, -0.0119, -0.0352],\n","                      [-0.0005,  0.0129, -0.0145,  ..., -0.0033, -0.0217, -0.0028],\n","                      [ 0.0004, -0.0210, -0.0065,  ..., -0.0069,  0.0012,  0.0530]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.0.self_attn.o_proj.weight',\n","              tensor([[ 0.0216,  0.0228,  0.0038,  ..., -0.0083,  0.0087, -0.0155],\n","                      [ 0.0378,  0.0491, -0.0079,  ...,  0.0064,  0.0186, -0.0068],\n","                      [ 0.0405,  0.0354,  0.0003,  ..., -0.0215,  0.0041,  0.0078],\n","                      ...,\n","                      [-0.0161, -0.0114, -0.0085,  ..., -0.0014, -0.0032, -0.0013],\n","                      [-0.0060, -0.0376,  0.0005,  ...,  0.0007, -0.0491,  0.0209],\n","                      [ 0.0095, -0.0038, -0.0074,  ..., -0.0093,  0.0135,  0.0032]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.0.mlp.gate_proj.weight',\n","              tensor([[-0.0137,  0.0025,  0.0398,  ..., -0.0047,  0.0073, -0.0120],\n","                      [ 0.0209, -0.0012,  0.0087,  ...,  0.0152,  0.0334,  0.0334],\n","                      [ 0.0270,  0.0007,  0.0055,  ..., -0.0036,  0.0007, -0.0282],\n","                      ...,\n","                      [-0.0320,  0.0137, -0.0248,  ..., -0.0068,  0.0145,  0.0059],\n","                      [ 0.0091, -0.0170,  0.0164,  ...,  0.0391, -0.0148,  0.0425],\n","                      [ 0.0251,  0.0150,  0.0049,  ...,  0.0228, -0.0226, -0.0244]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.0.mlp.up_proj.weight',\n","              tensor([[-0.0065,  0.0272, -0.0120,  ..., -0.0015,  0.0211,  0.0135],\n","                      [ 0.0200, -0.0265, -0.0106,  ...,  0.0057, -0.0315,  0.0559],\n","                      [-0.0082, -0.0330,  0.0137,  ...,  0.0023, -0.0128,  0.0175],\n","                      ...,\n","                      [-0.0244, -0.0179,  0.0159,  ..., -0.0211,  0.0025, -0.0413],\n","                      [ 0.0134,  0.0038, -0.0299,  ...,  0.0288, -0.0261, -0.0025],\n","                      [-0.0021, -0.0231, -0.0039,  ..., -0.0197, -0.0098, -0.0066]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.0.mlp.down_proj.weight',\n","              tensor([[-0.0253, -0.0500, -0.0228,  ..., -0.0259, -0.0164,  0.0275],\n","                      [ 0.0442,  0.0126, -0.0038,  ..., -0.0003,  0.0040,  0.0255],\n","                      [ 0.0019,  0.0327,  0.0130,  ...,  0.0320, -0.0005,  0.0009],\n","                      ...,\n","                      [-0.0010, -0.0204,  0.0016,  ..., -0.0124, -0.0515, -0.0138],\n","                      [ 0.0381,  0.0364,  0.0126,  ...,  0.0060,  0.0432,  0.0292],\n","                      [-0.0098, -0.0374, -0.0280,  ..., -0.0242, -0.0138,  0.0037]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.0.input_layernorm.weight',\n","              tensor([-1.0000,  2.4375,  0.4297,  ...,  1.6953,  2.0000,  1.9766],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.0.post_attention_layernorm.weight',\n","              tensor([0.3574, 0.9258, 0.7031,  ..., 0.7266, 0.8164, 0.7227],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.1.self_attn.q_proj.weight',\n","              tensor([[-0.0020,  0.0072, -0.0008,  ..., -0.0084, -0.0050, -0.0038],\n","                      [-0.0084, -0.0017,  0.0103,  ...,  0.0079,  0.0021, -0.0209],\n","                      [ 0.0074, -0.0178, -0.0037,  ..., -0.0007, -0.0082,  0.0012],\n","                      ...,\n","                      [ 0.0145,  0.0220,  0.0077,  ...,  0.0850,  0.0128, -0.0277],\n","                      [ 0.0398,  0.0320,  0.0219,  ...,  0.0258,  0.0043, -0.0903],\n","                      [-0.0134, -0.0608,  0.0126,  ..., -0.0850,  0.0111,  0.0381]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.1.self_attn.k_proj.weight',\n","              tensor([[-0.0037, -0.0085,  0.0056,  ...,  0.0102, -0.0063,  0.0072],\n","                      [ 0.0114, -0.0065,  0.0028,  ..., -0.0026,  0.0016,  0.0010],\n","                      [ 0.0310,  0.0162,  0.0161,  ..., -0.0251,  0.0330,  0.0027],\n","                      ...,\n","                      [ 0.0104,  0.0062,  0.0342,  ..., -0.0110,  0.0500, -0.0048],\n","                      [ 0.0479, -0.0579,  0.0449,  ...,  0.0259,  0.0061,  0.0039],\n","                      [-0.0142,  0.0459,  0.0012,  ..., -0.0073, -0.0143, -0.0039]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.1.self_attn.v_proj.weight',\n","              tensor([[-0.0126,  0.0305, -0.0258,  ...,  0.0144,  0.0013,  0.0170],\n","                      [-0.0075, -0.0013, -0.0273,  ..., -0.0038, -0.0408,  0.0378],\n","                      [-0.0015, -0.0212,  0.0100,  ..., -0.0145, -0.0547, -0.0732],\n","                      ...,\n","                      [ 0.0144, -0.0081, -0.0388,  ..., -0.0186, -0.0097, -0.0106],\n","                      [ 0.0253, -0.0327,  0.0559,  ...,  0.0067, -0.0062,  0.0013],\n","                      [ 0.0454,  0.0154,  0.0035,  ...,  0.0042,  0.0452,  0.0322]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.1.self_attn.o_proj.weight',\n","              tensor([[ 0.0046, -0.0104,  0.0248,  ..., -0.0081,  0.0078,  0.0289],\n","                      [-0.0007,  0.0109,  0.0173,  ..., -0.0157, -0.0177, -0.0089],\n","                      [ 0.0114, -0.0037,  0.0212,  ..., -0.0040, -0.0289, -0.0271],\n","                      ...,\n","                      [ 0.0476,  0.0103, -0.0074,  ..., -0.0299,  0.0094,  0.0114],\n","                      [-0.0347, -0.0133,  0.0013,  ..., -0.0160,  0.0204,  0.0181],\n","                      [-0.0298, -0.0121,  0.0272,  ...,  0.0066, -0.0272,  0.0161]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.1.mlp.gate_proj.weight',\n","              tensor([[-0.0031,  0.0055,  0.0302,  ..., -0.0361,  0.0184,  0.0332],\n","                      [ 0.0352,  0.0291, -0.0413,  ...,  0.0033, -0.0134,  0.0214],\n","                      [ 0.0216,  0.0156,  0.0137,  ..., -0.0148,  0.0077,  0.0295],\n","                      ...,\n","                      [-0.0603, -0.0500, -0.0157,  ..., -0.0302, -0.0042,  0.0466],\n","                      [ 0.0007,  0.0051, -0.0422,  ..., -0.0352, -0.0081, -0.0286],\n","                      [ 0.0086, -0.0234, -0.0078,  ..., -0.0081,  0.0095, -0.0043]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.1.mlp.up_proj.weight',\n","              tensor([[-0.0525, -0.0461,  0.0297,  ...,  0.0044, -0.0031,  0.0356],\n","                      [-0.0427,  0.0559,  0.0245,  ..., -0.0033,  0.0063, -0.0361],\n","                      [-0.0168, -0.0016,  0.0018,  ...,  0.0043,  0.0118,  0.0437],\n","                      ...,\n","                      [-0.0178,  0.0515,  0.0114,  ..., -0.0172, -0.0264,  0.0884],\n","                      [ 0.0254, -0.0322, -0.0135,  ..., -0.0162, -0.0182, -0.0038],\n","                      [ 0.0022,  0.0106, -0.0132,  ..., -0.0142,  0.0236, -0.0034]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.1.mlp.down_proj.weight',\n","              tensor([[-0.0067,  0.0214, -0.0361,  ...,  0.0287, -0.0120,  0.0022],\n","                      [-0.0664,  0.0212,  0.0159,  ..., -0.0267,  0.0361, -0.0078],\n","                      [-0.0031, -0.0008, -0.0265,  ...,  0.0021, -0.0171,  0.0171],\n","                      ...,\n","                      [-0.0022,  0.0261, -0.0048,  ...,  0.0280, -0.0378,  0.0101],\n","                      [ 0.0172,  0.0479, -0.0001,  ..., -0.0237, -0.0393, -0.0190],\n","                      [-0.0108, -0.0070, -0.0361,  ..., -0.0159, -0.0214,  0.0117]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.1.input_layernorm.weight',\n","              tensor([0.5508, 0.3770, 0.5820,  ..., 0.4551, 0.3418, 0.2334],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.1.post_attention_layernorm.weight',\n","              tensor([1.0078, 0.9062, 1.1406,  ..., 0.9102, 1.1094, 0.7305],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.2.self_attn.q_proj.weight',\n","              tensor([[ 0.0010, -0.0095, -0.0098,  ...,  0.0051, -0.0004,  0.0195],\n","                      [-0.0079, -0.0028,  0.0061,  ..., -0.0020, -0.0028, -0.0071],\n","                      [ 0.0046, -0.0030, -0.0077,  ...,  0.0542,  0.0125, -0.0024],\n","                      ...,\n","                      [-0.0073,  0.0184,  0.0080,  ...,  0.0181, -0.0093, -0.0129],\n","                      [ 0.0028, -0.0011, -0.0227,  ...,  0.0056, -0.0013, -0.0139],\n","                      [ 0.0071, -0.0093, -0.0041,  ..., -0.0161, -0.0254, -0.0212]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.2.self_attn.k_proj.weight',\n","              tensor([[ 0.0004,  0.0276, -0.0002,  ...,  0.0055,  0.0118,  0.0128],\n","                      [ 0.0005, -0.0457,  0.0104,  ..., -0.0040,  0.0237, -0.0131],\n","                      [-0.0359,  0.0588, -0.0004,  ...,  0.0063, -0.0281,  0.0164],\n","                      ...,\n","                      [-0.0454, -0.0084, -0.0767,  ..., -0.0187, -0.0334, -0.0302],\n","                      [-0.0305,  0.0141, -0.0041,  ..., -0.0240,  0.0193,  0.0093],\n","                      [ 0.0139, -0.0352, -0.0173,  ...,  0.0151, -0.0459,  0.0063]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.2.self_attn.v_proj.weight',\n","              tensor([[-0.0025, -0.0115, -0.0337,  ...,  0.0322, -0.0029, -0.0123],\n","                      [-0.0181, -0.0092,  0.0126,  ..., -0.0178, -0.0413, -0.0069],\n","                      [-0.0308,  0.0113,  0.0264,  ...,  0.0128,  0.0087, -0.0378],\n","                      ...,\n","                      [ 0.0065, -0.0046,  0.0001,  ..., -0.0396,  0.0261,  0.0295],\n","                      [-0.0219, -0.0060,  0.0132,  ..., -0.0337, -0.0342, -0.0087],\n","                      [-0.0630,  0.0036,  0.0264,  ..., -0.0014, -0.0457,  0.0189]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.2.self_attn.o_proj.weight',\n","              tensor([[ 0.0066,  0.0422,  0.0251,  ...,  0.0086,  0.0064,  0.0190],\n","                      [-0.0065,  0.0022,  0.0113,  ..., -0.0004,  0.0312,  0.0227],\n","                      [ 0.0354, -0.0325,  0.0199,  ...,  0.0281, -0.0036, -0.0127],\n","                      ...,\n","                      [ 0.0001,  0.0118,  0.0055,  ..., -0.0630, -0.0154,  0.0281],\n","                      [-0.0259,  0.0172,  0.0225,  ...,  0.0303, -0.0009, -0.0330],\n","                      [-0.0063,  0.0059,  0.0104,  ...,  0.0417, -0.0277,  0.0388]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.2.mlp.gate_proj.weight',\n","              tensor([[-4.2480e-02,  2.6123e-02, -1.4954e-02,  ..., -1.2894e-03,\n","                       -4.3335e-03,  1.9165e-02],\n","                      [-1.3855e-02,  1.8799e-02,  5.7220e-04,  ..., -1.5991e-02,\n","                       -1.4282e-02,  1.2756e-02],\n","                      [ 3.9978e-03,  1.2131e-03, -1.3184e-02,  ..., -1.6235e-02,\n","                       -1.8311e-03,  9.3384e-03],\n","                      ...,\n","                      [ 6.3477e-03,  4.6875e-02,  9.8267e-03,  ..., -3.3569e-03,\n","                       -1.0834e-03, -1.8066e-02],\n","                      [ 1.9043e-02,  9.5215e-03,  2.5269e-02,  ..., -2.6733e-02,\n","                        2.4536e-02, -8.9722e-03],\n","                      [-9.2983e-05, -5.4016e-03,  2.2339e-02,  ..., -2.1240e-02,\n","                       -2.6489e-02, -2.4780e-02]], dtype=torch.bfloat16)),\n","             ('language_model.model.layers.2.mlp.up_proj.weight',\n","              tensor([[ 0.0253,  0.0112, -0.0105,  ...,  0.0020, -0.0036,  0.0325],\n","                      [-0.0206, -0.0476,  0.0099,  ...,  0.0242,  0.0310,  0.0206],\n","                      [-0.0238, -0.0060,  0.0107,  ..., -0.0117, -0.0034,  0.0056],\n","                      ...,\n","                      [ 0.0305,  0.0006,  0.0168,  ..., -0.0101,  0.0022,  0.0073],\n","                      [ 0.0052, -0.0187, -0.0488,  ..., -0.0366,  0.0231, -0.0442],\n","                      [ 0.0137, -0.0134,  0.0099,  ...,  0.0317, -0.0021,  0.0122]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.2.mlp.down_proj.weight',\n","              tensor([[ 2.6123e-02,  8.6060e-03,  2.8809e-02,  ...,  5.5847e-03,\n","                       -5.4321e-03,  7.0496e-03],\n","                      [-4.1260e-02, -2.1515e-03,  9.9487e-03,  ...,  9.7046e-03,\n","                        1.7334e-02,  1.9287e-02],\n","                      [ 7.3242e-03,  1.7822e-02, -1.1230e-02,  ...,  3.5889e-02,\n","                       -3.2471e-02, -8.1787e-03],\n","                      ...,\n","                      [ 1.4587e-02, -2.4109e-03, -7.9346e-03,  ..., -7.1716e-04,\n","                       -5.2002e-02, -9.7046e-03],\n","                      [ 1.1658e-02,  3.1128e-02,  6.6833e-03,  ...,  9.1791e-06,\n","                        1.6479e-02,  3.8910e-03],\n","                      [-1.8921e-02,  3.4332e-03, -4.8218e-03,  ..., -3.3447e-02,\n","                       -2.9419e-02,  1.1536e-02]], dtype=torch.bfloat16)),\n","             ('language_model.model.layers.2.input_layernorm.weight',\n","              tensor([1.1094, 0.7773, 1.1484,  ..., 0.8867, 1.0547, 0.6016],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.2.post_attention_layernorm.weight',\n","              tensor([1.8906, 1.2266, 1.9141,  ..., 1.3750, 1.9453, 0.9570],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.3.self_attn.q_proj.weight',\n","              tensor([[ 3.8605e-03,  6.2256e-03, -6.1646e-03,  ...,  7.4768e-03,\n","                        3.3264e-03,  1.3611e-02],\n","                      [ 1.2512e-02,  1.7090e-02,  1.0437e-02,  ...,  7.1106e-03,\n","                        1.0437e-02,  1.7944e-02],\n","                      [ 8.0490e-04, -4.6387e-03, -3.1494e-02,  ...,  5.0049e-03,\n","                        1.7700e-03,  1.8066e-02],\n","                      ...,\n","                      [-2.8198e-02, -4.1748e-02,  2.2278e-03,  ..., -4.8218e-03,\n","                       -1.2268e-02, -2.4048e-02],\n","                      [ 2.3346e-03,  7.2956e-05, -1.1658e-02,  ...,  1.7334e-02,\n","                        3.0884e-02,  2.8076e-02],\n","                      [ 8.9722e-03,  4.0527e-02,  1.4954e-02,  ...,  2.0020e-02,\n","                       -1.9653e-02, -5.3467e-02]], dtype=torch.bfloat16)),\n","             ('language_model.model.layers.3.self_attn.k_proj.weight',\n","              tensor([[ 0.0102, -0.0149,  0.0008,  ...,  0.0009,  0.0186, -0.0371],\n","                      [ 0.0075,  0.0134, -0.0342,  ...,  0.0292, -0.0442,  0.0025],\n","                      [ 0.0026, -0.0085, -0.0154,  ...,  0.0173, -0.0029,  0.0236],\n","                      ...,\n","                      [-0.0145, -0.0410,  0.0197,  ...,  0.0148, -0.0088, -0.0342],\n","                      [ 0.0045,  0.0040, -0.0084,  ...,  0.0076,  0.0081,  0.0422],\n","                      [ 0.0022,  0.0530,  0.0019,  ..., -0.0043,  0.0142, -0.0884]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.3.self_attn.v_proj.weight',\n","              tensor([[ 0.0073, -0.0140,  0.0016,  ..., -0.0077, -0.0052,  0.0104],\n","                      [-0.0413, -0.0055, -0.0220,  ..., -0.0027,  0.0149,  0.0109],\n","                      [-0.0223,  0.0049, -0.0234,  ...,  0.0035, -0.0059,  0.0171],\n","                      ...,\n","                      [-0.0520, -0.0011, -0.0204,  ...,  0.0245, -0.0172,  0.0120],\n","                      [ 0.0488, -0.0298, -0.0378,  ..., -0.0037, -0.0153, -0.0117],\n","                      [ 0.0003, -0.0352, -0.0004,  ...,  0.0288,  0.0220,  0.0393]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.3.self_attn.o_proj.weight',\n","              tensor([[-0.0013,  0.0337, -0.0276,  ...,  0.0182, -0.0056,  0.0076],\n","                      [ 0.0204, -0.0913,  0.0210,  ..., -0.0034,  0.0204, -0.0067],\n","                      [ 0.0084, -0.0123,  0.0187,  ...,  0.0066, -0.0125, -0.0039],\n","                      ...,\n","                      [-0.0227, -0.0160, -0.0223,  ..., -0.0104, -0.0074, -0.0081],\n","                      [-0.0015,  0.0369,  0.0117,  ...,  0.0036,  0.0080, -0.0131],\n","                      [-0.0006,  0.0515,  0.0086,  ..., -0.0063,  0.0147,  0.0077]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.3.mlp.gate_proj.weight',\n","              tensor([[ 0.0811, -0.0271,  0.0306,  ..., -0.0165,  0.0427, -0.0312],\n","                      [ 0.0371,  0.0089, -0.0151,  ...,  0.0045,  0.0148,  0.0027],\n","                      [ 0.0120,  0.0247,  0.0139,  ...,  0.0126, -0.0383, -0.0101],\n","                      ...,\n","                      [-0.0137,  0.0256, -0.0254,  ...,  0.0009,  0.0078, -0.0466],\n","                      [ 0.0162,  0.0222,  0.0221,  ..., -0.0036,  0.0051, -0.0106],\n","                      [ 0.0311,  0.0243, -0.0292,  ..., -0.0282, -0.0295, -0.0223]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.3.mlp.up_proj.weight',\n","              tensor([[ 0.0197,  0.0188, -0.0147,  ...,  0.0405,  0.0166, -0.0022],\n","                      [-0.0109,  0.0135, -0.0271,  ...,  0.0030, -0.0364, -0.0088],\n","                      [-0.0413,  0.0126, -0.0208,  ..., -0.0304, -0.0238,  0.0520],\n","                      ...,\n","                      [ 0.0150, -0.0349, -0.0393,  ..., -0.0461, -0.0728, -0.0104],\n","                      [-0.0347,  0.0128,  0.0018,  ...,  0.0396,  0.0150, -0.0713],\n","                      [ 0.0447,  0.0034,  0.0417,  ...,  0.0334,  0.0062,  0.0025]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.3.mlp.down_proj.weight',\n","              tensor([[ 0.0039, -0.0242, -0.0159,  ...,  0.0143, -0.0273,  0.0182],\n","                      [-0.0148, -0.0176,  0.0183,  ...,  0.0051,  0.0011,  0.0211],\n","                      [-0.0063,  0.0132,  0.0025,  ..., -0.0089, -0.0031,  0.0479],\n","                      ...,\n","                      [-0.0226,  0.0194,  0.0098,  ..., -0.0221,  0.0271,  0.0229],\n","                      [-0.0282,  0.0122, -0.0014,  ..., -0.0354, -0.0110,  0.0165],\n","                      [-0.0215, -0.0168,  0.0144,  ..., -0.0022, -0.0225, -0.0109]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.3.input_layernorm.weight',\n","              tensor([1.5938, 0.6562, 0.9141,  ..., 0.4180, 0.8398, 0.1943],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.3.post_attention_layernorm.weight',\n","              tensor([2.4375, 1.0781, 2.0469,  ..., 1.2734, 2.1250, 0.8555],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.4.self_attn.q_proj.weight',\n","              tensor([[ 0.0306, -0.0469, -0.0562,  ..., -0.0496, -0.0160, -0.0017],\n","                      [ 0.0172,  0.0320,  0.0004,  ...,  0.0078,  0.0229, -0.0630],\n","                      [ 0.0129,  0.0243, -0.0022,  ..., -0.0342, -0.0232, -0.0068],\n","                      ...,\n","                      [-0.0344,  0.0065, -0.0703,  ...,  0.0515, -0.0105, -0.0148],\n","                      [ 0.0282, -0.0356, -0.0376,  ..., -0.0204,  0.0236, -0.0046],\n","                      [ 0.0388, -0.0339, -0.0108,  ...,  0.0640,  0.0051, -0.0136]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.4.self_attn.k_proj.weight',\n","              tensor([[ 0.0148, -0.0014, -0.0092,  ...,  0.0197,  0.0101, -0.0009],\n","                      [-0.0148, -0.0003, -0.0210,  ..., -0.0320, -0.0112,  0.0146],\n","                      [ 0.0364,  0.0168, -0.0144,  ...,  0.0135, -0.0135,  0.0087],\n","                      ...,\n","                      [-0.0046,  0.0149, -0.0153,  ..., -0.0253, -0.0286,  0.0286],\n","                      [ 0.0206,  0.0354, -0.0175,  ...,  0.0130, -0.0144,  0.0332],\n","                      [-0.0439,  0.0312,  0.0095,  ..., -0.0342,  0.0157,  0.0547]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.4.self_attn.v_proj.weight',\n","              tensor([[-1.5991e-02, -9.4604e-03,  6.5430e-02,  ...,  1.6846e-02,\n","                       -2.8687e-02,  1.0864e-02],\n","                      [-1.2207e-02, -2.4719e-03, -1.6113e-02,  ..., -5.7373e-03,\n","                        3.4668e-02, -2.0508e-02],\n","                      [ 4.0039e-02, -3.2715e-02,  3.8910e-03,  ..., -5.4688e-02,\n","                        7.5989e-03,  3.4912e-02],\n","                      ...,\n","                      [-3.3447e-02, -1.3809e-03, -5.0068e-05,  ..., -1.4038e-02,\n","                        1.8066e-02, -9.5825e-03],\n","                      [ 2.4292e-02,  7.0801e-02, -1.7700e-03,  ...,  4.1504e-02,\n","                       -2.4414e-02, -2.1240e-02],\n","                      [ 1.2329e-02, -1.5015e-02,  3.7598e-02,  ...,  3.5889e-02,\n","                       -2.3315e-02,  3.5400e-02]], dtype=torch.bfloat16)),\n","             ('language_model.model.layers.4.self_attn.o_proj.weight',\n","              tensor([[ 0.0645,  0.0249,  0.0009,  ..., -0.0679,  0.0002,  0.0457],\n","                      [ 0.0420, -0.0176, -0.0113,  ...,  0.0171,  0.0200, -0.0049],\n","                      [ 0.0469, -0.0267,  0.0076,  ...,  0.0114, -0.0079,  0.0022],\n","                      ...,\n","                      [-0.0167,  0.0256, -0.0432,  ..., -0.0159, -0.0245,  0.0025],\n","                      [ 0.0167, -0.0096,  0.0042,  ...,  0.0201,  0.0171, -0.0094],\n","                      [-0.0002,  0.0334,  0.0161,  ...,  0.0052,  0.0525,  0.0023]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.4.mlp.gate_proj.weight',\n","              tensor([[ 0.0063,  0.0042, -0.0317,  ..., -0.0204, -0.0186, -0.0139],\n","                      [ 0.0157, -0.0199, -0.0359,  ..., -0.0095,  0.0014,  0.0025],\n","                      [-0.0378,  0.0021, -0.0045,  ..., -0.0001,  0.0121, -0.0045],\n","                      ...,\n","                      [-0.0161, -0.0317, -0.0300,  ..., -0.0089,  0.0229,  0.0058],\n","                      [-0.0042, -0.0135,  0.0074,  ...,  0.0227,  0.0016, -0.0488],\n","                      [-0.0208,  0.0237,  0.0374,  ..., -0.0179, -0.0398,  0.0099]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.4.mlp.up_proj.weight',\n","              tensor([[-0.0231,  0.0199,  0.0021,  ..., -0.0137, -0.0205, -0.0284],\n","                      [ 0.0055,  0.0080,  0.0229,  ...,  0.0125,  0.0176, -0.0110],\n","                      [-0.0151, -0.0332,  0.0703,  ...,  0.0019, -0.0117, -0.0063],\n","                      ...,\n","                      [-0.0322, -0.0091,  0.0248,  ..., -0.0046, -0.0054,  0.0270],\n","                      [ 0.0281,  0.0165, -0.0131,  ..., -0.0167,  0.0291, -0.0339],\n","                      [-0.0315, -0.0034, -0.0023,  ..., -0.0288,  0.0043, -0.0264]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.4.mlp.down_proj.weight',\n","              tensor([[ 0.0309,  0.0231,  0.0011,  ..., -0.0012,  0.0064, -0.0283],\n","                      [-0.0127,  0.0211,  0.0004,  ..., -0.0540,  0.0067,  0.0099],\n","                      [ 0.0131,  0.0203,  0.0405,  ...,  0.0029,  0.0018,  0.0088],\n","                      ...,\n","                      [ 0.0309,  0.0190, -0.0152,  ..., -0.0094, -0.0179, -0.0134],\n","                      [ 0.0107,  0.0023, -0.0121,  ..., -0.0137,  0.0139,  0.0214],\n","                      [ 0.0374, -0.0066, -0.0398,  ...,  0.0020, -0.0090,  0.0056]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.4.input_layernorm.weight',\n","              tensor([1.1953, 0.8164, 1.0000,  ..., 0.5898, 1.4375, 0.2432],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.4.post_attention_layernorm.weight',\n","              tensor([2.3594, 0.9688, 2.2031,  ..., 1.2188, 2.5469, 0.7578],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.5.self_attn.q_proj.weight',\n","              tensor([[-5.7373e-02,  2.1118e-02, -1.5564e-02,  ...,  3.8818e-02,\n","                       -2.0508e-02,  4.1504e-02],\n","                      [-5.9570e-02,  2.2583e-02, -2.7466e-02,  ...,  3.2715e-02,\n","                       -2.7954e-02,  4.3457e-02],\n","                      [ 6.3782e-03, -1.0925e-02, -3.6621e-02,  ..., -6.4453e-02,\n","                        2.8931e-02, -9.1076e-05],\n","                      ...,\n","                      [-1.0193e-02,  1.2085e-02, -1.4114e-04,  ...,  2.7924e-03,\n","                        1.1292e-03, -9.3384e-03],\n","                      [ 1.3245e-02, -3.6621e-03,  1.1826e-04,  ...,  6.9275e-03,\n","                        6.6757e-05,  1.7456e-02],\n","                      [-1.7334e-02, -2.8442e-02,  1.1475e-02,  ..., -6.9580e-03,\n","                       -1.4526e-02,  4.0283e-03]], dtype=torch.bfloat16)),\n","             ('language_model.model.layers.5.self_attn.k_proj.weight',\n","              tensor([[-0.0006,  0.0209,  0.0309,  ..., -0.0247, -0.0105, -0.0035],\n","                      [ 0.0292, -0.0298, -0.0047,  ..., -0.0129, -0.0212,  0.0110],\n","                      [ 0.0310, -0.0281, -0.0173,  ...,  0.0034,  0.0003, -0.0038],\n","                      ...,\n","                      [ 0.0359,  0.0162, -0.0674,  ...,  0.0220,  0.0253, -0.0771],\n","                      [-0.0535, -0.0251, -0.0435,  ...,  0.0623,  0.0452,  0.0801],\n","                      [ 0.0136,  0.0198,  0.0437,  ..., -0.0286, -0.0016,  0.0581]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.5.self_attn.v_proj.weight',\n","              tensor([[-0.0150, -0.0209,  0.0250,  ...,  0.0128,  0.0142,  0.0056],\n","                      [ 0.0247, -0.0220, -0.0145,  ...,  0.0281,  0.0226,  0.0012],\n","                      [-0.0242,  0.0049,  0.0178,  ..., -0.0144,  0.0075, -0.0090],\n","                      ...,\n","                      [-0.0232,  0.0342, -0.0190,  ...,  0.0028,  0.0162, -0.0150],\n","                      [-0.0201, -0.0005,  0.0254,  ..., -0.0309, -0.0339,  0.0250],\n","                      [ 0.0320, -0.0009,  0.0033,  ...,  0.0165, -0.0137, -0.0005]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.5.self_attn.o_proj.weight',\n","              tensor([[ 0.0240, -0.0295,  0.0311,  ...,  0.0459,  0.0286,  0.0243],\n","                      [ 0.0203,  0.0211, -0.0186,  ..., -0.0119,  0.0113,  0.0167],\n","                      [-0.0084,  0.0103, -0.0232,  ..., -0.0254,  0.0074, -0.0055],\n","                      ...,\n","                      [-0.0117, -0.0520,  0.0271,  ..., -0.0077, -0.0327,  0.0250],\n","                      [-0.0160, -0.0045,  0.0028,  ..., -0.0145, -0.0032, -0.0474],\n","                      [-0.0110, -0.0078,  0.0093,  ...,  0.0118, -0.0133,  0.0001]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.5.mlp.gate_proj.weight',\n","              tensor([[ 1.5991e-02,  5.1880e-03,  7.4005e-04,  ...,  1.0498e-02,\n","                       -9.8267e-03,  2.1973e-02],\n","                      [ 1.2207e-02,  2.8687e-02,  8.0490e-04,  ...,  1.2390e-02,\n","                       -1.4404e-02,  2.6123e-02],\n","                      [ 1.9165e-02,  1.8921e-02,  7.7209e-03,  ..., -2.1118e-02,\n","                       -4.2419e-03,  6.9580e-03],\n","                      ...,\n","                      [-5.2979e-02, -1.7014e-03, -2.4170e-02,  ...,  7.9346e-03,\n","                       -6.7139e-03,  5.7129e-02],\n","                      [ 1.2360e-03,  2.3560e-02, -2.4536e-02,  ..., -1.4832e-02,\n","                       -3.7537e-03,  1.4160e-02],\n","                      [-9.5367e-06, -8.6670e-03, -1.5991e-02,  ...,  4.3640e-03,\n","                       -1.5869e-02, -1.9226e-03]], dtype=torch.bfloat16)),\n","             ('language_model.model.layers.5.mlp.up_proj.weight',\n","              tensor([[ 2.7313e-03, -3.9062e-02, -1.2939e-02,  ..., -1.3672e-02,\n","                       -2.9053e-02,  3.1006e-02],\n","                      [ 3.2471e-02,  3.0273e-02,  7.3242e-03,  ..., -5.8899e-03,\n","                       -5.6458e-03, -2.4902e-02],\n","                      [-4.7607e-03, -1.6602e-02,  2.9175e-02,  ..., -3.6377e-02,\n","                        3.8574e-02, -2.9419e-02],\n","                      ...,\n","                      [-1.2207e-02, -2.5757e-02, -1.7578e-02,  ..., -3.9978e-03,\n","                       -3.0151e-02,  1.5747e-02],\n","                      [-1.2329e-02,  2.1973e-03,  1.2390e-02,  ...,  2.0996e-02,\n","                       -4.3631e-05,  1.9653e-02],\n","                      [ 1.4587e-02, -5.6152e-02, -1.4893e-02,  ...,  1.8433e-02,\n","                       -3.5889e-02,  6.7749e-03]], dtype=torch.bfloat16)),\n","             ('language_model.model.layers.5.mlp.down_proj.weight',\n","              tensor([[-0.0115,  0.0069, -0.0199,  ...,  0.0047, -0.0197,  0.0108],\n","                      [ 0.0034,  0.0123, -0.0288,  ...,  0.0043,  0.0204, -0.0461],\n","                      [-0.0278,  0.0262,  0.0298,  ..., -0.0140,  0.0154,  0.0330],\n","                      ...,\n","                      [-0.0134,  0.0007,  0.0369,  ..., -0.0024,  0.0209, -0.0325],\n","                      [-0.0469,  0.0137, -0.0019,  ..., -0.0023,  0.0139, -0.0084],\n","                      [ 0.0190,  0.0087,  0.0172,  ...,  0.0500,  0.0157, -0.0102]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.5.input_layernorm.weight',\n","              tensor([1.7031, 0.9062, 1.4688,  ..., 0.6445, 1.7422, 0.2432],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.5.post_attention_layernorm.weight',\n","              tensor([2.6406, 0.9414, 2.6562,  ..., 1.3672, 3.0312, 0.8828],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.6.self_attn.q_proj.weight',\n","              tensor([[ 0.0070, -0.0007,  0.0087,  ...,  0.0131,  0.0039,  0.0161],\n","                      [ 0.0118, -0.0095,  0.0118,  ...,  0.0243, -0.0080,  0.0427],\n","                      [ 0.0014,  0.0098,  0.0085,  ..., -0.0025, -0.0033, -0.0027],\n","                      ...,\n","                      [-0.0115,  0.1436,  0.0374,  ..., -0.0645, -0.0219,  0.0059],\n","                      [-0.0275,  0.0049,  0.0136,  ..., -0.0796,  0.0005, -0.0211],\n","                      [ 0.0137, -0.0972, -0.0542,  ..., -0.0183, -0.0297,  0.0137]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.6.self_attn.k_proj.weight',\n","              tensor([[ 0.0049, -0.0219, -0.0175,  ...,  0.0182,  0.0065, -0.0002],\n","                      [-0.0074, -0.0087,  0.0170,  ...,  0.0057, -0.0035, -0.0060],\n","                      [ 0.0002, -0.0015, -0.0126,  ...,  0.0043,  0.0112, -0.0156],\n","                      ...,\n","                      [ 0.0229,  0.0610,  0.0245,  ..., -0.0122, -0.0127,  0.0312],\n","                      [-0.0260, -0.0562, -0.0554,  ..., -0.0381, -0.0034, -0.0270],\n","                      [ 0.0262,  0.0094,  0.0025,  ..., -0.0154,  0.0087,  0.0199]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.6.self_attn.v_proj.weight',\n","              tensor([[-0.0022,  0.0021, -0.0123,  ..., -0.0297, -0.0111,  0.0009],\n","                      [ 0.0291,  0.0032, -0.0208,  ...,  0.0034,  0.0304,  0.0116],\n","                      [-0.0405, -0.0167,  0.0088,  ...,  0.0008,  0.0052, -0.0413],\n","                      ...,\n","                      [ 0.0089,  0.0308, -0.0256,  ...,  0.0659, -0.0654, -0.0121],\n","                      [-0.0344,  0.0137,  0.0070,  ...,  0.0190, -0.0016,  0.0247],\n","                      [-0.0515, -0.0141,  0.0217,  ..., -0.0378,  0.0510, -0.0306]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.6.self_attn.o_proj.weight',\n","              tensor([[-0.0074,  0.0194, -0.0417,  ...,  0.0325, -0.0042,  0.0016],\n","                      [ 0.0126, -0.0079, -0.0381,  ...,  0.0226,  0.0142, -0.0058],\n","                      [-0.0002, -0.0198,  0.0063,  ..., -0.0154, -0.0117, -0.0525],\n","                      ...,\n","                      [ 0.0019, -0.0153,  0.0010,  ...,  0.0210,  0.0304, -0.0002],\n","                      [ 0.0118, -0.0016, -0.0108,  ..., -0.0151,  0.0019,  0.0188],\n","                      [-0.0019, -0.0210,  0.0067,  ..., -0.0102,  0.0194,  0.0118]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.6.mlp.gate_proj.weight',\n","              tensor([[ 0.0103,  0.0400,  0.0237,  ...,  0.0041,  0.0105,  0.0064],\n","                      [-0.0303, -0.0013,  0.0349,  ..., -0.0535,  0.0248, -0.0537],\n","                      [ 0.0393,  0.0099,  0.0173,  ...,  0.0173,  0.0245,  0.0145],\n","                      ...,\n","                      [ 0.0063, -0.0073, -0.0206,  ..., -0.0289, -0.0265,  0.0035],\n","                      [-0.0227, -0.0259,  0.0322,  ..., -0.0074, -0.0265, -0.0093],\n","                      [-0.0173,  0.0079, -0.0267,  ...,  0.0112, -0.0032,  0.0016]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.6.mlp.up_proj.weight',\n","              tensor([[-0.0072,  0.0087, -0.0033,  ..., -0.0366,  0.0255, -0.0126],\n","                      [-0.0142, -0.0381, -0.0320,  ...,  0.0079, -0.0226, -0.0055],\n","                      [-0.0222,  0.0035,  0.0045,  ..., -0.0148, -0.0498,  0.0129],\n","                      ...,\n","                      [ 0.0183,  0.0154,  0.0173,  ..., -0.0022, -0.0098,  0.0208],\n","                      [ 0.0007, -0.0063, -0.0226,  ..., -0.0270,  0.0089,  0.0182],\n","                      [-0.0291, -0.0204,  0.0025,  ..., -0.0289,  0.0038,  0.0287]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.6.mlp.down_proj.weight',\n","              tensor([[-0.0309,  0.0090, -0.0309,  ..., -0.0052,  0.0067, -0.0140],\n","                      [-0.0101, -0.0016, -0.0071,  ...,  0.0342, -0.0072,  0.0361],\n","                      [-0.0010, -0.0369, -0.0081,  ..., -0.0101,  0.0309, -0.0012],\n","                      ...,\n","                      [-0.0141, -0.0119, -0.0115,  ..., -0.0309,  0.0179,  0.0049],\n","                      [-0.0109, -0.0155, -0.0166,  ..., -0.0147,  0.0024, -0.0147],\n","                      [-0.0155, -0.0354, -0.0125,  ...,  0.0145, -0.0053,  0.0215]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.6.input_layernorm.weight',\n","              tensor([2.1562, 0.9844, 2.2656,  ..., 0.9180, 3.2500, 0.2354],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.6.post_attention_layernorm.weight',\n","              tensor([2.6406, 0.8164, 2.6875,  ..., 1.3516, 3.5781, 0.8945],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.7.self_attn.q_proj.weight',\n","              tensor([[-0.0530,  0.0610,  0.0060,  ..., -0.0165,  0.0544, -0.0098],\n","                      [ 0.0127,  0.0176,  0.0334,  ...,  0.0032, -0.0120,  0.0044],\n","                      [-0.0308,  0.0957,  0.0173,  ..., -0.0393,  0.0186, -0.0378],\n","                      ...,\n","                      [-0.0327,  0.0396, -0.0374,  ...,  0.0032,  0.0308,  0.0123],\n","                      [ 0.0342,  0.0223, -0.0067,  ..., -0.0200, -0.0364, -0.0200],\n","                      [ 0.0077, -0.0113, -0.0334,  ..., -0.0271, -0.1123, -0.0242]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.7.self_attn.k_proj.weight',\n","              tensor([[-0.0216,  0.0140, -0.0251,  ..., -0.0056,  0.0206,  0.0216],\n","                      [-0.0045, -0.0105, -0.0077,  ...,  0.0254,  0.0315,  0.0110],\n","                      [-0.0312,  0.0225, -0.0342,  ...,  0.0102, -0.0018,  0.0270],\n","                      ...,\n","                      [-0.0177,  0.0850, -0.0544,  ...,  0.0173,  0.0085, -0.0095],\n","                      [ 0.0500,  0.0023, -0.0161,  ...,  0.0005,  0.0095, -0.0151],\n","                      [ 0.0620,  0.0337, -0.0332,  ..., -0.0176, -0.0525, -0.1533]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.7.self_attn.v_proj.weight',\n","              tensor([[-0.0266, -0.0018,  0.0332,  ...,  0.0197,  0.0332, -0.0211],\n","                      [-0.0718,  0.0179,  0.0189,  ...,  0.0054, -0.0160,  0.0305],\n","                      [-0.0051,  0.0311, -0.0090,  ...,  0.0203, -0.0177, -0.0049],\n","                      ...,\n","                      [-0.0435,  0.0090, -0.0388,  ...,  0.0444, -0.0349, -0.0275],\n","                      [-0.0410,  0.0183,  0.0464,  ...,  0.0391,  0.0459, -0.0461],\n","                      [ 0.0146,  0.0317,  0.0110,  ...,  0.0145,  0.0317, -0.0079]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.7.self_attn.o_proj.weight',\n","              tensor([[ 0.0092, -0.0099,  0.0234,  ...,  0.0244,  0.0051,  0.0020],\n","                      [-0.0454,  0.0081, -0.0317,  ..., -0.0123, -0.0208,  0.0015],\n","                      [-0.0182, -0.0391, -0.0058,  ..., -0.0264,  0.0214,  0.0283],\n","                      ...,\n","                      [-0.0260,  0.0171, -0.0029,  ...,  0.0056,  0.0044,  0.0093],\n","                      [ 0.0052,  0.0061, -0.0049,  ..., -0.0272,  0.0106,  0.0104],\n","                      [-0.0184, -0.0093, -0.0142,  ...,  0.0267, -0.0112,  0.0145]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.7.mlp.gate_proj.weight',\n","              tensor([[ 0.0063,  0.0024, -0.0018,  ...,  0.0396, -0.0134, -0.0280],\n","                      [-0.0063,  0.0046, -0.0090,  ..., -0.0024,  0.0190, -0.0151],\n","                      [ 0.0096,  0.0199, -0.0181,  ..., -0.0019,  0.0128,  0.0157],\n","                      ...,\n","                      [ 0.0118,  0.0103, -0.0101,  ...,  0.0205,  0.0091,  0.0168],\n","                      [-0.0457,  0.0179,  0.0532,  ...,  0.0147, -0.0164, -0.0344],\n","                      [-0.0153, -0.0060,  0.0214,  ..., -0.0123,  0.0347,  0.0096]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.7.mlp.up_proj.weight',\n","              tensor([[ 0.0126, -0.0308, -0.0144,  ..., -0.0272,  0.0549,  0.0286],\n","                      [ 0.0036, -0.0010,  0.0197,  ...,  0.0250, -0.0144,  0.0369],\n","                      [-0.0271,  0.0010,  0.0084,  ..., -0.0043, -0.0029,  0.0154],\n","                      ...,\n","                      [ 0.0199,  0.0262, -0.0086,  ..., -0.0178,  0.0008,  0.0544],\n","                      [-0.0042,  0.0742,  0.0097,  ..., -0.0134,  0.0269, -0.0400],\n","                      [ 0.0284, -0.0076, -0.0140,  ..., -0.0267,  0.0105, -0.0033]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.7.mlp.down_proj.weight',\n","              tensor([[-0.0072,  0.0042, -0.0216,  ..., -0.0105, -0.0066,  0.0410],\n","                      [ 0.0068,  0.0039,  0.0422,  ...,  0.0081, -0.0327, -0.0028],\n","                      [ 0.0244,  0.0208, -0.0334,  ...,  0.0038, -0.0114, -0.0201],\n","                      ...,\n","                      [-0.0337,  0.0166, -0.0182,  ...,  0.0008, -0.0145, -0.0312],\n","                      [ 0.0344, -0.0170,  0.0009,  ..., -0.0366,  0.0160,  0.0117],\n","                      [ 0.0374,  0.0093,  0.0204,  ..., -0.0010,  0.0479, -0.0280]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.7.input_layernorm.weight',\n","              tensor([1.4922, 0.8242, 1.5078,  ..., 0.9258, 2.0156, 0.4434],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.7.post_attention_layernorm.weight',\n","              tensor([2.4531, 0.6641, 2.4688,  ..., 1.1484, 3.1406, 0.7266],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.8.self_attn.q_proj.weight',\n","              tensor([[ 0.0209,  0.0405,  0.0012,  ...,  0.0294, -0.0137,  0.0114],\n","                      [ 0.0454, -0.0249,  0.0102,  ...,  0.0211, -0.0183, -0.0078],\n","                      [ 0.0002, -0.0260, -0.0079,  ..., -0.0396,  0.0347, -0.0160],\n","                      ...,\n","                      [ 0.0295, -0.0160, -0.0732,  ...,  0.0201, -0.0415, -0.0192],\n","                      [ 0.0033, -0.0562,  0.0040,  ..., -0.0630, -0.0239, -0.0530],\n","                      [-0.0014, -0.0220,  0.0698,  ...,  0.0503,  0.0503,  0.0292]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.8.self_attn.k_proj.weight',\n","              tensor([[-4.3030e-03,  1.4587e-02, -6.5994e-04,  ..., -2.2705e-02,\n","                       -9.4223e-04, -4.7302e-03],\n","                      [ 8.7280e-03, -2.1484e-02,  7.4158e-03,  ...,  7.3433e-05,\n","                       -6.1340e-03, -7.9956e-03],\n","                      [-2.1118e-02,  1.6357e-02,  3.9062e-03,  ...,  1.1536e-02,\n","                       -1.3550e-02, -8.5449e-03],\n","                      ...,\n","                      [ 6.1035e-02,  2.5879e-02, -7.3730e-02,  ..., -3.2959e-02,\n","                        2.3346e-03, -1.4062e-01],\n","                      [-1.4587e-02, -9.6680e-02,  5.1270e-03,  ..., -7.8613e-02,\n","                        8.6060e-03, -6.2012e-02],\n","                      [-1.6632e-03,  2.8442e-02, -7.8613e-02,  ...,  3.3936e-02,\n","                        4.3457e-02,  2.5391e-02]], dtype=torch.bfloat16)),\n","             ('language_model.model.layers.8.self_attn.v_proj.weight',\n","              tensor([[-0.0081,  0.0022, -0.0089,  ...,  0.0007, -0.0178,  0.0205],\n","                      [ 0.0249, -0.0260,  0.0115,  ...,  0.0078,  0.0364,  0.0222],\n","                      [-0.0160, -0.0071, -0.0200,  ..., -0.0013, -0.0121,  0.0024],\n","                      ...,\n","                      [ 0.0381,  0.0201, -0.0085,  ...,  0.0200, -0.0043, -0.0041],\n","                      [ 0.0015, -0.0063,  0.0073,  ...,  0.0084,  0.0056,  0.0150],\n","                      [ 0.0471,  0.0009,  0.0320,  ..., -0.0205, -0.0149,  0.0398]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.8.self_attn.o_proj.weight',\n","              tensor([[ 0.0131,  0.0058, -0.0248,  ...,  0.0152, -0.0214, -0.0156],\n","                      [ 0.0043, -0.0042, -0.0149,  ..., -0.0139,  0.0186,  0.0289],\n","                      [-0.0099,  0.0049, -0.0046,  ...,  0.0182,  0.0188, -0.0106],\n","                      ...,\n","                      [-0.0371,  0.0059,  0.0206,  ..., -0.0110,  0.0087,  0.0337],\n","                      [-0.0554,  0.0143, -0.0031,  ...,  0.0255, -0.0076,  0.0037],\n","                      [ 0.0209,  0.0410,  0.0132,  ..., -0.0071, -0.0076,  0.0099]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.8.mlp.gate_proj.weight',\n","              tensor([[ 0.0123,  0.0165, -0.0040,  ..., -0.0018, -0.0186, -0.0030],\n","                      [ 0.0184,  0.0247,  0.0002,  ...,  0.0085, -0.0105, -0.0014],\n","                      [-0.0192,  0.0391, -0.0073,  ..., -0.0386,  0.0156, -0.0077],\n","                      ...,\n","                      [ 0.0020, -0.0074,  0.0415,  ...,  0.0242,  0.0151,  0.0033],\n","                      [ 0.0292, -0.0046,  0.0013,  ...,  0.0136,  0.0043,  0.0077],\n","                      [-0.0078,  0.0013, -0.0203,  ..., -0.0123, -0.0391, -0.0201]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.8.mlp.up_proj.weight',\n","              tensor([[ 0.0618,  0.0120, -0.0104,  ..., -0.0615, -0.0072, -0.0430],\n","                      [-0.0364,  0.0359, -0.0195,  ...,  0.0035, -0.0167,  0.0067],\n","                      [ 0.0315, -0.0154,  0.0014,  ..., -0.0447, -0.0026,  0.0215],\n","                      ...,\n","                      [-0.0327, -0.0050,  0.0046,  ..., -0.0058,  0.0117, -0.0361],\n","                      [-0.0159,  0.0359, -0.0182,  ..., -0.0151, -0.0195, -0.0198],\n","                      [ 0.0029, -0.0032, -0.0474,  ..., -0.0051,  0.0364, -0.0327]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.8.mlp.down_proj.weight',\n","              tensor([[ 0.0378, -0.0046,  0.0216,  ...,  0.0107,  0.0033, -0.0059],\n","                      [-0.0123,  0.0330,  0.0006,  ..., -0.0374,  0.0767,  0.0109],\n","                      [-0.0142, -0.0302,  0.0145,  ..., -0.0119, -0.0259, -0.0117],\n","                      ...,\n","                      [-0.0214, -0.0049, -0.0417,  ...,  0.0081, -0.0168, -0.0281],\n","                      [-0.0111, -0.0344, -0.0093,  ...,  0.0211,  0.0176, -0.0036],\n","                      [ 0.0089,  0.0586, -0.0388,  ..., -0.0007,  0.0168, -0.0253]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.8.input_layernorm.weight',\n","              tensor([1.6641, 0.8008, 1.8828,  ..., 1.0312, 2.9062, 0.5273],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.8.post_attention_layernorm.weight',\n","              tensor([2.2812, 0.5977, 2.2969,  ..., 0.9727, 2.9844, 0.6250],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.9.self_attn.q_proj.weight',\n","              tensor([[ 0.0151,  0.0014, -0.0049,  ..., -0.0006, -0.0070,  0.0002],\n","                      [ 0.0178, -0.0115,  0.0059,  ...,  0.0151, -0.0170,  0.0085],\n","                      [-0.0077,  0.0187, -0.0024,  ...,  0.0236, -0.0079,  0.0393],\n","                      ...,\n","                      [ 0.0192,  0.0115, -0.0085,  ...,  0.0214,  0.0133, -0.0255],\n","                      [-0.0171, -0.0179, -0.0171,  ...,  0.0071,  0.0393,  0.0703],\n","                      [-0.0211,  0.0425,  0.0050,  ..., -0.0028,  0.0325,  0.0091]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.9.self_attn.k_proj.weight',\n","              tensor([[ 0.0284,  0.0115,  0.0405,  ...,  0.0083,  0.0144, -0.0074],\n","                      [ 0.0120,  0.0096,  0.0547,  ...,  0.0220,  0.0161, -0.0068],\n","                      [ 0.0084, -0.0243,  0.0236,  ..., -0.0043,  0.0033, -0.0019],\n","                      ...,\n","                      [-0.0194, -0.0081,  0.0635,  ..., -0.0155, -0.0408, -0.0474],\n","                      [ 0.0139,  0.0476, -0.0337,  ..., -0.0005,  0.0162, -0.0287],\n","                      [-0.0128, -0.0126,  0.0469,  ..., -0.0669, -0.0693,  0.0645]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.9.self_attn.v_proj.weight',\n","              tensor([[ 0.0596, -0.0442, -0.0479,  ..., -0.0115,  0.0005,  0.0413],\n","                      [-0.0036,  0.0315,  0.0104,  ..., -0.0273, -0.0238, -0.0087],\n","                      [-0.0554, -0.0142,  0.0168,  ...,  0.0057, -0.0356, -0.0198],\n","                      ...,\n","                      [-0.0092, -0.0040,  0.0032,  ..., -0.0159, -0.0723,  0.0417],\n","                      [ 0.0031, -0.0272,  0.0267,  ..., -0.0011,  0.0227, -0.0192],\n","                      [ 0.0053, -0.0037,  0.0039,  ...,  0.0028, -0.0132,  0.0122]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.9.self_attn.o_proj.weight',\n","              tensor([[ 0.0204, -0.0032, -0.0737,  ...,  0.0029, -0.0084, -0.0012],\n","                      [-0.0359, -0.0047, -0.0432,  ...,  0.0264,  0.0140,  0.0093],\n","                      [ 0.0006,  0.0133,  0.0476,  ...,  0.0093,  0.0036,  0.0552],\n","                      ...,\n","                      [ 0.0003, -0.0270,  0.0090,  ..., -0.0029,  0.0258,  0.0047],\n","                      [-0.0116, -0.0339, -0.0488,  ..., -0.0135,  0.0297,  0.0109],\n","                      [ 0.0013, -0.0129,  0.0096,  ...,  0.0116,  0.0177, -0.0040]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.9.mlp.gate_proj.weight',\n","              tensor([[-0.0178, -0.0247, -0.0162,  ..., -0.0349,  0.0099,  0.0291],\n","                      [ 0.0258,  0.0315,  0.0439,  ..., -0.0618,  0.0135,  0.0051],\n","                      [-0.0035,  0.0092, -0.0091,  ..., -0.0262, -0.0293, -0.0089],\n","                      ...,\n","                      [-0.0002, -0.0043,  0.0371,  ..., -0.0238, -0.0386,  0.0115],\n","                      [ 0.0160, -0.0012, -0.0173,  ...,  0.0359,  0.0060,  0.0332],\n","                      [-0.0024, -0.0286, -0.0081,  ..., -0.0091,  0.0157,  0.0035]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.9.mlp.up_proj.weight',\n","              tensor([[-0.0105,  0.0439, -0.0251,  ...,  0.0281,  0.0366,  0.0160],\n","                      [ 0.0036, -0.0040,  0.0215,  ...,  0.0366,  0.0186, -0.0137],\n","                      [ 0.0054,  0.0131,  0.0330,  ...,  0.0092, -0.0159, -0.0037],\n","                      ...,\n","                      [ 0.0011,  0.0164,  0.0359,  ..., -0.0293,  0.0070, -0.0061],\n","                      [-0.0229,  0.0103,  0.0128,  ...,  0.0261,  0.0102, -0.0022],\n","                      [-0.0073,  0.0288, -0.0166,  ..., -0.0439, -0.0325,  0.0004]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.9.mlp.down_proj.weight',\n","              tensor([[-0.0354,  0.0193, -0.0162,  ...,  0.0408,  0.0067,  0.0239],\n","                      [ 0.0294,  0.0320, -0.0476,  ..., -0.0081, -0.0143,  0.0304],\n","                      [-0.0435,  0.0197,  0.0356,  ..., -0.0035,  0.0114, -0.0369],\n","                      ...,\n","                      [-0.0121, -0.0156, -0.0184,  ..., -0.0220,  0.0193,  0.0192],\n","                      [ 0.0029,  0.0304, -0.0242,  ..., -0.0391,  0.0137, -0.0080],\n","                      [-0.0452,  0.0151, -0.0170,  ..., -0.0059,  0.0557, -0.0449]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.9.input_layernorm.weight',\n","              tensor([1.2812, 0.6055, 1.5000,  ..., 0.4395, 2.5625, 0.0102],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.9.post_attention_layernorm.weight',\n","              tensor([2.1250, 0.5547, 2.1875,  ..., 0.8359, 2.9688, 0.4707],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.10.self_attn.q_proj.weight',\n","              tensor([[-6.8970e-03,  2.8229e-03, -2.5177e-03,  ..., -5.8174e-05,\n","                       -8.2397e-03,  4.4861e-03],\n","                      [-1.1826e-03, -1.3184e-02, -5.6458e-03,  ...,  1.7944e-02,\n","                       -1.2329e-02,  4.4556e-03],\n","                      [-1.2878e-02, -9.9487e-03, -1.1719e-02,  ...,  1.1841e-02,\n","                       -1.9409e-02, -1.1902e-02],\n","                      ...,\n","                      [-1.1963e-02, -3.5156e-02,  4.0527e-02,  ...,  9.5703e-02,\n","                        1.0742e-02, -5.3223e-02],\n","                      [ 1.3062e-02,  4.2480e-02,  3.5553e-03,  ...,  6.0120e-03,\n","                       -2.2339e-02, -3.1494e-02],\n","                      [-3.9864e-04, -1.8188e-02,  4.3335e-03,  ..., -2.3071e-02,\n","                        7.2754e-02,  6.4453e-02]], dtype=torch.bfloat16)),\n","             ('language_model.model.layers.10.self_attn.k_proj.weight',\n","              tensor([[ 0.0198, -0.0013,  0.0378,  ..., -0.0061,  0.0052, -0.0036],\n","                      [ 0.0030,  0.0035,  0.0045,  ..., -0.0250, -0.0007,  0.0087],\n","                      [ 0.0253,  0.0114,  0.0151,  ...,  0.0212,  0.0107, -0.0147],\n","                      ...,\n","                      [-0.0087, -0.1523, -0.0571,  ...,  0.0815, -0.0020, -0.0203],\n","                      [-0.0109,  0.0610,  0.0183,  ..., -0.0339,  0.0042, -0.0513],\n","                      [-0.0300,  0.0295, -0.0035,  ..., -0.0767,  0.0211,  0.0608]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.10.self_attn.v_proj.weight',\n","              tensor([[-0.0121, -0.0125,  0.0052,  ...,  0.0361, -0.0043,  0.0243],\n","                      [ 0.0270,  0.0192,  0.0027,  ..., -0.0134, -0.0128,  0.0046],\n","                      [-0.0034, -0.0177,  0.0212,  ..., -0.0383, -0.0037,  0.0112],\n","                      ...,\n","                      [ 0.0354,  0.0247,  0.0114,  ..., -0.0139,  0.0192,  0.0027],\n","                      [ 0.0469, -0.0498,  0.0320,  ...,  0.0249,  0.0008,  0.0042],\n","                      [ 0.0120,  0.0173, -0.0022,  ...,  0.0146,  0.0151,  0.0106]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.10.self_attn.o_proj.weight',\n","              tensor([[-0.0312, -0.0603,  0.0017,  ...,  0.0083, -0.0166,  0.0187],\n","                      [ 0.0012, -0.0064, -0.0178,  ...,  0.0408, -0.0109,  0.0231],\n","                      [ 0.0082, -0.0391, -0.0110,  ...,  0.0630, -0.0247, -0.0320],\n","                      ...,\n","                      [ 0.0508,  0.0064,  0.0176,  ..., -0.0383,  0.0093,  0.0251],\n","                      [ 0.0344,  0.0005, -0.0334,  ...,  0.0055, -0.0119,  0.0547],\n","                      [-0.0082,  0.0238, -0.0100,  ..., -0.0090, -0.0035,  0.0071]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.10.mlp.gate_proj.weight',\n","              tensor([[-0.0288,  0.0126,  0.0087,  ..., -0.0010, -0.0066, -0.0105],\n","                      [ 0.0240, -0.0076,  0.0108,  ..., -0.0143,  0.0056,  0.0099],\n","                      [ 0.0430, -0.0072, -0.0310,  ..., -0.0092,  0.0126, -0.0309],\n","                      ...,\n","                      [ 0.0133, -0.0033, -0.0025,  ..., -0.0045, -0.0227, -0.0273],\n","                      [ 0.0162, -0.0025,  0.0041,  ..., -0.0044, -0.0378,  0.0354],\n","                      [-0.0171,  0.0215,  0.0281,  ...,  0.0330, -0.0310, -0.0138]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.10.mlp.up_proj.weight',\n","              tensor([[-0.0069, -0.0003, -0.0018,  ..., -0.0197,  0.0131, -0.0056],\n","                      [-0.0205,  0.0294, -0.0166,  ..., -0.0004, -0.0074, -0.0156],\n","                      [-0.0121, -0.0234,  0.0007,  ..., -0.0144, -0.0144,  0.0200],\n","                      ...,\n","                      [ 0.0209, -0.0294,  0.0124,  ...,  0.0564,  0.0145, -0.0374],\n","                      [ 0.0040, -0.0137,  0.0062,  ...,  0.0070,  0.0664,  0.0320],\n","                      [-0.0094,  0.0142, -0.0103,  ..., -0.0123,  0.0146,  0.0184]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.10.mlp.down_proj.weight',\n","              tensor([[-0.0061, -0.0276,  0.0259,  ...,  0.0347,  0.0110,  0.0186],\n","                      [-0.0129,  0.0137, -0.0147,  ..., -0.0085, -0.0036,  0.0119],\n","                      [ 0.0232,  0.0087,  0.0339,  ...,  0.0009,  0.0195, -0.0272],\n","                      ...,\n","                      [ 0.0231, -0.0222, -0.0374,  ...,  0.0080, -0.0054, -0.0053],\n","                      [-0.0128,  0.0117, -0.0151,  ...,  0.0400,  0.0009,  0.0025],\n","                      [-0.0015,  0.0483,  0.0322,  ..., -0.0147,  0.0154,  0.0209]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.10.input_layernorm.weight',\n","              tensor([1.7266, 0.8867, 1.9453,  ..., 0.8203, 3.2188, 0.2500],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.10.post_attention_layernorm.weight',\n","              tensor([2.0781, 0.6680, 2.1250,  ..., 0.9492, 2.6250, 0.5977],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.11.self_attn.q_proj.weight',\n","              tensor([[ 0.0205, -0.0150, -0.0084,  ..., -0.0181,  0.0002, -0.0047],\n","                      [ 0.0049,  0.0098, -0.0104,  ...,  0.0012,  0.0075,  0.0079],\n","                      [ 0.0234,  0.0139,  0.0050,  ..., -0.0053,  0.0135, -0.0123],\n","                      ...,\n","                      [ 0.0173,  0.0082,  0.0145,  ...,  0.0029,  0.0109,  0.0206],\n","                      [-0.0156,  0.0181,  0.0005,  ..., -0.0083,  0.0137, -0.0028],\n","                      [-0.0162, -0.0043,  0.0017,  ...,  0.0098, -0.0092, -0.0145]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.11.self_attn.k_proj.weight',\n","              tensor([[ 4.1962e-04,  3.7003e-04,  6.1035e-03,  ...,  1.4954e-03,\n","                       -4.9438e-03, -8.5449e-04],\n","                      [ 1.7822e-02, -1.1780e-02, -1.4771e-02,  ...,  1.0757e-03,\n","                        7.1716e-03,  2.1973e-03],\n","                      [-7.1716e-04, -4.6997e-03, -7.7820e-03,  ...,  5.8289e-03,\n","                        7.2937e-03,  7.8678e-05],\n","                      ...,\n","                      [-2.9175e-02,  1.3550e-02, -3.3691e-02,  ..., -9.7046e-03,\n","                        3.2471e-02, -2.1973e-02],\n","                      [ 1.0254e-02,  3.0884e-02,  6.4941e-02,  ...,  3.3936e-02,\n","                       -2.0905e-03, -5.7678e-03],\n","                      [-4.0283e-03, -7.9590e-02, -3.9307e-02,  ...,  5.9082e-02,\n","                        9.6130e-04, -1.3306e-02]], dtype=torch.bfloat16)),\n","             ('language_model.model.layers.11.self_attn.v_proj.weight',\n","              tensor([[-0.0452, -0.0256,  0.0219,  ...,  0.0356,  0.0276,  0.0123],\n","                      [ 0.0132,  0.0245,  0.0160,  ...,  0.0306,  0.0376,  0.0091],\n","                      [-0.0106, -0.0116, -0.0101,  ...,  0.0013,  0.0262,  0.0104],\n","                      ...,\n","                      [ 0.0052, -0.0275,  0.0145,  ..., -0.0474,  0.0305, -0.0187],\n","                      [-0.0204, -0.0049,  0.0505,  ..., -0.0098, -0.0195, -0.0231],\n","                      [ 0.0071,  0.0009,  0.0031,  ...,  0.0552,  0.0019, -0.0006]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.11.self_attn.o_proj.weight',\n","              tensor([[-0.0044, -0.0422, -0.0175,  ..., -0.0400, -0.0289, -0.0100],\n","                      [-0.0110, -0.0157, -0.0488,  ...,  0.0081,  0.0119,  0.0311],\n","                      [ 0.0391,  0.0201,  0.0469,  ..., -0.0123,  0.0315,  0.0205],\n","                      ...,\n","                      [ 0.0292,  0.0087,  0.0168,  ..., -0.0091,  0.0096,  0.0330],\n","                      [ 0.0110, -0.0439,  0.0289,  ...,  0.0265, -0.0072, -0.0198],\n","                      [-0.0232, -0.0199, -0.0195,  ..., -0.0432,  0.0053,  0.0396]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.11.mlp.gate_proj.weight',\n","              tensor([[ 0.0264,  0.0034,  0.0119,  ..., -0.0227, -0.0201, -0.0376],\n","                      [ 0.0113,  0.0076, -0.0223,  ..., -0.0168, -0.0291, -0.0306],\n","                      [-0.0009, -0.0593, -0.0095,  ..., -0.0013, -0.0106, -0.0520],\n","                      ...,\n","                      [-0.0283, -0.0077,  0.0057,  ...,  0.0065,  0.0364,  0.0177],\n","                      [-0.0269, -0.0061, -0.0155,  ..., -0.0020, -0.0312, -0.0118],\n","                      [-0.0189,  0.0065, -0.0103,  ..., -0.0121,  0.0132, -0.0115]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.11.mlp.up_proj.weight',\n","              tensor([[ 0.0016, -0.0215, -0.0095,  ..., -0.0289,  0.0116, -0.0084],\n","                      [-0.0354, -0.0023, -0.0374,  ...,  0.0051,  0.0287, -0.0267],\n","                      [ 0.0219,  0.0051, -0.0113,  ...,  0.0204,  0.0223,  0.0581],\n","                      ...,\n","                      [-0.0234,  0.0400, -0.0103,  ...,  0.0425,  0.0008,  0.0273],\n","                      [ 0.0203, -0.0439, -0.0231,  ...,  0.0201,  0.0188, -0.0308],\n","                      [-0.0168,  0.0064,  0.0054,  ..., -0.0139,  0.0010,  0.0068]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.11.mlp.down_proj.weight',\n","              tensor([[ 0.0162, -0.0073, -0.0033,  ..., -0.0327,  0.0576,  0.0267],\n","                      [-0.0043,  0.0327, -0.0170,  ..., -0.0510,  0.0131, -0.0186],\n","                      [-0.0140, -0.0081,  0.0162,  ...,  0.0164,  0.0713, -0.0059],\n","                      ...,\n","                      [ 0.0044,  0.0041,  0.0031,  ...,  0.0145,  0.0172,  0.0107],\n","                      [ 0.0222,  0.0312, -0.0229,  ...,  0.0420, -0.0051,  0.0132],\n","                      [-0.0030, -0.0237,  0.0046,  ...,  0.0262,  0.0201, -0.0024]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.11.input_layernorm.weight',\n","              tensor([1.6250, 1.2578, 1.9922,  ..., 1.0469, 2.2188, 0.5117],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.11.post_attention_layernorm.weight',\n","              tensor([1.8828, 0.7188, 1.9219,  ..., 1.0234, 2.2500, 0.7422],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.12.self_attn.q_proj.weight',\n","              tensor([[ 0.0110,  0.0020,  0.0005,  ...,  0.0132,  0.0267, -0.0132],\n","                      [ 0.0102,  0.0403,  0.0417,  ..., -0.0023, -0.0352,  0.0204],\n","                      [-0.0508,  0.0020,  0.0393,  ..., -0.0082,  0.0198,  0.0226],\n","                      ...,\n","                      [-0.0500, -0.0522,  0.0005,  ..., -0.0178, -0.0220,  0.0214],\n","                      [ 0.0045, -0.0019, -0.0064,  ...,  0.0020,  0.0889,  0.0255],\n","                      [-0.0332, -0.0033,  0.0422,  ...,  0.0154, -0.1147, -0.0057]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.12.self_attn.k_proj.weight',\n","              tensor([[ 1.3367e-02,  1.4038e-02,  5.8289e-03,  ..., -1.2390e-02,\n","                        1.8433e-02, -1.0193e-02],\n","                      [ 9.8267e-03, -5.7678e-03,  7.5378e-03,  ..., -5.0354e-03,\n","                       -8.7357e-04,  4.8523e-03],\n","                      [-5.5237e-03,  3.4027e-03,  5.9814e-03,  ..., -1.5259e-02,\n","                        7.8201e-05, -2.2583e-03],\n","                      ...,\n","                      [ 6.9885e-03,  4.7913e-03,  4.8584e-02,  ..., -2.2125e-03,\n","                       -1.0864e-02, -1.2573e-02],\n","                      [ 3.4668e-02,  1.6113e-02, -1.2024e-02,  ...,  4.8340e-02,\n","                        1.0925e-02, -4.1016e-02],\n","                      [ 4.4861e-03, -2.6489e-02, -1.4191e-03,  ..., -1.3550e-02,\n","                       -3.6774e-03,  1.6327e-03]], dtype=torch.bfloat16)),\n","             ('language_model.model.layers.12.self_attn.v_proj.weight',\n","              tensor([[ 0.0334, -0.0527, -0.0232,  ...,  0.0703,  0.0075, -0.0208],\n","                      [-0.0106, -0.0056, -0.0461,  ...,  0.0236, -0.0117,  0.0065],\n","                      [-0.0160, -0.0233, -0.0603,  ..., -0.0255,  0.0369, -0.0352],\n","                      ...,\n","                      [-0.0034,  0.0068, -0.0154,  ...,  0.0267,  0.0264, -0.0027],\n","                      [-0.0530,  0.0160, -0.0074,  ...,  0.0070, -0.0425, -0.0649],\n","                      [ 0.0034, -0.0270,  0.0388,  ...,  0.0287,  0.0415, -0.0425]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.12.self_attn.o_proj.weight',\n","              tensor([[-0.0097,  0.0123,  0.0177,  ..., -0.0002, -0.0198, -0.0066],\n","                      [ 0.0315,  0.0238,  0.0277,  ..., -0.0022,  0.0086,  0.0148],\n","                      [ 0.0249,  0.0645,  0.0398,  ...,  0.0138,  0.0070,  0.0208],\n","                      ...,\n","                      [-0.0830, -0.0170,  0.0270,  ...,  0.0245,  0.0048,  0.0143],\n","                      [-0.0049,  0.0265, -0.0291,  ...,  0.0030, -0.0121,  0.0150],\n","                      [ 0.0267, -0.0126,  0.0349,  ..., -0.0220, -0.0344,  0.0046]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.12.mlp.gate_proj.weight',\n","              tensor([[-0.0223, -0.0137,  0.0135,  ...,  0.0157, -0.0022, -0.0576],\n","                      [ 0.0020,  0.0182,  0.0227,  ..., -0.0474,  0.0140, -0.0010],\n","                      [ 0.0449, -0.0095,  0.0189,  ...,  0.0076, -0.0034, -0.0243],\n","                      ...,\n","                      [ 0.0135, -0.0058,  0.0140,  ..., -0.0211, -0.0154, -0.0102],\n","                      [-0.0240, -0.0229, -0.0282,  ...,  0.0162, -0.0233, -0.0010],\n","                      [ 0.0112,  0.0157,  0.0027,  ...,  0.0129, -0.0020,  0.0136]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.12.mlp.up_proj.weight',\n","              tensor([[-0.0293, -0.0150,  0.0031,  ..., -0.0157, -0.0014, -0.0022],\n","                      [-0.0081,  0.0066,  0.0209,  ...,  0.0089, -0.0898, -0.0036],\n","                      [-0.0092,  0.0194, -0.0084,  ..., -0.0135,  0.0115,  0.0347],\n","                      ...,\n","                      [-0.0109, -0.0030,  0.0013,  ...,  0.0056, -0.0014,  0.0154],\n","                      [ 0.0159, -0.0077, -0.0105,  ..., -0.0271,  0.0190, -0.0208],\n","                      [ 0.0020,  0.0094,  0.0028,  ..., -0.0233,  0.0264, -0.0164]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.12.mlp.down_proj.weight',\n","              tensor([[-0.0019,  0.0508,  0.0371,  ...,  0.0469, -0.0036,  0.0004],\n","                      [-0.0173, -0.0053, -0.0067,  ...,  0.0552,  0.0233, -0.0195],\n","                      [ 0.0168,  0.0206, -0.0121,  ..., -0.0215,  0.0092,  0.0243],\n","                      ...,\n","                      [-0.0069,  0.0292,  0.0454,  ..., -0.0342, -0.0112, -0.0069],\n","                      [-0.0496, -0.0131, -0.0006,  ..., -0.0087, -0.0087, -0.0145],\n","                      [ 0.0108,  0.0084, -0.0386,  ..., -0.0134,  0.0070,  0.0232]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.12.input_layernorm.weight',\n","              tensor([1.5547, 1.1328, 1.9297,  ..., 1.4531, 2.2500, 1.1719],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.12.post_attention_layernorm.weight',\n","              tensor([1.8281, 0.8789, 1.8203,  ..., 1.2656, 2.0000, 1.0781],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.13.self_attn.q_proj.weight',\n","              tensor([[-0.0107, -0.0544, -0.0115,  ..., -0.0160, -0.0166,  0.0114],\n","                      [ 0.0070,  0.0075, -0.0184,  ..., -0.0267, -0.0070,  0.0227],\n","                      [-0.0074, -0.0679, -0.0199,  ...,  0.0002, -0.0186,  0.0165],\n","                      ...,\n","                      [-0.0198,  0.0058,  0.0153,  ..., -0.0212,  0.0016, -0.0168],\n","                      [ 0.0101, -0.0057, -0.0099,  ..., -0.0101,  0.0057,  0.0203],\n","                      [-0.0256, -0.0087, -0.0096,  ..., -0.0019, -0.0098,  0.0023]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.13.self_attn.k_proj.weight',\n","              tensor([[ 7.8964e-04,  1.0986e-02,  1.7242e-03,  ..., -1.9775e-02,\n","                        1.0803e-02,  1.1368e-03],\n","                      [-8.9169e-05,  2.4567e-03, -1.8677e-02,  ...,  2.0508e-02,\n","                        1.7578e-02,  1.5198e-02],\n","                      [-4.0817e-04,  1.3428e-02, -3.8910e-03,  ...,  1.1780e-02,\n","                        7.3612e-06,  1.8677e-02],\n","                      ...,\n","                      [-1.4343e-02, -3.1738e-02,  4.3945e-03,  ...,  4.7913e-03,\n","                       -1.1139e-03, -1.9409e-02],\n","                      [ 2.1210e-03, -1.4465e-02,  6.4468e-04,  ..., -2.6611e-02,\n","                       -8.4229e-03, -4.0771e-02],\n","                      [ 9.7046e-03, -2.4292e-02, -3.1250e-02,  ...,  3.4912e-02,\n","                        1.0834e-03, -1.6235e-02]], dtype=torch.bfloat16)),\n","             ('language_model.model.layers.13.self_attn.v_proj.weight',\n","              tensor([[-0.0277,  0.0466, -0.0060,  ..., -0.0425,  0.0588, -0.0004],\n","                      [-0.0125,  0.0471,  0.0261,  ...,  0.0245, -0.0317,  0.0048],\n","                      [-0.0219, -0.0002, -0.0325,  ..., -0.0311,  0.0150,  0.0928],\n","                      ...,\n","                      [ 0.0135,  0.0342,  0.0086,  ..., -0.0030,  0.0212,  0.0815],\n","                      [ 0.0398,  0.0393, -0.0146,  ...,  0.0104,  0.0064, -0.0547],\n","                      [ 0.0204,  0.0109,  0.0010,  ...,  0.0352,  0.0386, -0.0330]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.13.self_attn.o_proj.weight',\n","              tensor([[-0.0265, -0.0133,  0.0309,  ..., -0.0320, -0.0140,  0.0393],\n","                      [ 0.0508,  0.0228, -0.0106,  ...,  0.0077, -0.0187,  0.0229],\n","                      [-0.0026, -0.0059, -0.0359,  ...,  0.0054, -0.0085, -0.0275],\n","                      ...,\n","                      [ 0.0095,  0.0048, -0.0269,  ...,  0.0130, -0.0082, -0.0713],\n","                      [ 0.0006, -0.0425, -0.0114,  ..., -0.0072, -0.0092,  0.0133],\n","                      [-0.0216, -0.0109,  0.0183,  ..., -0.0159,  0.0035, -0.0615]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.13.mlp.gate_proj.weight',\n","              tensor([[-0.0255,  0.0087,  0.0258,  ..., -0.0014,  0.0106,  0.0161],\n","                      [ 0.0344, -0.0293, -0.0115,  ..., -0.0422,  0.0223,  0.0623],\n","                      [-0.0269,  0.0371, -0.0121,  ..., -0.0212,  0.0076, -0.0038],\n","                      ...,\n","                      [ 0.0226, -0.0068, -0.0327,  ..., -0.0221,  0.0042,  0.0564],\n","                      [ 0.0132,  0.0045, -0.0334,  ..., -0.0065, -0.0002, -0.0194],\n","                      [ 0.0591, -0.0027, -0.0287,  ...,  0.0068, -0.0154,  0.0293]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.13.mlp.up_proj.weight',\n","              tensor([[ 0.0047,  0.0043,  0.0101,  ..., -0.0087,  0.0089, -0.0095],\n","                      [ 0.0308, -0.0052, -0.0737,  ...,  0.0254, -0.0498, -0.0238],\n","                      [-0.0190, -0.0277,  0.0032,  ...,  0.0244,  0.0347, -0.0270],\n","                      ...,\n","                      [-0.0157, -0.0084, -0.0289,  ...,  0.0184, -0.0283, -0.0742],\n","                      [ 0.0072, -0.0140, -0.0151,  ..., -0.0227, -0.0183, -0.0200],\n","                      [-0.0262, -0.0153, -0.0471,  ...,  0.0115, -0.0256,  0.0222]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.13.mlp.down_proj.weight',\n","              tensor([[-0.0522, -0.0214,  0.0408,  ...,  0.0449,  0.0369,  0.0146],\n","                      [-0.0016, -0.0349, -0.0227,  ...,  0.0159, -0.0201,  0.0378],\n","                      [ 0.0074,  0.0008,  0.0359,  ..., -0.0103, -0.0022, -0.0214],\n","                      ...,\n","                      [ 0.0388, -0.0014,  0.0312,  ..., -0.0141, -0.0060,  0.0422],\n","                      [-0.0036, -0.0129,  0.0167,  ..., -0.0089, -0.0050,  0.0217],\n","                      [ 0.0189,  0.0164,  0.0054,  ...,  0.0654, -0.0100, -0.0311]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.13.input_layernorm.weight',\n","              tensor([1.0938, 1.2578, 1.1094,  ..., 1.1016, 1.3906, 0.6992],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.13.post_attention_layernorm.weight',\n","              tensor([1.8281, 1.1250, 1.8438,  ..., 1.4609, 2.0469, 1.3281],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.14.self_attn.q_proj.weight',\n","              tensor([[ 0.0167,  0.0023, -0.0172,  ..., -0.0005,  0.0029, -0.0100],\n","                      [-0.0074,  0.0049,  0.0132,  ...,  0.0007, -0.0077, -0.0001],\n","                      [ 0.0136,  0.0026,  0.0045,  ..., -0.0251,  0.0126, -0.0004],\n","                      ...,\n","                      [ 0.0208, -0.0569, -0.0074,  ..., -0.0095,  0.0008,  0.0023],\n","                      [ 0.0267, -0.0302, -0.0051,  ..., -0.0001, -0.0586,  0.0048],\n","                      [ 0.0349,  0.0248,  0.0172,  ..., -0.0134, -0.0048, -0.0193]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.14.self_attn.k_proj.weight',\n","              tensor([[ 0.0115, -0.0142,  0.0049,  ..., -0.0062, -0.0107,  0.0051],\n","                      [ 0.0149,  0.0156,  0.0087,  ..., -0.0266,  0.0058, -0.0125],\n","                      [-0.0176,  0.0013,  0.0020,  ...,  0.0099,  0.0004,  0.0065],\n","                      ...,\n","                      [ 0.0060, -0.0039, -0.0564,  ...,  0.0038, -0.0217,  0.0068],\n","                      [-0.0092, -0.0391,  0.0410,  ...,  0.0244,  0.0439,  0.0135],\n","                      [-0.0228,  0.0718, -0.0703,  ..., -0.0549, -0.0342,  0.0327]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.14.self_attn.v_proj.weight',\n","              tensor([[-0.0281,  0.0044,  0.0140,  ..., -0.0217, -0.0273, -0.0030],\n","                      [-0.0111, -0.0066, -0.0227,  ..., -0.0214, -0.0396, -0.0181],\n","                      [-0.0089, -0.0140,  0.0500,  ..., -0.0037,  0.0221,  0.0282],\n","                      ...,\n","                      [ 0.0337, -0.0056,  0.0171,  ...,  0.0154,  0.0048, -0.0493],\n","                      [ 0.0325,  0.0007,  0.0286,  ..., -0.0262,  0.0239,  0.0090],\n","                      [-0.0117,  0.0087,  0.0071,  ..., -0.0354,  0.0009,  0.0349]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.14.self_attn.o_proj.weight',\n","              tensor([[-0.0447, -0.0264, -0.0280,  ..., -0.0084,  0.0075,  0.0022],\n","                      [-0.0019, -0.0086,  0.0247,  ...,  0.0400, -0.0056, -0.0289],\n","                      [-0.0129,  0.0097,  0.0398,  ...,  0.0091, -0.0159, -0.0023],\n","                      ...,\n","                      [-0.0339,  0.0033,  0.0088,  ...,  0.0311,  0.0437,  0.0164],\n","                      [-0.0352, -0.0039,  0.0077,  ...,  0.0212, -0.0115, -0.0618],\n","                      [-0.0201, -0.0127,  0.0124,  ...,  0.0064,  0.0337,  0.0200]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.14.mlp.gate_proj.weight',\n","              tensor([[ 0.0113, -0.0113, -0.0029,  ..., -0.0133, -0.0219,  0.0126],\n","                      [-0.0222,  0.0305,  0.0471,  ...,  0.0117, -0.0075, -0.0115],\n","                      [-0.0105, -0.0206,  0.0005,  ..., -0.0042,  0.0226,  0.0098],\n","                      ...,\n","                      [ 0.0070,  0.0095, -0.0139,  ...,  0.0200, -0.0121, -0.0189],\n","                      [ 0.0167, -0.0104,  0.0054,  ...,  0.0189,  0.0088,  0.0157],\n","                      [-0.0271, -0.0197, -0.0025,  ..., -0.0171, -0.0006, -0.0214]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.14.mlp.up_proj.weight',\n","              tensor([[ 0.0197, -0.0046,  0.0096,  ..., -0.0303, -0.0271, -0.0038],\n","                      [ 0.0059, -0.0266,  0.0248,  ..., -0.0076, -0.0466, -0.0135],\n","                      [ 0.0027, -0.0325, -0.0149,  ..., -0.0442,  0.0161, -0.0327],\n","                      ...,\n","                      [-0.0106, -0.0014,  0.0205,  ..., -0.0157, -0.0176, -0.0153],\n","                      [ 0.0291, -0.0012,  0.0237,  ..., -0.0001,  0.0012,  0.0165],\n","                      [-0.0167,  0.0295, -0.0289,  ..., -0.0481, -0.0117,  0.0137]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.14.mlp.down_proj.weight',\n","              tensor([[-0.0091,  0.0427,  0.0280,  ...,  0.0386, -0.0038, -0.0129],\n","                      [ 0.0107, -0.0388,  0.0018,  ..., -0.0332, -0.0425,  0.0140],\n","                      [-0.0615,  0.0381, -0.0073,  ..., -0.0109, -0.0327,  0.0117],\n","                      ...,\n","                      [ 0.0259, -0.0282,  0.0245,  ..., -0.0212, -0.0002,  0.0197],\n","                      [ 0.0059,  0.0223, -0.0275,  ...,  0.0117, -0.0215,  0.0272],\n","                      [-0.0194,  0.0237, -0.0347,  ...,  0.0194, -0.0505,  0.0053]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.14.input_layernorm.weight',\n","              tensor([0.5430, 1.2266, 0.9375,  ..., 0.6172, 0.9414, 0.6328],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.14.post_attention_layernorm.weight',\n","              tensor([1.9062, 1.3047, 1.8125,  ..., 1.7109, 1.9531, 1.6875],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.15.self_attn.q_proj.weight',\n","              tensor([[-0.0033, -0.0007, -0.0030,  ..., -0.0208,  0.0022, -0.0026],\n","                      [ 0.0065,  0.0099, -0.0020,  ...,  0.0028, -0.0097,  0.0052],\n","                      [ 0.0038, -0.0004, -0.0065,  ..., -0.0059,  0.0212,  0.0013],\n","                      ...,\n","                      [ 0.0106, -0.0143,  0.0227,  ..., -0.0204, -0.0239, -0.0007],\n","                      [ 0.0037, -0.0195,  0.0243,  ...,  0.0137, -0.0267,  0.0102],\n","                      [ 0.0028,  0.0398,  0.0160,  ..., -0.0004, -0.0090, -0.0168]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.15.self_attn.k_proj.weight',\n","              tensor([[-0.0061,  0.0100,  0.0109,  ..., -0.0056, -0.0069, -0.0039],\n","                      [-0.0142, -0.0057, -0.0021,  ...,  0.0061, -0.0072,  0.0006],\n","                      [-0.0015,  0.0098, -0.0047,  ..., -0.0048,  0.0066, -0.0017],\n","                      ...,\n","                      [-0.0045,  0.0425,  0.0491,  ..., -0.0547,  0.0005,  0.0220],\n","                      [-0.0245,  0.0417, -0.0192,  ..., -0.0277,  0.0051,  0.0104],\n","                      [-0.0106,  0.0172,  0.0248,  ..., -0.0527,  0.0067,  0.0021]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.15.self_attn.v_proj.weight',\n","              tensor([[-8.4473e-02,  2.0508e-02,  1.4844e-01,  ..., -2.0752e-02,\n","                       -5.2979e-02,  1.0791e-01],\n","                      [-5.2002e-02, -9.9945e-04,  4.4189e-02,  ..., -2.8076e-02,\n","                        5.7617e-02, -7.7148e-02],\n","                      [ 5.1025e-02, -4.9744e-03, -2.1729e-02,  ..., -9.4727e-02,\n","                        4.4922e-02,  1.7944e-02],\n","                      ...,\n","                      [-1.0645e-01, -1.4832e-02,  7.2002e-05,  ..., -9.4604e-03,\n","                       -3.5156e-02, -5.1575e-03],\n","                      [ 7.2754e-02,  1.0803e-02,  3.0029e-02,  ..., -2.0386e-02,\n","                       -4.6387e-02, -9.5215e-02],\n","                      [-6.5918e-02, -4.1992e-02, -4.5166e-02,  ..., -8.0566e-02,\n","                        1.4709e-02,  1.9043e-02]], dtype=torch.bfloat16)),\n","             ('language_model.model.layers.15.self_attn.o_proj.weight',\n","              tensor([[-0.0179,  0.0476,  0.0064,  ..., -0.0337,  0.0272, -0.0131],\n","                      [-0.0014,  0.0120,  0.0371,  ..., -0.0306,  0.0038,  0.0171],\n","                      [-0.0001, -0.0150, -0.0016,  ..., -0.0030,  0.0220, -0.0352],\n","                      ...,\n","                      [-0.0232, -0.0074, -0.0058,  ...,  0.0170, -0.0148, -0.0115],\n","                      [-0.0391, -0.0178, -0.0179,  ..., -0.0223, -0.0215,  0.0109],\n","                      [ 0.0170, -0.0110, -0.0024,  ..., -0.0069, -0.0312,  0.0151]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.15.mlp.gate_proj.weight',\n","              tensor([[-0.0051,  0.0075,  0.0103,  ..., -0.0229, -0.0038,  0.0085],\n","                      [ 0.0284, -0.0118,  0.0369,  ...,  0.0126, -0.0155, -0.0334],\n","                      [-0.0364,  0.0292, -0.0065,  ..., -0.0146,  0.0049, -0.0354],\n","                      ...,\n","                      [ 0.0128,  0.0178,  0.0007,  ...,  0.0291, -0.0117,  0.0352],\n","                      [-0.0123, -0.0109,  0.0325,  ...,  0.0122,  0.0299, -0.0293],\n","                      [ 0.0013, -0.0145,  0.0006,  ..., -0.0009,  0.0107, -0.0027]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.15.mlp.up_proj.weight',\n","              tensor([[-0.0084, -0.0286,  0.0019,  ..., -0.0047,  0.0311, -0.0253],\n","                      [-0.0396,  0.0635,  0.0162,  ..., -0.0010,  0.0280,  0.0110],\n","                      [ 0.0123,  0.0189, -0.0053,  ..., -0.0118, -0.0065,  0.0352],\n","                      ...,\n","                      [-0.0381, -0.0049, -0.0146,  ...,  0.0383, -0.0723, -0.0289],\n","                      [-0.0045, -0.0190,  0.0166,  ..., -0.0008,  0.0227, -0.0209],\n","                      [ 0.0057,  0.0203,  0.0276,  ...,  0.0115, -0.0193, -0.0125]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.15.mlp.down_proj.weight',\n","              tensor([[ 0.0114, -0.0359, -0.0088,  ..., -0.0119,  0.0493,  0.0034],\n","                      [ 0.0342,  0.0199,  0.0173,  ...,  0.0197, -0.0210, -0.0334],\n","                      [-0.0070, -0.0148, -0.0184,  ...,  0.0391,  0.0449, -0.0289],\n","                      ...,\n","                      [-0.0017,  0.0052, -0.0215,  ..., -0.0156, -0.0181, -0.0210],\n","                      [-0.0142,  0.0115,  0.0027,  ...,  0.0349,  0.0043,  0.0040],\n","                      [-0.0447,  0.0439, -0.0615,  ..., -0.0151,  0.0094,  0.0354]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.15.input_layernorm.weight',\n","              tensor([0.6289, 1.6641, 0.8906,  ..., 0.7891, 0.8867, 1.4844],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.15.post_attention_layernorm.weight',\n","              tensor([1.8906, 1.5156, 1.9375,  ..., 1.8203, 1.9531, 1.8359],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.16.self_attn.q_proj.weight',\n","              tensor([[-0.0425,  0.0227,  0.0148,  ...,  0.0063, -0.0112,  0.0161],\n","                      [-0.0049,  0.0063,  0.0092,  ..., -0.0232, -0.0003, -0.0110],\n","                      [ 0.0006, -0.0104,  0.0107,  ..., -0.0161, -0.0046, -0.0170],\n","                      ...,\n","                      [-0.0104, -0.0255,  0.0234,  ..., -0.0053, -0.0036,  0.0040],\n","                      [-0.0315, -0.0425,  0.0347,  ..., -0.0078,  0.0076,  0.0669],\n","                      [-0.0085, -0.0405,  0.0198,  ..., -0.0265, -0.0038,  0.0261]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.16.self_attn.k_proj.weight',\n","              tensor([[ 0.0124, -0.0214,  0.0032,  ...,  0.0107, -0.0098, -0.0200],\n","                      [ 0.0051,  0.0053, -0.0092,  ..., -0.0175,  0.0022, -0.0147],\n","                      [ 0.0087,  0.0060,  0.0013,  ...,  0.0009,  0.0009, -0.0055],\n","                      ...,\n","                      [-0.0145,  0.0120, -0.0096,  ...,  0.0291,  0.0089,  0.0101],\n","                      [ 0.0217, -0.0483,  0.0002,  ..., -0.0082,  0.0012,  0.0272],\n","                      [ 0.0149,  0.0386, -0.0272,  ...,  0.0265,  0.0110, -0.0065]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.16.self_attn.v_proj.weight',\n","              tensor([[ 0.0510,  0.0544, -0.0361,  ..., -0.0327,  0.0060, -0.0060],\n","                      [ 0.0598,  0.0066, -0.0356,  ..., -0.0093, -0.0737,  0.0095],\n","                      [ 0.0133,  0.0967, -0.0557,  ...,  0.0249,  0.0332,  0.0039],\n","                      ...,\n","                      [-0.0342,  0.0134,  0.0037,  ...,  0.0781, -0.0046, -0.0559],\n","                      [ 0.0215,  0.0679, -0.0112,  ..., -0.0102, -0.0054, -0.0515],\n","                      [-0.0581,  0.0054,  0.0610,  ..., -0.0310,  0.0082,  0.0579]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.16.self_attn.o_proj.weight',\n","              tensor([[ 0.0150,  0.0063, -0.0049,  ..., -0.0199, -0.0012, -0.0386],\n","                      [ 0.0261,  0.0309,  0.0476,  ...,  0.0041, -0.0040,  0.0302],\n","                      [-0.0154, -0.0150, -0.0137,  ...,  0.0405,  0.0287,  0.0356],\n","                      ...,\n","                      [-0.0187,  0.0045,  0.0109,  ...,  0.0188, -0.0125,  0.0192],\n","                      [ 0.0156, -0.0092,  0.0352,  ..., -0.0037, -0.0056,  0.0204],\n","                      [-0.0170,  0.0281, -0.0286,  ..., -0.0043, -0.0193, -0.0005]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.16.mlp.gate_proj.weight',\n","              tensor([[ 0.0124,  0.0303, -0.0078,  ...,  0.0157, -0.0053,  0.0060],\n","                      [-0.0050, -0.0039, -0.0171,  ..., -0.0031, -0.0076, -0.0027],\n","                      [-0.0077, -0.0059,  0.0057,  ...,  0.0231, -0.0062, -0.0139],\n","                      ...,\n","                      [ 0.0016,  0.0090,  0.0154,  ..., -0.0298,  0.0256,  0.0139],\n","                      [ 0.0052, -0.0006, -0.0073,  ..., -0.0080, -0.0070,  0.0198],\n","                      [ 0.0162,  0.0228,  0.0002,  ...,  0.0289,  0.0084, -0.0113]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.16.mlp.up_proj.weight',\n","              tensor([[-4.0894e-03,  2.0264e-02,  3.3447e-02,  ...,  3.4180e-02,\n","                       -2.7710e-02, -4.5410e-02],\n","                      [ 8.1539e-05,  6.9275e-03,  3.7537e-03,  ...,  3.6621e-02,\n","                       -3.7598e-02,  3.2715e-02],\n","                      [ 6.1340e-03,  4.9591e-04,  1.5259e-02,  ..., -1.5488e-03,\n","                        1.9165e-02, -3.8818e-02],\n","                      ...,\n","                      [ 5.4688e-02, -1.5442e-02, -4.1992e-02,  ..., -8.3618e-03,\n","                        9.8267e-03, -2.2339e-02],\n","                      [-4.0771e-02,  2.1851e-02, -1.3306e-02,  ...,  8.4305e-04,\n","                        2.9182e-04, -1.3611e-02],\n","                      [-2.9663e-02, -2.7954e-02, -1.2085e-02,  ...,  1.5625e-02,\n","                        2.4902e-02,  9.9182e-04]], dtype=torch.bfloat16)),\n","             ('language_model.model.layers.16.mlp.down_proj.weight',\n","              tensor([[-0.0209,  0.0141,  0.0305,  ..., -0.0106,  0.0130, -0.0128],\n","                      [ 0.0317, -0.0386, -0.0146,  ..., -0.0054, -0.0339,  0.0036],\n","                      [-0.0688, -0.0079, -0.0090,  ..., -0.0027,  0.0179, -0.0242],\n","                      ...,\n","                      [ 0.0126, -0.0359,  0.0175,  ...,  0.0089, -0.0033,  0.0194],\n","                      [-0.0013, -0.0176,  0.0334,  ..., -0.0332, -0.0075, -0.0304],\n","                      [ 0.0337, -0.0082,  0.0038,  ...,  0.0059,  0.0051,  0.0052]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.16.input_layernorm.weight',\n","              tensor([0.7461, 1.5234, 1.1406,  ..., 0.7656, 1.2969, 0.9297],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.16.post_attention_layernorm.weight',\n","              tensor([1.8359, 1.7500, 1.9844,  ..., 1.9297, 1.9609, 1.9062],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.17.self_attn.q_proj.weight',\n","              tensor([[-0.0009, -0.0131, -0.0107,  ...,  0.0123, -0.0051,  0.0031],\n","                      [-0.0549,  0.0131,  0.0466,  ...,  0.0261, -0.0002,  0.0378],\n","                      [ 0.0281,  0.0050, -0.0093,  ...,  0.0195, -0.0079,  0.0128],\n","                      ...,\n","                      [ 0.0153, -0.0099, -0.0051,  ..., -0.0041, -0.0277, -0.0058],\n","                      [-0.0032,  0.0095,  0.0105,  ...,  0.0092, -0.0217,  0.0054],\n","                      [ 0.0099, -0.0134,  0.0009,  ...,  0.0040,  0.0114,  0.0146]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.17.self_attn.k_proj.weight',\n","              tensor([[-1.9043e-02, -3.5553e-03,  6.1951e-03,  ...,  6.5002e-03,\n","                        1.2436e-03,  1.4038e-02],\n","                      [-1.4954e-02, -1.6846e-02, -1.6357e-02,  ..., -1.5640e-04,\n","                       -5.9891e-04, -7.9346e-03],\n","                      [-5.3024e-04, -4.9973e-04,  8.6060e-03,  ...,  1.6235e-02,\n","                       -6.4392e-03, -5.9814e-03],\n","                      ...,\n","                      [-7.5073e-03, -3.9368e-03,  7.6294e-03,  ...,  2.7084e-04,\n","                       -8.0566e-03, -1.3550e-02],\n","                      [-7.8735e-03, -1.3611e-02, -1.5869e-02,  ...,  5.5552e-05,\n","                       -5.7678e-03, -3.8910e-03],\n","                      [ 5.1117e-04, -5.1880e-03, -2.3560e-02,  ..., -5.8594e-03,\n","                       -5.3101e-03,  2.6978e-02]], dtype=torch.bfloat16)),\n","             ('language_model.model.layers.17.self_attn.v_proj.weight',\n","              tensor([[ 0.0018, -0.0048, -0.0947,  ..., -0.0786, -0.0630,  0.0092],\n","                      [-0.0311,  0.0342,  0.0254,  ..., -0.0330, -0.0332, -0.0025],\n","                      [-0.0339,  0.0095, -0.0081,  ..., -0.0063, -0.0325,  0.0215],\n","                      ...,\n","                      [ 0.0121, -0.0327,  0.0145,  ...,  0.0146, -0.0160, -0.0486],\n","                      [-0.0159,  0.0193,  0.0165,  ..., -0.0879, -0.0645, -0.0232],\n","                      [ 0.0206,  0.0245, -0.0322,  ..., -0.0035,  0.0277, -0.0488]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.17.self_attn.o_proj.weight',\n","              tensor([[ 0.0315, -0.0011, -0.0272,  ...,  0.0095,  0.0115, -0.0039],\n","                      [ 0.0386,  0.0125,  0.0640,  ...,  0.0298, -0.0225, -0.0077],\n","                      [-0.0051, -0.0139,  0.0133,  ...,  0.0432, -0.0271,  0.0505],\n","                      ...,\n","                      [-0.0315, -0.0187, -0.0134,  ...,  0.0052,  0.0311,  0.0291],\n","                      [-0.0144, -0.0123, -0.0605,  ...,  0.0325,  0.0299, -0.0047],\n","                      [-0.0114, -0.0198,  0.0216,  ...,  0.0674,  0.0098,  0.0347]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.17.mlp.gate_proj.weight',\n","              tensor([[-0.0118, -0.0243,  0.0137,  ..., -0.0014,  0.0035, -0.0038],\n","                      [-0.0121, -0.0067, -0.0253,  ...,  0.0004, -0.0134,  0.0048],\n","                      [-0.0251,  0.0288,  0.0239,  ...,  0.0005, -0.0159,  0.0041],\n","                      ...,\n","                      [ 0.0115,  0.0005, -0.0011,  ..., -0.0041, -0.0099, -0.0040],\n","                      [ 0.0469, -0.0077,  0.0177,  ...,  0.0007, -0.0146,  0.0210],\n","                      [ 0.0259,  0.0258,  0.0223,  ...,  0.0204,  0.0129,  0.0219]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.17.mlp.up_proj.weight',\n","              tensor([[-2.0142e-02, -5.9509e-03,  2.5391e-02,  ..., -7.9346e-03,\n","                        3.2227e-02, -8.0109e-05],\n","                      [-2.3682e-02, -2.3560e-02,  6.3477e-03,  ...,  5.1514e-02,\n","                        3.0823e-03,  2.6367e-02],\n","                      [-7.7820e-03, -4.4678e-02, -2.0020e-02,  ..., -9.2163e-03,\n","                       -1.5991e-02, -6.5308e-03],\n","                      ...,\n","                      [-3.1738e-02,  6.8970e-03, -3.3417e-03,  ...,  1.6235e-02,\n","                        6.7749e-03, -4.9133e-03],\n","                      [ 1.5625e-02, -3.9978e-03, -2.8076e-02,  ..., -2.3560e-02,\n","                       -5.6152e-03, -1.9775e-02],\n","                      [-1.0376e-02, -7.0190e-03, -3.4668e-02,  ...,  1.2512e-02,\n","                       -6.2561e-04, -4.4678e-02]], dtype=torch.bfloat16)),\n","             ('language_model.model.layers.17.mlp.down_proj.weight',\n","              tensor([[-0.0041, -0.0079,  0.0105,  ..., -0.0023,  0.0226, -0.0225],\n","                      [ 0.0141,  0.0040,  0.0255,  ...,  0.0085,  0.0137,  0.0042],\n","                      [ 0.0089,  0.0237, -0.0054,  ..., -0.0040, -0.0101, -0.0417],\n","                      ...,\n","                      [-0.0439,  0.0238,  0.0008,  ..., -0.0187,  0.0031,  0.0058],\n","                      [-0.0027, -0.0058,  0.0056,  ...,  0.0007, -0.0396,  0.0579],\n","                      [-0.0040, -0.0135, -0.0070,  ...,  0.0099,  0.0200, -0.0080]],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.17.input_layernorm.weight',\n","              tensor([1.4531, 2.1719, 1.5391,  ..., 1.7734, 1.5781, 1.6797],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.layers.17.post_attention_layernorm.weight',\n","              tensor([2.2188, 2.3125, 2.8125,  ..., 2.3438, 2.3906, 2.3750],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.model.norm.weight',\n","              tensor([0.2852, 0.8242, 0.5898,  ..., 0.8828, 0.5195, 0.8672],\n","                     dtype=torch.bfloat16)),\n","             ('language_model.lm_head.weight',\n","              tensor([[ 0.5352, -0.0250,  0.0874,  ...,  0.0432,  0.2617,  0.0674],\n","                      [ 0.1611, -0.1611, -0.1455,  ..., -0.0271,  0.0056, -0.0359],\n","                      [ 0.1177,  0.0214, -0.0289,  ..., -0.0042,  0.0044, -0.0056],\n","                      ...,\n","                      [ 0.3965, -0.0184,  0.0253,  ...,  0.0159,  0.1309,  0.0148],\n","                      [ 0.3965, -0.0182,  0.0260,  ...,  0.0157,  0.1309,  0.0151],\n","                      [ 0.3965, -0.0186,  0.0258,  ...,  0.0156,  0.1318,  0.0151]],\n","                     dtype=torch.bfloat16))])"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":[],"metadata":{"id":"omuht8B5sf71"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["d"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":121},"id":"UP-M86REpBnx","outputId":"35867af2-ccc7-4e84-da93-652d10f7dde2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<bound method PaliGemmaProcessor.__call__ of <__main__.PaliGemmaProcessor object at 0x7f868387ea10>>"],"text/html":["<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n","      pre.function-repr-contents {\n","        overflow-x: auto;\n","        padding: 8px 12px;\n","        max-height: 500px;\n","      }\n","\n","      pre.function-repr-contents.function-repr-contents-collapsed {\n","        cursor: pointer;\n","        max-height: 100px;\n","      }\n","    </style>\n","    <pre style=\"white-space: initial; background:\n","         var(--colab-secondary-surface-color); padding: 8px 12px;\n","         border-bottom: 1px solid var(--colab-border-color);\"><b>PaliGemmaProcessor.__call__</b><br/>def __call__(text: List[str], images: List[Image.Image], padding: str=&#x27;longest&#x27;, truncation: bool=True) -&gt; dict</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/&lt;ipython-input-20-8fd7b1a3770c&gt;</a>Call self as a function.</pre></div>"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["# MODEL_PATH=\"$HOME/projects/paligemma-weights/paligemma-3b-pt-224\"\n","# PROMPT=\"this building is \"\n","# IMAGE_FILE_PATH=\"test_images/pic1.jpeg\"\n","# MAX_TOKENS_TO_GENERATE=100\n","# TEMPERATURE=0.8\n","# TOP_P=0.9\n","# DO_SAMPLE=\"False\"\n","# ONLY_CPU=\"False\"\n","\n","# python inference.py \\\n","#     --model_path \"$MODEL_PATH\" \\\n","#     --prompt \"$PROMPT\" \\\n","#     --image_file_path \"$IMAGE_FILE_PATH\" \\\n","#     --max_tokens_to_generate $MAX_TOKENS_TO_GENERATE \\\n","#     --temperature $TEMPERATURE \\\n","#     --top_p $TOP_P \\\n","#     --do_sample $DO_SAMPLE \\\n","#     --only_cpu $ONLY_CPU \\\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2v3rQiWPqvSv","outputId":"36b8e2d8-ecb4-4c73-fae5-a7969bafecc9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([196])"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":["# import os\n","\n","# os.mkdir('testImages')"],"metadata":{"id":"PSe9lvvUtMyf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"UmiU61Ael0Rj","outputId":"42328e43-d26f-4267-91e5-d18234fc79fd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["import os\n","# os.getcwd()\n","os.listdir('testImages')\n","img = np.array(Image.open('testImages/garden.jpg'))\n","img.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v8aeVSPrqvim","outputId":"bed26a75-d057-463c-97b6-91734fdd95d0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1200, 1800, 3)"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GRnDGnw1n3sA","outputId":"9741a2e4-f785-4dfa-aee6-879ec26d2f00"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 9.91M/9.91M [00:00<00:00, 40.2MB/s]\n","100%|██████████| 28.9k/28.9k [00:00<00:00, 1.17MB/s]\n","100%|██████████| 1.65M/1.65M [00:00<00:00, 10.1MB/s]\n","100%|██████████| 4.54k/4.54k [00:00<00:00, 4.64MB/s]\n"]}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"28BniNJJeWvi","outputId":"2a84deac-ba52-4b37-915a-520020c1f3a5"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'VisionComponent' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-06846f706136>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfigs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatchSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddingChannels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVisionComponent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'VisionComponent' is not defined"]}]},{"cell_type":"code","source":[],"metadata":{"id":"tyBjPI__oODD"},"execution_count":null,"outputs":[]}]}